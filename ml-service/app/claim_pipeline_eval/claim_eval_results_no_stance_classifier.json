{
  "results": [
    {
      "claim": "Scaling laws indicate that increasing model size and data generally improves language model performance.",
      "expected_verdict": "SUPPORT",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Scaling laws indicate that increasing model size improves language model performance.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6142,
                "faiss_score": 0.9138842225074768,
                "faiss_rank": 6,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 5.783466339111328,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.697350561618805
              },
              {
                "id": 6124,
                "faiss_score": 0.8996810913085938,
                "faiss_rank": 8,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 5.338567733764648,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.238248825073242
              },
              {
                "id": 6133,
                "faiss_score": 0.8973473310470581,
                "faiss_rank": 10,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.928947925567627,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.826295256614685
              }
            ]
          }
        },
        {
          "subclaim": "Scaling laws indicate that increasing data generally improves language model performance.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6127,
                "faiss_score": 0.9171009063720703,
                "faiss_rank": 2,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "Empirical observations have shown that, under certain conditions, model performance improves predictably as scale increases.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.851520538330078,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.768621444702148
              },
              {
                "id": 6142,
                "faiss_score": 0.9079248309135437,
                "faiss_rank": 5,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.8335371017456055,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.741461932659149
              },
              {
                "id": 6124,
                "faiss_score": 0.8999987840652466,
                "faiss_rank": 9,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.305981636047363,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.20598042011261
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Scaling laws indicate that increasing model size improves language model performance.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6142,
                      "faiss_score": 0.9138842225074768,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 5.783466339111328,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.697350561618805
                    },
                    {
                      "id": 6124,
                      "faiss_score": 0.8996810913085938,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 5.338567733764648,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.238248825073242
                    },
                    {
                      "id": 6133,
                      "faiss_score": 0.8973473310470581,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.928947925567627,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.826295256614685
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling laws indicate that increasing data generally improves language model performance.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6127,
                      "faiss_score": 0.9171009063720703,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "Empirical observations have shown that, under certain conditions, model performance improves predictably as scale increases.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.851520538330078,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.768621444702148
                    },
                    {
                      "id": 6142,
                      "faiss_score": 0.9079248309135437,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.8335371017456055,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.741461932659149
                    },
                    {
                      "id": 6124,
                      "faiss_score": 0.8999987840652466,
                      "faiss_rank": 9,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.305981636047363,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.20598042011261
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Transformer architectures enabled significant improvements in natural language processing tasks.",
      "expected_verdict": "SUPPORT",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Transformer architectures were used in natural language processing tasks.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6296,
                "faiss_score": 0.9599642753601074,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 35,
                "sentence": "The transformer architecture proved highly effective across a wide range of natural language processing tasks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 9.242033958435059,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 10.201998233795166
              },
              {
                "id": 2184,
                "faiss_score": 0.9435071349143982,
                "faiss_rank": 2,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 115,
                "sentence": "Transformers have increasingly become the model of choice for natural language processing.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": 4.645409107208252,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.58891624212265
              },
              {
                "id": 3003,
                "faiss_score": 0.8870973587036133,
                "faiss_rank": 17,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 58,
                "sentence": "Tasks for pretraining and fine-tuning commonly include: language modeling next-sentence prediction question answering reading comprehension sentiment analysis paraphrasing The T5 transformer report documents a large number of natural language pretraining tasks.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": 3.4558331966400146,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.342930555343628
              }
            ]
          }
        },
        {
          "subclaim": "These tasks saw significant improvements.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5586,
                "faiss_score": 0.868844747543335,
                "faiss_rank": 11,
                "doc_id": "local_bio_gene_editing.txt",
                "file_type": ".txt",
                "position": 59,
                "sentence": "Improvements in precision, delivery, and control have expanded the range of possible applications.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_gene_editing.txt",
                "primary_category": null,
                "rerank_score": -1.6070451736450195,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.7382004261016846
              },
              {
                "id": 1877,
                "faiss_score": 0.8633498549461365,
                "faiss_rank": 15,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 145,
                "sentence": "Early research demonstrated that inserting intermediate \"scratchpad\" computations could improve performance on such tasks.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -2.0456624031066895,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.182312548160553
              },
              {
                "id": 6046,
                "faiss_score": 0.8739926815032959,
                "faiss_rank": 5,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 6,
                "sentence": "These include improved generalization, better handling of rare or ambiguous inputs, and the ability to adapt to new tasks with minimal additional data.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -2.1880946159362793,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.3141019344329834
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Transformer architectures were used in natural language processing tasks.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6296,
                      "faiss_score": 0.9599642753601074,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 35,
                      "sentence": "The transformer architecture proved highly effective across a wide range of natural language processing tasks.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 9.242033958435059,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 10.201998233795166
                    },
                    {
                      "id": 2184,
                      "faiss_score": 0.9435071349143982,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Artificial_neural_network",
                      "file_type": ".txt",
                      "position": 115,
                      "sentence": "Transformers have increasingly become the model of choice for natural language processing.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                      "primary_category": "neural networks",
                      "rerank_score": 4.645409107208252,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.58891624212265
                    },
                    {
                      "id": 3003,
                      "faiss_score": 0.8870973587036133,
                      "faiss_rank": 17,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 58,
                      "sentence": "Tasks for pretraining and fine-tuning commonly include: language modeling next-sentence prediction question answering reading comprehension sentiment analysis paraphrasing The T5 transformer report documents a large number of natural language pretraining tasks.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": 3.4558331966400146,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.342930555343628
                    }
                  ]
                }
              },
              {
                "subclaim": "These tasks saw significant improvements.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5586,
                      "faiss_score": 0.868844747543335,
                      "faiss_rank": 11,
                      "doc_id": "local_bio_gene_editing.txt",
                      "file_type": ".txt",
                      "position": 59,
                      "sentence": "Improvements in precision, delivery, and control have expanded the range of possible applications.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_gene_editing.txt",
                      "primary_category": null,
                      "rerank_score": -1.6070451736450195,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.7382004261016846
                    },
                    {
                      "id": 1877,
                      "faiss_score": 0.8633498549461365,
                      "faiss_rank": 15,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 145,
                      "sentence": "Early research demonstrated that inserting intermediate \"scratchpad\" computations could improve performance on such tasks.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -2.0456624031066895,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.182312548160553
                    },
                    {
                      "id": 6046,
                      "faiss_score": 0.8739926815032959,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 6,
                      "sentence": "These include improved generalization, better handling of rare or ambiguous inputs, and the ability to adapt to new tasks with minimal additional data.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -2.1880946159362793,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.3141019344329834
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Fault tolerance is a fundamental requirement for reliable distributed systems.",
      "expected_verdict": "SUPPORT",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Fault tolerance is a fundamental requirement for reliable distributed systems.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6597,
                "faiss_score": 0.9170854091644287,
                "faiss_rank": 10,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Fault tolerance and reliability describe a system\u2019s ability to continue operating correctly in the presence of failures.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": 5.004147529602051,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.9212329387664795
              },
              {
                "id": 568,
                "faiss_score": 0.9223860502243042,
                "faiss_rank": 6,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 144,
                "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 4.329262733459473,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.251648783683777
              },
              {
                "id": 6601,
                "faiss_score": 0.9003157615661621,
                "faiss_rank": 16,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 4,
                "sentence": "Fault tolerance focuses on how systems respond to failures, while reliability concerns the probability that a system performs its intended function over time.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": 4.317968368530273,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.2182841300964355
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Fault tolerance is a fundamental requirement for reliable distributed systems.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6597,
                      "faiss_score": 0.9170854091644287,
                      "faiss_rank": 10,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Fault tolerance and reliability describe a system\u2019s ability to continue operating correctly in the presence of failures.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": 5.004147529602051,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.9212329387664795
                    },
                    {
                      "id": 568,
                      "faiss_score": 0.9223860502243042,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 144,
                      "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 4.329262733459473,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.251648783683777
                    },
                    {
                      "id": 6601,
                      "faiss_score": 0.9003157615661621,
                      "faiss_rank": 16,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 4,
                      "sentence": "Fault tolerance focuses on how systems respond to failures, while reliability concerns the probability that a system performs its intended function over time.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": 4.317968368530273,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.2182841300964355
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Increasing computational resources can improve the training performance of deep learning models.",
      "expected_verdict": "SUPPORT",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Increasing computational resources can improve the training performance of deep learning models.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5906,
                "faiss_score": 0.8968743085861206,
                "faiss_rank": 4,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 4.011983871459961,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.9088581800460815
              },
              {
                "id": 1901,
                "faiss_score": 0.8911174535751343,
                "faiss_rank": 9,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 169,
                "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 3.919332504272461,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.810449957847595
              },
              {
                "id": 2622,
                "faiss_score": 0.8935152292251587,
                "faiss_rank": 6,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 217,
                "sentence": "Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 2.851893901824951,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.74540913105011
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Increasing computational resources can improve the training performance of deep learning models.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5906,
                      "faiss_score": 0.8968743085861206,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 1,
                      "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 4.011983871459961,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.9088581800460815
                    },
                    {
                      "id": 1901,
                      "faiss_score": 0.8911174535751343,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 169,
                      "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 3.919332504272461,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.810449957847595
                    },
                    {
                      "id": 2622,
                      "faiss_score": 0.8935152292251587,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 217,
                      "sentence": "Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": 2.851893901824951,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.74540913105011
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Large language models can perform multiple language tasks without task-specific fine-tuning.",
      "expected_verdict": "SUPPORT",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Large language models can perform multiple language tasks",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6079,
                "faiss_score": 0.9092576503753662,
                "faiss_rank": 2,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "Large language models are often described as general-purpose systems because they can be adapted to many tasks with minimal fine-tuning.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 6.446345329284668,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 7.355602979660034
              },
              {
                "id": 6121,
                "faiss_score": 0.9074077606201172,
                "faiss_rank": 3,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 5.14181661605835,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.049224376678467
              },
              {
                "id": 6040,
                "faiss_score": 0.9093468189239502,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 5.019293785095215,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.928640604019165
              }
            ]
          }
        },
        {
          "subclaim": "without task-specific fine-tuning",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 1833,
                "faiss_score": 0.8788925409317017,
                "faiss_rank": 5,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 101,
                "sentence": "This technique, called few-shot prompting, allows LLMs to be adapted to any task without requiring fine-tuning.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 5.452361583709717,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.3312541246414185
              },
              {
                "id": 6079,
                "faiss_score": 0.8675508499145508,
                "faiss_rank": 14,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "Large language models are often described as general-purpose systems because they can be adapted to many tasks with minimal fine-tuning.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 0.734641432762146,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.6021922826766968
              },
              {
                "id": 2566,
                "faiss_score": 0.875229001045227,
                "faiss_rank": 7,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 161,
                "sentence": "Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 0.4810207486152649,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.356249749660492
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Large language models can perform multiple language tasks",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6079,
                      "faiss_score": 0.9092576503753662,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 39,
                      "sentence": "Large language models are often described as general-purpose systems because they can be adapted to many tasks with minimal fine-tuning.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 6.446345329284668,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 7.355602979660034
                    },
                    {
                      "id": 6121,
                      "faiss_score": 0.9074077606201172,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 5.14181661605835,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.049224376678467
                    },
                    {
                      "id": 6040,
                      "faiss_score": 0.9093468189239502,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 5.019293785095215,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.928640604019165
                    }
                  ]
                }
              },
              {
                "subclaim": "without task-specific fine-tuning",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 1833,
                      "faiss_score": 0.8788925409317017,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 101,
                      "sentence": "This technique, called few-shot prompting, allows LLMs to be adapted to any task without requiring fine-tuning.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 5.452361583709717,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.3312541246414185
                    },
                    {
                      "id": 6079,
                      "faiss_score": 0.8675508499145508,
                      "faiss_rank": 14,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 39,
                      "sentence": "Large language models are often described as general-purpose systems because they can be adapted to many tasks with minimal fine-tuning.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 0.734641432762146,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.6021922826766968
                    },
                    {
                      "id": 2566,
                      "faiss_score": 0.875229001045227,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 161,
                      "sentence": "Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": 0.4810207486152649,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.356249749660492
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Distributed consensus protocols help systems remain consistent despite node failures.",
      "expected_verdict": "SUPPORT",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Distributed consensus protocols help systems remain consistent",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5612,
                "faiss_score": 0.888638973236084,
                "faiss_rank": 18,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 15,
                "sentence": "Strong consistency models aim to make distributed systems behave as if there were a single shared state, but enforcing such behavior requires coordination and synchronization, which can be expensive or impossible under certain failure conditions.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 3.147623062133789,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.036262035369873
              },
              {
                "id": 5618,
                "faiss_score": 0.9274933934211731,
                "faiss_rank": 1,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Consensus is a fundamental problem in distributed systems that captures the difficulty of agreement in the presence of failures.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 2.1761515140533447,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.103644907474518
              },
              {
                "id": 5610,
                "faiss_score": 0.9084477424621582,
                "faiss_rank": 3,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Consistency is a central concept in distributed systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 1.8636924028396606,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.772140145301819
              }
            ]
          }
        },
        {
          "subclaim": "Systems remain consistent despite node failures",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 3474,
                "faiss_score": 0.9279701709747314,
                "faiss_rank": 1,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 45,
                "sentence": "Resilient networks continue to transmit data despite the failure of some links or nodes.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 2.9349451065063477,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.862915277481079
              },
              {
                "id": 5615,
                "faiss_score": 0.9032431840896606,
                "faiss_rank": 3,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "By maintaining multiple copies of data across different nodes, a system can continue to operate even if some replicas fail.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 2.553046703338623,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.4562898874282837
              },
              {
                "id": 6603,
                "faiss_score": 0.8794848322868347,
                "faiss_rank": 11,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 6,
                "sentence": "A system may tolerate certain failures gracefully yet still exhibit low overall reliability if failures occur frequently.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": -0.1449195146560669,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.7345653176307678
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Distributed consensus protocols help systems remain consistent",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5612,
                      "faiss_score": 0.888638973236084,
                      "faiss_rank": 18,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 15,
                      "sentence": "Strong consistency models aim to make distributed systems behave as if there were a single shared state, but enforcing such behavior requires coordination and synchronization, which can be expensive or impossible under certain failure conditions.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 3.147623062133789,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.036262035369873
                    },
                    {
                      "id": 5618,
                      "faiss_score": 0.9274933934211731,
                      "faiss_rank": 1,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Consensus is a fundamental problem in distributed systems that captures the difficulty of agreement in the presence of failures.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 2.1761515140533447,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.103644907474518
                    },
                    {
                      "id": 5610,
                      "faiss_score": 0.9084477424621582,
                      "faiss_rank": 3,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Consistency is a central concept in distributed systems.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 1.8636924028396606,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.772140145301819
                    }
                  ]
                }
              },
              {
                "subclaim": "Systems remain consistent despite node failures",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 3474,
                      "faiss_score": 0.9279701709747314,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Fault_tolerance",
                      "file_type": ".txt",
                      "position": 45,
                      "sentence": "Resilient networks continue to transmit data despite the failure of some links or nodes.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 2.9349451065063477,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.862915277481079
                    },
                    {
                      "id": 5615,
                      "faiss_score": 0.9032431840896606,
                      "faiss_rank": 3,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "By maintaining multiple copies of data across different nodes, a system can continue to operate even if some replicas fail.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 2.553046703338623,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.4562898874282837
                    },
                    {
                      "id": 6603,
                      "faiss_score": 0.8794848322868347,
                      "faiss_rank": 11,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 6,
                      "sentence": "A system may tolerate certain failures gracefully yet still exhibit low overall reliability if failures occur frequently.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": -0.1449195146560669,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.7345653176307678
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Increasing model size always guarantees better generalization performance.",
      "expected_verdict": "CONTRADICT",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Increasing model size always guarantees better generalization performance.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6133,
                "faiss_score": 0.8974666595458984,
                "faiss_rank": 15,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 5.871306896209717,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.768773555755615
              },
              {
                "id": 6137,
                "faiss_score": 0.9333542585372925,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 3.651726245880127,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.585080504417419
              },
              {
                "id": 5941,
                "faiss_score": 0.901360034942627,
                "faiss_rank": 9,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 36,
                "sentence": "Smaller or compressed models may generalize better due to implicit regularization, but excessive compression can harm performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 2.947385549545288,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.848745584487915
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Increasing model size always guarantees better generalization performance.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6133,
                      "faiss_score": 0.8974666595458984,
                      "faiss_rank": 15,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 5.871306896209717,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.768773555755615
                    },
                    {
                      "id": 6137,
                      "faiss_score": 0.9333542585372925,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 3.651726245880127,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.585080504417419
                    },
                    {
                      "id": 5941,
                      "faiss_score": 0.901360034942627,
                      "faiss_rank": 9,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 36,
                      "sentence": "Smaller or compressed models may generalize better due to implicit regularization, but excessive compression can harm performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 2.947385549545288,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.848745584487915
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Distributed systems do not need fault tolerance mechanisms.",
      "expected_verdict": "CONTRADICT",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Distributed systems do not need fault tolerance mechanisms.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 430,
                "faiss_score": 0.8796877861022949,
                "faiss_rank": 13,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 6,
                "sentence": "Other typical properties of distributed systems are: The system must tolerate failures in individual computers.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.483339786529541,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.363027572631836
              },
              {
                "id": 3809,
                "faiss_score": 0.9038978815078735,
                "faiss_rank": 4,
                "doc_id": "wiki_CAP_theorem",
                "file_type": ".txt",
                "position": 0,
                "sentence": "No distributed system is safe from network failures, thus network partitioning generally has to be tolerated.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/CAP_theorem",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 0.8771834373474121,
                "rerank_rank": 4,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.7810813188552856
              },
              {
                "id": 3456,
                "faiss_score": 0.893700361251831,
                "faiss_rank": 9,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 27,
                "sentence": "It is helpful if the time between failures is as long as possible, but this is not specifically required in a fault-tolerant system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 0.8421114683151245,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.7358118295669556
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Distributed systems do not need fault tolerance mechanisms.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 430,
                      "faiss_score": 0.8796877861022949,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 6,
                      "sentence": "Other typical properties of distributed systems are: The system must tolerate failures in individual computers.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.483339786529541,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.363027572631836
                    },
                    {
                      "id": 3809,
                      "faiss_score": 0.9038978815078735,
                      "faiss_rank": 4,
                      "doc_id": "wiki_CAP_theorem",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "No distributed system is safe from network failures, thus network partitioning generally has to be tolerated.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/CAP_theorem",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 0.8771834373474121,
                      "rerank_rank": 4,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.7810813188552856
                    },
                    {
                      "id": 3456,
                      "faiss_score": 0.893700361251831,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Fault_tolerance",
                      "file_type": ".txt",
                      "position": 27,
                      "sentence": "It is helpful if the time between failures is as long as possible, but this is not specifically required in a fault-tolerant system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 0.8421114683151245,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.7358118295669556
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Quantum computers can function reliably without error correction.",
      "expected_verdict": "CONTRADICT",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Quantum computers can function reliably without error correction.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6553,
                "faiss_score": 0.9091418981552124,
                "faiss_rank": 1,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 4.795175075531006,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.704316973686218
              },
              {
                "id": 6561,
                "faiss_score": 0.8931423425674438,
                "faiss_rank": 16,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Small errors accumulate quickly in quantum circuits, limiting the depth of computations that can be performed reliably.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 4.629236221313477,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.52237856388092
              },
              {
                "id": 793,
                "faiss_score": 0.8939269781112671,
                "faiss_rank": 13,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.389732837677002,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.283659815788269
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Quantum computers can function reliably without error correction.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6553,
                      "faiss_score": 0.9091418981552124,
                      "faiss_rank": 1,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 4.795175075531006,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.704316973686218
                    },
                    {
                      "id": 6561,
                      "faiss_score": 0.8931423425674438,
                      "faiss_rank": 16,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Small errors accumulate quickly in quantum circuits, limiting the depth of computations that can be performed reliably.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 4.629236221313477,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.52237856388092
                    },
                    {
                      "id": 793,
                      "faiss_score": 0.8939269781112671,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.389732837677002,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.283659815788269
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Scaling neural networks has no impact on performance improvements.",
      "expected_verdict": "CONTRADICT",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Scaling neural networks has no impact on performance improvements.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6131,
                "faiss_score": 0.8818397521972656,
                "faiss_rank": 2,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 0.9271154403686523,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.808955192565918
              },
              {
                "id": 6124,
                "faiss_score": 0.8662250638008118,
                "faiss_rank": 8,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 0.04794492572546005,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.9141699895262718
              },
              {
                "id": 6125,
                "faiss_score": 0.8836263418197632,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Over the past decade, scaling has emerged as one of the most reliable drivers of progress in machine learning systems, particularly in deep learning.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -1.5403499603271484,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.6567236185073853
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Scaling neural networks has no impact on performance improvements.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6131,
                      "faiss_score": 0.8818397521972656,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 0.9271154403686523,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.808955192565918
                    },
                    {
                      "id": 6124,
                      "faiss_score": 0.8662250638008118,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 0.04794492572546005,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.9141699895262718
                    },
                    {
                      "id": 6125,
                      "faiss_score": 0.8836263418197632,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 1,
                      "sentence": "Over the past decade, scaling has emerged as one of the most reliable drivers of progress in machine learning systems, particularly in deep learning.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -1.5403499603271484,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.6567236185073853
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Larger datasets always reduce overfitting in machine learning models.",
      "expected_verdict": "CONTRADICT",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Larger datasets reduce overfitting in machine learning models.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 2613,
                "faiss_score": 0.8993738889694214,
                "faiss_rank": 10,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 208,
                "sentence": "Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 4.625890731811523,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.525264620780945
              },
              {
                "id": 6136,
                "faiss_score": 0.912011444568634,
                "faiss_rank": 5,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 12,
                "sentence": "Training large models on insufficient or low-quality data can lead to overfitting or wasted capacity.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 3.8103489875793457,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.72236043214798
              },
              {
                "id": 6309,
                "faiss_score": 0.901115894317627,
                "faiss_rank": 8,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 48,
                "sentence": "High-capacity models are prone to overfitting when data is scarce, and strong generalization typically requires pretraining on massive corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 1.2675398588180542,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.168655753135681
              }
            ]
          }
        },
        {
          "subclaim": "Machine learning models can suffer from overfitting.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 1311,
                "faiss_score": 0.9410011172294617,
                "faiss_rank": 3,
                "doc_id": "wiki_Statistical_learning_theory",
                "file_type": ".txt",
                "position": 21,
                "sentence": "In machine learning problems, a major problem that arises is that of overfitting.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Statistical_learning_theory",
                "primary_category": "machine learning",
                "rerank_score": 5.9156341552734375,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.856635272502899
              },
              {
                "id": 1218,
                "faiss_score": 0.9458591341972351,
                "faiss_rank": 1,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 243,
                "sentence": "Overfitting is something to watch out for when training a machine learning model.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": 5.892265319824219,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.838124454021454
              },
              {
                "id": 5988,
                "faiss_score": 0.9075533747673035,
                "faiss_rank": 18,
                "doc_id": "local_ml_evaluation_and_generalization.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Overfitting occurs when a model learns patterns specific to the training data that do not generalize.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                "primary_category": null,
                "rerank_score": 4.331396102905273,
                "rerank_rank": 4,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.238949477672577
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Larger datasets reduce overfitting in machine learning models.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 2613,
                      "faiss_score": 0.8993738889694214,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 208,
                      "sentence": "Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": 4.625890731811523,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.525264620780945
                    },
                    {
                      "id": 6136,
                      "faiss_score": 0.912011444568634,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 12,
                      "sentence": "Training large models on insufficient or low-quality data can lead to overfitting or wasted capacity.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 3.8103489875793457,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.72236043214798
                    },
                    {
                      "id": 6309,
                      "faiss_score": 0.901115894317627,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 48,
                      "sentence": "High-capacity models are prone to overfitting when data is scarce, and strong generalization typically requires pretraining on massive corpora.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 1.2675398588180542,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.168655753135681
                    }
                  ]
                }
              },
              {
                "subclaim": "Machine learning models can suffer from overfitting.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 1311,
                      "faiss_score": 0.9410011172294617,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Statistical_learning_theory",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "In machine learning problems, a major problem that arises is that of overfitting.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Statistical_learning_theory",
                      "primary_category": "machine learning",
                      "rerank_score": 5.9156341552734375,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.856635272502899
                    },
                    {
                      "id": 1218,
                      "faiss_score": 0.9458591341972351,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 243,
                      "sentence": "Overfitting is something to watch out for when training a machine learning model.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": 5.892265319824219,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.838124454021454
                    },
                    {
                      "id": 5988,
                      "faiss_score": 0.9075533747673035,
                      "faiss_rank": 18,
                      "doc_id": "local_ml_evaluation_and_generalization.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "Overfitting occurs when a model learns patterns specific to the training data that do not generalize.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                      "primary_category": null,
                      "rerank_score": 4.331396102905273,
                      "rerank_rank": 4,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.238949477672577
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Transformer models eliminate the need for optimization techniques.",
      "expected_verdict": "CONTRADICT",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Transformer models exist",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 2987,
                "faiss_score": 0.8926093578338623,
                "faiss_rank": 12,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": 4.5255537033081055,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.418163061141968
              },
              {
                "id": 6392,
                "faiss_score": 0.8948999643325806,
                "faiss_rank": 8,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 131,
                "sentence": "As transformer-based models become more capable, concerns about misuse and unintended consequences grow.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 4.196916580200195,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.091816544532776
              },
              {
                "id": 1759,
                "faiss_score": 0.9132465124130249,
                "faiss_rank": 1,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 27,
                "sentence": "As of 2024, the largest and most capable models are all based on the transformer architecture.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 4.139178276062012,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.052424788475037
              }
            ]
          }
        },
        {
          "subclaim": "Optimization techniques are needed",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5957,
                "faiss_score": 0.892740786075592,
                "faiss_rank": 6,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 52,
                "sentence": "Implementing advanced compression or optimization techniques may require specialized expertise and tooling.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 5.0751824378967285,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.967923223972321
              },
              {
                "id": 1636,
                "faiss_score": 0.8969464898109436,
                "faiss_rank": 4,
                "doc_id": "wiki_Optimization_(mathematics)",
                "file_type": ".txt",
                "position": 85,
                "sentence": "Many optimization algorithms need to start from a feasible point.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                "primary_category": "all articles with style issues",
                "rerank_score": 4.919436454772949,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.816382944583893
              },
              {
                "id": 1719,
                "faiss_score": 0.885511040687561,
                "faiss_rank": 18,
                "doc_id": "wiki_Optimization_(mathematics)",
                "file_type": ".txt",
                "position": 168,
                "sentence": "Another field that uses optimization techniques extensively is operations research.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                "primary_category": "all articles with style issues",
                "rerank_score": 3.6928787231445312,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.578389763832092
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Transformer models exist",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 2987,
                      "faiss_score": 0.8926093578338623,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": 4.5255537033081055,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.418163061141968
                    },
                    {
                      "id": 6392,
                      "faiss_score": 0.8948999643325806,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 131,
                      "sentence": "As transformer-based models become more capable, concerns about misuse and unintended consequences grow.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 4.196916580200195,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.091816544532776
                    },
                    {
                      "id": 1759,
                      "faiss_score": 0.9132465124130249,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 27,
                      "sentence": "As of 2024, the largest and most capable models are all based on the transformer architecture.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 4.139178276062012,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.052424788475037
                    }
                  ]
                }
              },
              {
                "subclaim": "Optimization techniques are needed",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5957,
                      "faiss_score": 0.892740786075592,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 52,
                      "sentence": "Implementing advanced compression or optimization techniques may require specialized expertise and tooling.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 5.0751824378967285,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.967923223972321
                    },
                    {
                      "id": 1636,
                      "faiss_score": 0.8969464898109436,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Optimization_(mathematics)",
                      "file_type": ".txt",
                      "position": 85,
                      "sentence": "Many optimization algorithms need to start from a feasible point.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                      "primary_category": "all articles with style issues",
                      "rerank_score": 4.919436454772949,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.816382944583893
                    },
                    {
                      "id": 1719,
                      "faiss_score": 0.885511040687561,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Optimization_(mathematics)",
                      "file_type": ".txt",
                      "position": 168,
                      "sentence": "Another field that uses optimization techniques extensively is operations research.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                      "primary_category": "all articles with style issues",
                      "rerank_score": 3.6928787231445312,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.578389763832092
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Scaling model size improves performance but introduces efficiency and stability challenges.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Scaling model size improves performance",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6124,
                "faiss_score": 0.9064903259277344,
                "faiss_rank": 6,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 7.801340579986572,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 8.707830905914307
              },
              {
                "id": 6133,
                "faiss_score": 0.9184768795967102,
                "faiss_rank": 2,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 7.172950744628906,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 8.091427624225616
              },
              {
                "id": 6127,
                "faiss_score": 0.9101240634918213,
                "faiss_rank": 4,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "Empirical observations have shown that, under certain conditions, model performance improves predictably as scale increases.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 6.25351619720459,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 7.163640260696411
              }
            ]
          }
        },
        {
          "subclaim": "Scaling model size introduces efficiency challenges",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6149,
                "faiss_score": 0.9049237966537476,
                "faiss_rank": 4,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 25,
                "sentence": "Scaling also introduces engineering challenges.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 2.976050615310669,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.8809744119644165
              },
              {
                "id": 6349,
                "faiss_score": 0.9152157306671143,
                "faiss_rank": 3,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 88,
                "sentence": "As models scale, training efficiency becomes a primary concern.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 1.1722049713134766,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.087420701980591
              },
              {
                "id": 6124,
                "faiss_score": 0.8902746438980103,
                "faiss_rank": 18,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 0.3690628707408905,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.2593375146389008
              }
            ]
          }
        },
        {
          "subclaim": "Scaling model size introduces stability challenges",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6149,
                "faiss_score": 0.8830956220626831,
                "faiss_rank": 8,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 25,
                "sentence": "Scaling also introduces engineering challenges.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 2.749152183532715,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.632247805595398
              },
              {
                "id": 6133,
                "faiss_score": 0.9155011177062988,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -1.6219263076782227,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.7064251899719238
              },
              {
                "id": 6131,
                "faiss_score": 0.8920418620109558,
                "faiss_rank": 3,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -2.5046567916870117,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.612614929676056
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Scaling model size improves performance",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6124,
                      "faiss_score": 0.9064903259277344,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 7.801340579986572,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 8.707830905914307
                    },
                    {
                      "id": 6133,
                      "faiss_score": 0.9184768795967102,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 7.172950744628906,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 8.091427624225616
                    },
                    {
                      "id": 6127,
                      "faiss_score": 0.9101240634918213,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "Empirical observations have shown that, under certain conditions, model performance improves predictably as scale increases.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 6.25351619720459,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 7.163640260696411
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling model size introduces efficiency challenges",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6149,
                      "faiss_score": 0.9049237966537476,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 25,
                      "sentence": "Scaling also introduces engineering challenges.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 2.976050615310669,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.8809744119644165
                    },
                    {
                      "id": 6349,
                      "faiss_score": 0.9152157306671143,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 88,
                      "sentence": "As models scale, training efficiency becomes a primary concern.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 1.1722049713134766,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.087420701980591
                    },
                    {
                      "id": 6124,
                      "faiss_score": 0.8902746438980103,
                      "faiss_rank": 18,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 0.3690628707408905,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.2593375146389008
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling model size introduces stability challenges",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6149,
                      "faiss_score": 0.8830956220626831,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 25,
                      "sentence": "Scaling also introduces engineering challenges.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 2.749152183532715,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.632247805595398
                    },
                    {
                      "id": 6133,
                      "faiss_score": 0.9155011177062988,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -1.6219263076782227,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.7064251899719238
                    },
                    {
                      "id": 6131,
                      "faiss_score": 0.8920418620109558,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -2.5046567916870117,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.612614929676056
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Distributed systems improve scalability but increase system complexity.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Distributed systems improve scalability",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5658,
                "faiss_score": 0.9028811454772949,
                "faiss_rank": 9,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 61,
                "sentence": "Eventually consistent systems illustrate how relaxing guarantees can improve scalability.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 2.7626805305480957,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.6655616760253906
              },
              {
                "id": 499,
                "faiss_score": 0.9017831683158875,
                "faiss_rank": 10,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 75,
                "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.7927249670028687,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.694508135318756
              },
              {
                "id": 445,
                "faiss_score": 0.9056973457336426,
                "faiss_rank": 4,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.2563084363937378,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.1620057821273804
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems increase system complexity",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5674,
                "faiss_score": 0.9308851957321167,
                "faiss_rank": 1,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 77,
                "sentence": "Distributed systems exemplify the broader theme that complexity emerges from interaction.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 6.873047828674316,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 7.803933024406433
              },
              {
                "id": 499,
                "faiss_score": 0.9024401903152466,
                "faiss_rank": 8,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 75,
                "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.669622778892517,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.5720629692077637
              },
              {
                "id": 5597,
                "faiss_score": 0.9027834534645081,
                "faiss_rank": 7,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 1.5515655279159546,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.4543489813804626
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Distributed systems improve scalability",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5658,
                      "faiss_score": 0.9028811454772949,
                      "faiss_rank": 9,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 61,
                      "sentence": "Eventually consistent systems illustrate how relaxing guarantees can improve scalability.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 2.7626805305480957,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.6655616760253906
                    },
                    {
                      "id": 499,
                      "faiss_score": 0.9017831683158875,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.7927249670028687,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.694508135318756
                    },
                    {
                      "id": 445,
                      "faiss_score": 0.9056973457336426,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.2563084363937378,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.1620057821273804
                    }
                  ]
                }
              },
              {
                "subclaim": "Distributed systems increase system complexity",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5674,
                      "faiss_score": 0.9308851957321167,
                      "faiss_rank": 1,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 77,
                      "sentence": "Distributed systems exemplify the broader theme that complexity emerges from interaction.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 6.873047828674316,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 7.803933024406433
                    },
                    {
                      "id": 499,
                      "faiss_score": 0.9024401903152466,
                      "faiss_rank": 8,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.669622778892517,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.5720629692077637
                    },
                    {
                      "id": 5597,
                      "faiss_score": 0.9027834534645081,
                      "faiss_rank": 7,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 1.5515655279159546,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.4543489813804626
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Large language models are powerful but can produce incorrect or misleading outputs.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Large language models are powerful",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6121,
                "faiss_score": 0.9278501868247986,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 9.14767837524414,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 10.07552856206894
              },
              {
                "id": 2020,
                "faiss_score": 0.9012563228607178,
                "faiss_rank": 10,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 6.476112365722656,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 7.377368688583374
              },
              {
                "id": 6040,
                "faiss_score": 0.925839900970459,
                "faiss_rank": 2,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 6.383889675140381,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 7.30972957611084
              }
            ]
          }
        },
        {
          "subclaim": "can produce incorrect or misleading outputs",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6606,
                "faiss_score": 0.863052248954773,
                "faiss_rank": 1,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "Components may crash completely, producing no output, or they may continue running while producing incorrect results.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": 3.4065425395965576,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.269594788551331
              },
              {
                "id": 3535,
                "faiss_score": 0.8539549112319946,
                "faiss_rank": 4,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 106,
                "sentence": "In this case, the voting circuit can output the correct result, and discard the erroneous version.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 1.0036264657974243,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.857581377029419
              },
              {
                "id": 892,
                "faiss_score": 0.8531967401504517,
                "faiss_rank": 5,
                "doc_id": "wiki_Error_correction",
                "file_type": ".txt",
                "position": 65,
                "sentence": "of errors in the output.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Error_detection_and_correction",
                "primary_category": "all articles needing additional references",
                "rerank_score": -0.670259952545166,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.18293678760528564
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Large language models are powerful",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6121,
                      "faiss_score": 0.9278501868247986,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 9.14767837524414,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 10.07552856206894
                    },
                    {
                      "id": 2020,
                      "faiss_score": 0.9012563228607178,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 6.476112365722656,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 7.377368688583374
                    },
                    {
                      "id": 6040,
                      "faiss_score": 0.925839900970459,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 6.383889675140381,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 7.30972957611084
                    }
                  ]
                }
              },
              {
                "subclaim": "can produce incorrect or misleading outputs",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6606,
                      "faiss_score": 0.863052248954773,
                      "faiss_rank": 1,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "Components may crash completely, producing no output, or they may continue running while producing incorrect results.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": 3.4065425395965576,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.269594788551331
                    },
                    {
                      "id": 3535,
                      "faiss_score": 0.8539549112319946,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Fault_tolerance",
                      "file_type": ".txt",
                      "position": 106,
                      "sentence": "In this case, the voting circuit can output the correct result, and discard the erroneous version.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 1.0036264657974243,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.857581377029419
                    },
                    {
                      "id": 892,
                      "faiss_score": 0.8531967401504517,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Error_correction",
                      "file_type": ".txt",
                      "position": 65,
                      "sentence": "of errors in the output.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Error_detection_and_correction",
                      "primary_category": "all articles needing additional references",
                      "rerank_score": -0.670259952545166,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.18293678760528564
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Quantum error correction enables scaling but adds significant overhead.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Quantum error correction enables scaling",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6553,
                "faiss_score": 0.9426121711730957,
                "faiss_rank": 3,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 6.97873592376709,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 7.9213480949401855
              },
              {
                "id": 4719,
                "faiss_score": 0.9455520510673523,
                "faiss_rank": 1,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 6.933759689331055,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 7.879311740398407
              },
              {
                "id": 4795,
                "faiss_score": 0.9083656072616577,
                "faiss_rank": 9,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 4.952396392822266,
                "rerank_rank": 4,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.860762000083923
              }
            ]
          }
        },
        {
          "subclaim": "Quantum error correction adds significant overhead",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 764,
                "faiss_score": 0.9263245463371277,
                "faiss_rank": 6,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 168,
                "sentence": "However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 5.299180507659912,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.22550505399704
              },
              {
                "id": 4719,
                "faiss_score": 0.9596387147903442,
                "faiss_rank": 1,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.731478214263916,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.69111692905426
              },
              {
                "id": 6553,
                "faiss_score": 0.9569774270057678,
                "faiss_rank": 3,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": -0.2708771228790283,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.6861003041267395
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Quantum error correction enables scaling",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6553,
                      "faiss_score": 0.9426121711730957,
                      "faiss_rank": 3,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 6.97873592376709,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 7.9213480949401855
                    },
                    {
                      "id": 4719,
                      "faiss_score": 0.9455520510673523,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 6.933759689331055,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 7.879311740398407
                    },
                    {
                      "id": 4795,
                      "faiss_score": 0.9083656072616577,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 4.952396392822266,
                      "rerank_rank": 4,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.860762000083923
                    }
                  ]
                }
              },
              {
                "subclaim": "Quantum error correction adds significant overhead",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 764,
                      "faiss_score": 0.9263245463371277,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 168,
                      "sentence": "However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 5.299180507659912,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.22550505399704
                    },
                    {
                      "id": 4719,
                      "faiss_score": 0.9596387147903442,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.731478214263916,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.69111692905426
                    },
                    {
                      "id": 6553,
                      "faiss_score": 0.9569774270057678,
                      "faiss_rank": 3,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": -0.2708771228790283,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.6861003041267395
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Future architectures will eliminate the need for large datasets in machine learning.",
      "expected_verdict": "INCONCLUSIVE",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Future architectures will eliminate the need for large datasets in machine learning.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6124,
                "faiss_score": 0.8617883920669556,
                "faiss_rank": 16,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -1.6935912370681763,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.8318028450012207
              },
              {
                "id": 2944,
                "faiss_score": 0.8668415546417236,
                "faiss_rank": 9,
                "doc_id": "wiki_Self-supervised_learning",
                "file_type": ".txt",
                "position": 53,
                "sentence": "Its ability to leverage unlabeled data effectively opens new possibilities for advancement in machine learning, especially in data-driven application domains.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Self-supervised_learning",
                "primary_category": "machine learning",
                "rerank_score": -2.1371984481811523,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.2703568935394287
              },
              {
                "id": 349,
                "faiss_score": 0.8643662333488464,
                "faiss_rank": 13,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 240,
                "sentence": "Typically, machine learning models require a high quantity of reliable data to perform accurate predictions.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -2.83878231048584,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.9744160771369934
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Future architectures will eliminate the need for large datasets in machine learning.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6124,
                      "faiss_score": 0.8617883920669556,
                      "faiss_rank": 16,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -1.6935912370681763,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.8318028450012207
                    },
                    {
                      "id": 2944,
                      "faiss_score": 0.8668415546417236,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Self-supervised_learning",
                      "file_type": ".txt",
                      "position": 53,
                      "sentence": "Its ability to leverage unlabeled data effectively opens new possibilities for advancement in machine learning, especially in data-driven application domains.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Self-supervised_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -2.1371984481811523,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.2703568935394287
                    },
                    {
                      "id": 349,
                      "faiss_score": 0.8643662333488464,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 240,
                      "sentence": "Typically, machine learning models require a high quantity of reliable data to perform accurate predictions.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -2.83878231048584,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.9744160771369934
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "A single algorithm can optimally solve all machine learning problems.",
      "expected_verdict": "INCONCLUSIVE",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "A single algorithm can optimally solve all machine learning problems.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 331,
                "faiss_score": 0.8863186836242676,
                "faiss_rank": 5,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 222,
                "sentence": "Efficient algorithms exist that perform inference and learning.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -0.2585304081439972,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.6277882754802704
              },
              {
                "id": 267,
                "faiss_score": 0.8847181797027588,
                "faiss_rank": 9,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 158,
                "sentence": "This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -0.3544308841228485,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.5302872955799103
              },
              {
                "id": 1558,
                "faiss_score": 0.8794730305671692,
                "faiss_rank": 14,
                "doc_id": "wiki_Optimization_(mathematics)",
                "file_type": ".txt",
                "position": 7,
                "sentence": "In machine learning, it is always necessary to continuously evaluate the quality of a data model by using a cost function where a minimum implies a set of possibly optimal parameters with an optimal (lowest) error.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                "primary_category": "all articles with style issues",
                "rerank_score": -0.5462067127227783,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.33326631784439087
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "A single algorithm can optimally solve all machine learning problems.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 331,
                      "faiss_score": 0.8863186836242676,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 222,
                      "sentence": "Efficient algorithms exist that perform inference and learning.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -0.2585304081439972,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.6277882754802704
                    },
                    {
                      "id": 267,
                      "faiss_score": 0.8847181797027588,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 158,
                      "sentence": "This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -0.3544308841228485,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.5302872955799103
                    },
                    {
                      "id": 1558,
                      "faiss_score": 0.8794730305671692,
                      "faiss_rank": 14,
                      "doc_id": "wiki_Optimization_(mathematics)",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "In machine learning, it is always necessary to continuously evaluate the quality of a data model by using a cost function where a minimum implies a set of possibly optimal parameters with an optimal (lowest) error.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                      "primary_category": "all articles with style issues",
                      "rerank_score": -0.5462067127227783,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.33326631784439087
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Quantum computers will replace classical computers for most workloads.",
      "expected_verdict": "INCONCLUSIVE",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Quantum computers will replace classical computers for most workloads.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6574,
                "faiss_score": 0.9328113794326782,
                "faiss_rank": 3,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 34,
                "sentence": "Rather than replacing classical systems, quantum computers are expected to act as accelerators for specific subroutines.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 5.315627098083496,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.248438477516174
              },
              {
                "id": 736,
                "faiss_score": 0.9381494522094727,
                "faiss_rank": 1,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 140,
                "sentence": "As of 2023, classical computers outperform quantum computers for all real-world applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.903462886810303,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.841612339019775
              },
              {
                "id": 6594,
                "faiss_score": 0.9081912636756897,
                "faiss_rank": 11,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 54,
                "sentence": "Ultimately, quantum computing represents a long-term research effort rather than a near-term replacement for classical computation.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 3.555327892303467,
                "rerank_rank": 4,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.4635191559791565
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Quantum computers will replace classical computers for most workloads.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6574,
                      "faiss_score": 0.9328113794326782,
                      "faiss_rank": 3,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 34,
                      "sentence": "Rather than replacing classical systems, quantum computers are expected to act as accelerators for specific subroutines.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 5.315627098083496,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.248438477516174
                    },
                    {
                      "id": 736,
                      "faiss_score": 0.9381494522094727,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 140,
                      "sentence": "As of 2023, classical computers outperform quantum computers for all real-world applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.903462886810303,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.841612339019775
                    },
                    {
                      "id": 6594,
                      "faiss_score": 0.9081912636756897,
                      "faiss_rank": 11,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 54,
                      "sentence": "Ultimately, quantum computing represents a long-term research effort rather than a near-term replacement for classical computation.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 3.555327892303467,
                      "rerank_rank": 4,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.4635191559791565
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Increasing data quality is more important than model size for all tasks.",
      "expected_verdict": "INCONCLUSIVE",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Increasing data quality is more important than model size for all tasks.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6137,
                "faiss_score": 0.9282467365264893,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 6.42487907409668,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 7.353125810623169
              },
              {
                "id": 6147,
                "faiss_score": 0.8930780291557312,
                "faiss_rank": 7,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 23,
                "sentence": "For example, increasing model size without increasing data may yield limited benefits, while increasing data without sufficient model capacity may fail to exploit the additional information.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.379410743713379,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.27248877286911
              },
              {
                "id": 5767,
                "faiss_score": 0.9157058596611023,
                "faiss_rank": 2,
                "doc_id": "local_math_information_theory_and_learning.txt",
                "file_type": ".txt",
                "position": 15,
                "sentence": "This observation emphasizes the importance of data quality and task definition.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_information_theory_and_learning.txt",
                "primary_category": null,
                "rerank_score": -0.11810709536075592,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.7975987643003464
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Increasing data quality is more important than model size for all tasks.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6137,
                      "faiss_score": 0.9282467365264893,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 6.42487907409668,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 7.353125810623169
                    },
                    {
                      "id": 6147,
                      "faiss_score": 0.8930780291557312,
                      "faiss_rank": 7,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 23,
                      "sentence": "For example, increasing model size without increasing data may yield limited benefits, while increasing data without sufficient model capacity may fail to exploit the additional information.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.379410743713379,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.27248877286911
                    },
                    {
                      "id": 5767,
                      "faiss_score": 0.9157058596611023,
                      "faiss_rank": 2,
                      "doc_id": "local_math_information_theory_and_learning.txt",
                      "file_type": ".txt",
                      "position": 15,
                      "sentence": "This observation emphasizes the importance of data quality and task definition.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_information_theory_and_learning.txt",
                      "primary_category": null,
                      "rerank_score": -0.11810709536075592,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.7975987643003464
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Scaling large language models improves performance, enables emergent abilities, and supports zero-shot learning, but increases training cost, energy consumption, optimization instability, and diminishing returns.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Scaling large language models improves performance",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6124,
                "faiss_score": 0.877862274646759,
                "faiss_rank": 16,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 5.232515811920166,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.110378086566925
              },
              {
                "id": 6043,
                "faiss_score": 0.9281105995178223,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 5.195529460906982,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.123640060424805
              },
              {
                "id": 2020,
                "faiss_score": 0.9002507925033569,
                "faiss_rank": 7,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 4.025500774383545,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.925751566886902
              }
            ]
          }
        },
        {
          "subclaim": "Scaling large language models enables emergent abilities",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6043,
                "faiss_score": 0.9116201400756836,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.754950761795044,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.6665709018707275
              },
              {
                "id": 2020,
                "faiss_score": 0.8869941234588623,
                "faiss_rank": 7,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 1.3232548236846924,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.2102489471435547
              },
              {
                "id": 1940,
                "faiss_score": 0.8720008134841919,
                "faiss_rank": 18,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 208,
                "sentence": "argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 0.9061471223831177,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.7781479358673096
              }
            ]
          }
        },
        {
          "subclaim": "Scaling large language models supports zero-shot learning",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6043,
                "faiss_score": 0.8965744376182556,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.581221580505371,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.4777960181236267
              },
              {
                "id": 2020,
                "faiss_score": 0.8652899861335754,
                "faiss_rank": 5,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 0.850409209728241,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.7156991958618164
              },
              {
                "id": 6040,
                "faiss_score": 0.8675651550292969,
                "faiss_rank": 4,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 0.335788369178772,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.2033535242080688
              }
            ]
          }
        },
        {
          "subclaim": "Scaling large language models increases training cost",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6092,
                "faiss_score": 0.9561026096343994,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 52,
                "sentence": "The computational cost of training large language models is substantial.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 4.304480075836182,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.260582685470581
              },
              {
                "id": 1789,
                "faiss_score": 0.8806159496307373,
                "faiss_rank": 13,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 57,
                "sentence": "Training of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 1.4576606750488281,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.3382766246795654
              },
              {
                "id": 6124,
                "faiss_score": 0.8770360350608826,
                "faiss_rank": 17,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 1.055026650428772,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.9320626854896545
              }
            ]
          }
        },
        {
          "subclaim": "Scaling large language models increases energy consumption",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6043,
                "faiss_score": 0.8894945979118347,
                "faiss_rank": 4,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.1702418327331543,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.059736430644989
              },
              {
                "id": 5907,
                "faiss_score": 0.8961602449417114,
                "faiss_rank": 3,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 2,
                "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 0.8276621699333191,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.7238224148750305
              },
              {
                "id": 2020,
                "faiss_score": 0.8737888336181641,
                "faiss_rank": 13,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 0.13212397694587708,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.0059128105640411
              }
            ]
          }
        },
        {
          "subclaim": "Scaling large language models causes optimization instability",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 2020,
                "faiss_score": 0.9003187417984009,
                "faiss_rank": 3,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 0.8239505887031555,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.7242693305015564
              },
              {
                "id": 6043,
                "faiss_score": 0.9122200608253479,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -0.08233976364135742,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.8298802971839905
              },
              {
                "id": 2037,
                "faiss_score": 0.8899509906768799,
                "faiss_rank": 8,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 305,
                "sentence": "This phenomenon undermines the reliability of large language models in multiple-choice settings.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -1.1177924871444702,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.22784149646759033
              }
            ]
          }
        },
        {
          "subclaim": "Scaling large language models exhibits diminishing returns",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 2020,
                "faiss_score": 0.9000077843666077,
                "faiss_rank": 6,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 3.5399813652038574,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.439989149570465
              },
              {
                "id": 6043,
                "faiss_score": 0.924456000328064,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.306876540184021,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.231332540512085
              },
              {
                "id": 6121,
                "faiss_score": 0.9107099771499634,
                "faiss_rank": 3,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -0.2476663887500763,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.6630435883998871
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Scaling large language models improves performance",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6043,
                      "faiss_score": 0.9281105995178223,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 5.195529460906982,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.123640060424805
                    },
                    {
                      "id": 6124,
                      "faiss_score": 0.877862274646759,
                      "faiss_rank": 16,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 5.232515811920166,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.110378086566925
                    },
                    {
                      "id": 2020,
                      "faiss_score": 0.9002507925033569,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 4.025500774383545,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.925751566886902
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling large language models enables emergent abilities",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6043,
                      "faiss_score": 0.9116201400756836,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.754950761795044,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.6665709018707275
                    },
                    {
                      "id": 2020,
                      "faiss_score": 0.8869941234588623,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 1.3232548236846924,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.2102489471435547
                    },
                    {
                      "id": 1940,
                      "faiss_score": 0.8720008134841919,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 208,
                      "sentence": "argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 0.9061471223831177,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.7781479358673096
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling large language models supports zero-shot learning",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6043,
                      "faiss_score": 0.8965744376182556,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.581221580505371,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.4777960181236267
                    },
                    {
                      "id": 2020,
                      "faiss_score": 0.8652899861335754,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 0.850409209728241,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.7156991958618164
                    },
                    {
                      "id": 6040,
                      "faiss_score": 0.8675651550292969,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 0.335788369178772,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.2033535242080688
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling large language models increases training cost",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6092,
                      "faiss_score": 0.9561026096343994,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 52,
                      "sentence": "The computational cost of training large language models is substantial.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 4.304480075836182,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.260582685470581
                    },
                    {
                      "id": 1789,
                      "faiss_score": 0.8806159496307373,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 57,
                      "sentence": "Training of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 1.4576606750488281,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.3382766246795654
                    },
                    {
                      "id": 6124,
                      "faiss_score": 0.8770360350608826,
                      "faiss_rank": 17,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 1.055026650428772,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.9320626854896545
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling large language models increases energy consumption",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6043,
                      "faiss_score": 0.8894945979118347,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.1702418327331543,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.059736430644989
                    },
                    {
                      "id": 5907,
                      "faiss_score": 0.8961602449417114,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 0.8276621699333191,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.7238224148750305
                    },
                    {
                      "id": 2020,
                      "faiss_score": 0.8737888336181641,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 0.13212397694587708,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.0059128105640411
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling large language models causes optimization instability",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 2020,
                      "faiss_score": 0.9003187417984009,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 0.8239505887031555,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.7242693305015564
                    },
                    {
                      "id": 6043,
                      "faiss_score": 0.9122200608253479,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -0.08233976364135742,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.8298802971839905
                    },
                    {
                      "id": 2037,
                      "faiss_score": 0.8899509906768799,
                      "faiss_rank": 8,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 305,
                      "sentence": "This phenomenon undermines the reliability of large language models in multiple-choice settings.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -1.1177924871444702,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.22784149646759033
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling large language models exhibits diminishing returns",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 2020,
                      "faiss_score": 0.9000077843666077,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 3.5399813652038574,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.439989149570465
                    },
                    {
                      "id": 6043,
                      "faiss_score": 0.924456000328064,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.306876540184021,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.231332540512085
                    },
                    {
                      "id": 6121,
                      "faiss_score": 0.9107099771499634,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -0.2476663887500763,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.6630435883998871
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Transformer architectures improve parallelization, capture long-range dependencies, and scale efficiently, but require large compute budgets, are sensitive to hyperparameters, and struggle with very long contexts.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Transformer architectures improve parallelization",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6273,
                "faiss_score": 0.8840447664260864,
                "faiss_rank": 19,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 12,
                "sentence": "The transformer architecture generalized this concept by eliminating recurrence entirely and relying solely on attention mechanisms to model relationships within a sequence.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 1.5003280639648438,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.38437283039093
              },
              {
                "id": 6261,
                "faiss_score": 0.8891482353210449,
                "faiss_rank": 10,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Transformer-based neural network architectures emerged from a broader effort to overcome structural limitations present in earlier approaches to sequence modeling.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 1.3882910013198853,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.27743923664093
              },
              {
                "id": 1807,
                "faiss_score": 0.8870072960853577,
                "faiss_rank": 14,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 75,
                "sentence": "LLMs are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 1.3345541954040527,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.2215614914894104
              }
            ]
          }
        },
        {
          "subclaim": "Transformer architectures capture long-range dependencies",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6296,
                "faiss_score": 0.8920329809188843,
                "faiss_rank": 8,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 35,
                "sentence": "The transformer architecture proved highly effective across a wide range of natural language processing tasks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -0.8406956791877747,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.05133730173110962
              },
              {
                "id": 1807,
                "faiss_score": 0.9011234641075134,
                "faiss_rank": 2,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 75,
                "sentence": "LLMs are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -1.2148985862731934,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.31377512216567993
              },
              {
                "id": 3083,
                "faiss_score": 0.8960912823677063,
                "faiss_rank": 4,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 138,
                "sentence": "The transformer architecture is constructed to calculate output tokens iteratively.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": -1.5020763874053955,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.6059851050376892
              }
            ]
          }
        },
        {
          "subclaim": "Transformer architectures scale efficiently",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6351,
                "faiss_score": 0.9340454339981079,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 90,
                "sentence": "The architecture of transformers lends itself well to these approaches, but achieving efficient scaling still requires careful engineering.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 5.214434623718262,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.14848005771637
              },
              {
                "id": 6296,
                "faiss_score": 0.8871057629585266,
                "faiss_rank": 12,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 35,
                "sentence": "The transformer architecture proved highly effective across a wide range of natural language processing tasks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 2.623847007751465,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.5109527707099915
              },
              {
                "id": 6333,
                "faiss_score": 0.8859295845031738,
                "faiss_rank": 16,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 72,
                "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 2.3689064979553223,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.254836082458496
              }
            ]
          }
        },
        {
          "subclaim": "Transformer architectures require large compute budgets",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 3083,
                "faiss_score": 0.9009952545166016,
                "faiss_rank": 4,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 138,
                "sentence": "The transformer architecture is constructed to calculate output tokens iteratively.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": -1.3453997373580933,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.4444044828414917
              },
              {
                "id": 6354,
                "faiss_score": 0.9239134192466736,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 93,
                "sentence": "Transformer-based models can be computationally expensive at inference time, particularly when generating long outputs or processing large batches.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -1.5679820775985718,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.6440686583518982
              },
              {
                "id": 1759,
                "faiss_score": 0.8875057697296143,
                "faiss_rank": 16,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 27,
                "sentence": "As of 2024, the largest and most capable models are all based on the transformer architecture.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -2.110743999481201,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.223238229751587
              }
            ]
          }
        },
        {
          "subclaim": "Transformer architectures are sensitive to hyperparameters",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6375,
                "faiss_score": 0.9090120196342468,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 114,
                "sentence": "Transformer-based models can be sensitive to adversarial inputs, distribution shifts, and subtle perturbations.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 2.766561508178711,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.6755735278129578
              },
              {
                "id": 6333,
                "faiss_score": 0.8922982215881348,
                "faiss_rank": 10,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 72,
                "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -2.543890953063965,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.65159273147583
              },
              {
                "id": 6261,
                "faiss_score": 0.8909812569618225,
                "faiss_rank": 14,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Transformer-based neural network architectures emerged from a broader effort to overcome structural limitations present in earlier approaches to sequence modeling.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -2.7282087802886963,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.8372275233268738
              }
            ]
          }
        },
        {
          "subclaim": "Transformer architectures struggle with very long contexts",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 2997,
                "faiss_score": 0.8859200477600098,
                "faiss_rank": 17,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 52,
                "sentence": "The plain transformer architecture had difficulty in converging.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": -0.9928778409957886,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.10695779323577881
              },
              {
                "id": 6333,
                "faiss_score": 0.8825433850288391,
                "faiss_rank": 20,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 72,
                "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -1.6507289409637451,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.768185555934906
              },
              {
                "id": 6426,
                "faiss_score": 0.9080196022987366,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 165,
                "sentence": "This interdependence complicates efforts to simplify or interpret transformer architectures.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -2.7841544151306152,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.8761348128318787
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Transformer architectures improve parallelization",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6273,
                      "faiss_score": 0.8840447664260864,
                      "faiss_rank": 19,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 12,
                      "sentence": "The transformer architecture generalized this concept by eliminating recurrence entirely and relying solely on attention mechanisms to model relationships within a sequence.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 1.5003280639648438,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.38437283039093
                    },
                    {
                      "id": 6261,
                      "faiss_score": 0.8891482353210449,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Transformer-based neural network architectures emerged from a broader effort to overcome structural limitations present in earlier approaches to sequence modeling.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 1.3882910013198853,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.27743923664093
                    },
                    {
                      "id": 1807,
                      "faiss_score": 0.8870072960853577,
                      "faiss_rank": 14,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "LLMs are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 1.3345541954040527,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.2215614914894104
                    }
                  ]
                }
              },
              {
                "subclaim": "Transformer architectures capture long-range dependencies",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6296,
                      "faiss_score": 0.8920329809188843,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 35,
                      "sentence": "The transformer architecture proved highly effective across a wide range of natural language processing tasks.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -0.8406956791877747,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.05133730173110962
                    },
                    {
                      "id": 1807,
                      "faiss_score": 0.9011234641075134,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "LLMs are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -1.2148985862731934,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.31377512216567993
                    },
                    {
                      "id": 3083,
                      "faiss_score": 0.8960912823677063,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 138,
                      "sentence": "The transformer architecture is constructed to calculate output tokens iteratively.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": -1.5020763874053955,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.6059851050376892
                    }
                  ]
                }
              },
              {
                "subclaim": "Transformer architectures scale efficiently",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6351,
                      "faiss_score": 0.9340454339981079,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 90,
                      "sentence": "The architecture of transformers lends itself well to these approaches, but achieving efficient scaling still requires careful engineering.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 5.214434623718262,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.14848005771637
                    },
                    {
                      "id": 6296,
                      "faiss_score": 0.8871057629585266,
                      "faiss_rank": 12,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 35,
                      "sentence": "The transformer architecture proved highly effective across a wide range of natural language processing tasks.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 2.623847007751465,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.5109527707099915
                    },
                    {
                      "id": 6333,
                      "faiss_score": 0.8859295845031738,
                      "faiss_rank": 16,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 72,
                      "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 2.3689064979553223,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.254836082458496
                    }
                  ]
                }
              },
              {
                "subclaim": "Transformer architectures require large compute budgets",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 3083,
                      "faiss_score": 0.9009952545166016,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 138,
                      "sentence": "The transformer architecture is constructed to calculate output tokens iteratively.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": -1.3453997373580933,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.4444044828414917
                    },
                    {
                      "id": 6354,
                      "faiss_score": 0.9239134192466736,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 93,
                      "sentence": "Transformer-based models can be computationally expensive at inference time, particularly when generating long outputs or processing large batches.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -1.5679820775985718,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.6440686583518982
                    },
                    {
                      "id": 1759,
                      "faiss_score": 0.8875057697296143,
                      "faiss_rank": 16,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 27,
                      "sentence": "As of 2024, the largest and most capable models are all based on the transformer architecture.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -2.110743999481201,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.223238229751587
                    }
                  ]
                }
              },
              {
                "subclaim": "Transformer architectures are sensitive to hyperparameters",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6375,
                      "faiss_score": 0.9090120196342468,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 114,
                      "sentence": "Transformer-based models can be sensitive to adversarial inputs, distribution shifts, and subtle perturbations.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 2.766561508178711,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.6755735278129578
                    },
                    {
                      "id": 6333,
                      "faiss_score": 0.8922982215881348,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 72,
                      "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -2.543890953063965,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.65159273147583
                    },
                    {
                      "id": 6261,
                      "faiss_score": 0.8909812569618225,
                      "faiss_rank": 14,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Transformer-based neural network architectures emerged from a broader effort to overcome structural limitations present in earlier approaches to sequence modeling.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -2.7282087802886963,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.8372275233268738
                    }
                  ]
                }
              },
              {
                "subclaim": "Transformer architectures struggle with very long contexts",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 2997,
                      "faiss_score": 0.8859200477600098,
                      "faiss_rank": 17,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 52,
                      "sentence": "The plain transformer architecture had difficulty in converging.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": -0.9928778409957886,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.10695779323577881
                    },
                    {
                      "id": 6333,
                      "faiss_score": 0.8825433850288391,
                      "faiss_rank": 20,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 72,
                      "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -1.6507289409637451,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.768185555934906
                    },
                    {
                      "id": 6426,
                      "faiss_score": 0.9080196022987366,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 165,
                      "sentence": "This interdependence complicates efforts to simplify or interpret transformer architectures.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -2.7841544151306152,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.8761348128318787
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Distributed systems improve scalability, availability, and fault tolerance, but introduce coordination overhead, consistency challenges, debugging difficulty, and increased system complexity.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Distributed systems improve scalability",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5658,
                "faiss_score": 0.9028811454772949,
                "faiss_rank": 9,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 61,
                "sentence": "Eventually consistent systems illustrate how relaxing guarantees can improve scalability.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 2.7626805305480957,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.6655616760253906
              },
              {
                "id": 499,
                "faiss_score": 0.9017831683158875,
                "faiss_rank": 10,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 75,
                "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.7927249670028687,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.694508135318756
              },
              {
                "id": 445,
                "faiss_score": 0.9056973457336426,
                "faiss_rank": 4,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.2563084363937378,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.1620057821273804
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems improve availability",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 3817,
                "faiss_score": 0.8974882364273071,
                "faiss_rank": 11,
                "doc_id": "wiki_CAP_theorem",
                "file_type": ".txt",
                "position": 8,
                "sentence": "Most modern distributed databases offer configuration options for both consistency and availability.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/CAP_theorem",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 2.578249454498291,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.475737690925598
              },
              {
                "id": 445,
                "faiss_score": 0.8953639268875122,
                "faiss_rank": 14,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.510157585144043,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.405521512031555
              },
              {
                "id": 499,
                "faiss_score": 0.8946641087532043,
                "faiss_rank": 18,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 75,
                "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.4583913087844849,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.353055417537689
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems improve fault tolerance",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 568,
                "faiss_score": 0.9054936170578003,
                "faiss_rank": 9,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 144,
                "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 4.992817401885986,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.898311018943787
              },
              {
                "id": 430,
                "faiss_score": 0.9204719066619873,
                "faiss_rank": 1,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 6,
                "sentence": "Other typical properties of distributed systems are: The system must tolerate failures in individual computers.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 2.3290367126464844,
                "rerank_rank": 4,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.2495086193084717
              },
              {
                "id": 493,
                "faiss_score": 0.8894362449645996,
                "faiss_rank": 19,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 69,
                "sentence": "Cell-based architecture has been adopted in some large-scale distributed systems, particularly in cloud-native and high-availability environments, where fault isolation and redundancy are key design considerations.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 2.1658031940460205,
                "rerank_rank": 7,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.05523943901062
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems introduce coordination overhead",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6685,
                "faiss_score": 0.9233837127685547,
                "faiss_rank": 1,
                "doc_id": "local_systems_latency_throughput_tradeoffs.txt",
                "file_type": ".txt",
                "position": 17,
                "sentence": "However, parallelism introduces coordination overhead, synchronization costs, and contention for shared resources.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_latency_throughput_tradeoffs.txt",
                "primary_category": null,
                "rerank_score": 4.450502395629883,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.3738861083984375
              },
              {
                "id": 585,
                "faiss_score": 0.9145564436912537,
                "faiss_rank": 2,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 161,
                "sentence": "In order to perform coordination, distributed systems employ the concept of coordinators.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 3.5947370529174805,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.509293496608734
              },
              {
                "id": 5633,
                "faiss_score": 0.9139895439147949,
                "faiss_rank": 5,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 36,
                "sentence": "However, scaling introduces coordination overhead that can limit achievable gains.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 2.998728036880493,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.912717580795288
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems introduce consistency challenges",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5612,
                "faiss_score": 0.9273715019226074,
                "faiss_rank": 2,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 15,
                "sentence": "Strong consistency models aim to make distributed systems behave as if there were a single shared state, but enforcing such behavior requires coordination and synchronization, which can be expensive or impossible under certain failure conditions.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 4.537818908691406,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.465190410614014
              },
              {
                "id": 5610,
                "faiss_score": 0.9464955925941467,
                "faiss_rank": 1,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Consistency is a central concept in distributed systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 4.305271148681641,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.251766741275787
              },
              {
                "id": 5600,
                "faiss_score": 0.8907561302185059,
                "faiss_rank": 16,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "Achieving this illusion of coherence in the presence of failures, delays, and partial information is the central challenge of distributed systems design.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 4.005821228027344,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.89657735824585
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems introduce debugging difficulty",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5644,
                "faiss_score": 0.9589457511901855,
                "faiss_rank": 1,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 47,
                "sentence": "Because failures and performance issues may arise from interactions between components, debugging distributed systems is notoriously difficult.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 6.9515380859375,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 7.9104838371276855
              },
              {
                "id": 568,
                "faiss_score": 0.8847370743751526,
                "faiss_rank": 15,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 144,
                "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": -0.7992189526557922,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.08551812171936035
              },
              {
                "id": 5634,
                "faiss_score": 0.9050875306129456,
                "faiss_rank": 2,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 37,
                "sentence": "State management is particularly challenging in distributed systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": -1.3250395059585571,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.4199519753456116
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems introduce increased system complexity",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5674,
                "faiss_score": 0.9300456047058105,
                "faiss_rank": 1,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 77,
                "sentence": "Distributed systems exemplify the broader theme that complexity emerges from interaction.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 4.361238479614258,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.291284084320068
              },
              {
                "id": 6234,
                "faiss_score": 0.8932847380638123,
                "faiss_rank": 12,
                "doc_id": "local_ml_training_dynamics.txt",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Distributed training introduces additional complexity into training dynamics.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                "primary_category": null,
                "rerank_score": 3.379138708114624,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.272423446178436
              },
              {
                "id": 5597,
                "faiss_score": 0.8919234275817871,
                "faiss_rank": 13,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": -0.5184941291809082,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.3734292984008789
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Distributed systems improve scalability",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5658,
                      "faiss_score": 0.9028811454772949,
                      "faiss_rank": 9,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 61,
                      "sentence": "Eventually consistent systems illustrate how relaxing guarantees can improve scalability.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 2.7626805305480957,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.6655616760253906
                    },
                    {
                      "id": 499,
                      "faiss_score": 0.9017831683158875,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.7927249670028687,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.694508135318756
                    },
                    {
                      "id": 445,
                      "faiss_score": 0.9056973457336426,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.2563084363937378,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.1620057821273804
                    }
                  ]
                }
              },
              {
                "subclaim": "Distributed systems improve availability",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 3817,
                      "faiss_score": 0.8974882364273071,
                      "faiss_rank": 11,
                      "doc_id": "wiki_CAP_theorem",
                      "file_type": ".txt",
                      "position": 8,
                      "sentence": "Most modern distributed databases offer configuration options for both consistency and availability.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/CAP_theorem",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 2.578249454498291,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.475737690925598
                    },
                    {
                      "id": 445,
                      "faiss_score": 0.8953639268875122,
                      "faiss_rank": 14,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.510157585144043,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.405521512031555
                    },
                    {
                      "id": 499,
                      "faiss_score": 0.8946641087532043,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.4583913087844849,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.353055417537689
                    }
                  ]
                }
              },
              {
                "subclaim": "Distributed systems improve fault tolerance",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 568,
                      "faiss_score": 0.9054936170578003,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 144,
                      "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 4.992817401885986,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.898311018943787
                    },
                    {
                      "id": 430,
                      "faiss_score": 0.9204719066619873,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 6,
                      "sentence": "Other typical properties of distributed systems are: The system must tolerate failures in individual computers.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 2.3290367126464844,
                      "rerank_rank": 4,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.2495086193084717
                    },
                    {
                      "id": 493,
                      "faiss_score": 0.8894362449645996,
                      "faiss_rank": 19,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 69,
                      "sentence": "Cell-based architecture has been adopted in some large-scale distributed systems, particularly in cloud-native and high-availability environments, where fault isolation and redundancy are key design considerations.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 2.1658031940460205,
                      "rerank_rank": 7,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.05523943901062
                    }
                  ]
                }
              },
              {
                "subclaim": "Distributed systems introduce coordination overhead",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6685,
                      "faiss_score": 0.9233837127685547,
                      "faiss_rank": 1,
                      "doc_id": "local_systems_latency_throughput_tradeoffs.txt",
                      "file_type": ".txt",
                      "position": 17,
                      "sentence": "However, parallelism introduces coordination overhead, synchronization costs, and contention for shared resources.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_latency_throughput_tradeoffs.txt",
                      "primary_category": null,
                      "rerank_score": 4.450502395629883,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.3738861083984375
                    },
                    {
                      "id": 585,
                      "faiss_score": 0.9145564436912537,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 161,
                      "sentence": "In order to perform coordination, distributed systems employ the concept of coordinators.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 3.5947370529174805,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.509293496608734
                    },
                    {
                      "id": 5633,
                      "faiss_score": 0.9139895439147949,
                      "faiss_rank": 5,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 36,
                      "sentence": "However, scaling introduces coordination overhead that can limit achievable gains.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 2.998728036880493,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.912717580795288
                    }
                  ]
                }
              },
              {
                "subclaim": "Distributed systems introduce consistency challenges",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5612,
                      "faiss_score": 0.9273715019226074,
                      "faiss_rank": 2,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 15,
                      "sentence": "Strong consistency models aim to make distributed systems behave as if there were a single shared state, but enforcing such behavior requires coordination and synchronization, which can be expensive or impossible under certain failure conditions.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 4.537818908691406,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.465190410614014
                    },
                    {
                      "id": 5610,
                      "faiss_score": 0.9464955925941467,
                      "faiss_rank": 1,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Consistency is a central concept in distributed systems.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 4.305271148681641,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.251766741275787
                    },
                    {
                      "id": 5600,
                      "faiss_score": 0.8907561302185059,
                      "faiss_rank": 16,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "Achieving this illusion of coherence in the presence of failures, delays, and partial information is the central challenge of distributed systems design.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 4.005821228027344,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.89657735824585
                    }
                  ]
                }
              },
              {
                "subclaim": "Distributed systems introduce debugging difficulty",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5644,
                      "faiss_score": 0.9589457511901855,
                      "faiss_rank": 1,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 47,
                      "sentence": "Because failures and performance issues may arise from interactions between components, debugging distributed systems is notoriously difficult.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 6.9515380859375,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 7.9104838371276855
                    },
                    {
                      "id": 568,
                      "faiss_score": 0.8847370743751526,
                      "faiss_rank": 15,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 144,
                      "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": -0.7992189526557922,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.08551812171936035
                    },
                    {
                      "id": 5634,
                      "faiss_score": 0.9050875306129456,
                      "faiss_rank": 2,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 37,
                      "sentence": "State management is particularly challenging in distributed systems.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": -1.3250395059585571,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.4199519753456116
                    }
                  ]
                }
              },
              {
                "subclaim": "Distributed systems introduce increased system complexity",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5674,
                      "faiss_score": 0.9300456047058105,
                      "faiss_rank": 1,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 77,
                      "sentence": "Distributed systems exemplify the broader theme that complexity emerges from interaction.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 4.361238479614258,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.291284084320068
                    },
                    {
                      "id": 6234,
                      "faiss_score": 0.8932847380638123,
                      "faiss_rank": 12,
                      "doc_id": "local_ml_training_dynamics.txt",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Distributed training introduces additional complexity into training dynamics.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                      "primary_category": null,
                      "rerank_score": 3.379138708114624,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.272423446178436
                    },
                    {
                      "id": 5597,
                      "faiss_score": 0.8919234275817871,
                      "faiss_rank": 13,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": -0.5184941291809082,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.3734292984008789
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Increasing dataset size improves model generalization, training stability, and robustness, but data collection is expensive, labeling is costly, noisy data degrades performance, and returns diminish beyond scale.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Increasing dataset size improves model generalization",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6137,
                "faiss_score": 0.9258404970169067,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 5.599934101104736,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.525774598121643
              },
              {
                "id": 1453,
                "faiss_score": 0.911958634853363,
                "faiss_rank": 3,
                "doc_id": "wiki_Regularization_(mathematics)",
                "file_type": ".txt",
                "position": 25,
                "sentence": "By regularizing for time, model complexity can be controlled, improving generalization.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
                "primary_category": "articles with short description",
                "rerank_score": 2.2767534255981445,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.1887120604515076
              },
              {
                "id": 5906,
                "faiss_score": 0.8898252248764038,
                "faiss_rank": 15,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 2.2275302410125732,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.117355465888977
              }
            ]
          }
        },
        {
          "subclaim": "Increasing dataset size improves training stability",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 2613,
                "faiss_score": 0.8831011056900024,
                "faiss_rank": 10,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 208,
                "sentence": "Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 3.4986326694488525,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.381733775138855
              },
              {
                "id": 1785,
                "faiss_score": 0.8818594813346863,
                "faiss_rank": 12,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 53,
                "sentence": "Cleaned datasets can increase training efficiency and lead to improved downstream performance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 3.4402031898498535,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.32206267118454
              },
              {
                "id": 6133,
                "faiss_score": 0.8990232944488525,
                "faiss_rank": 3,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 2.8726301193237305,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.771653413772583
              }
            ]
          }
        },
        {
          "subclaim": "Increasing dataset size improves robustness",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6137,
                "faiss_score": 0.9264270067214966,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 3.0166993141174316,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.9431263208389282
              },
              {
                "id": 6133,
                "faiss_score": 0.8859886527061462,
                "faiss_rank": 5,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 1.7032674551010132,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.5892561078071594
              },
              {
                "id": 6147,
                "faiss_score": 0.8822609782218933,
                "faiss_rank": 7,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 23,
                "sentence": "For example, increasing model size without increasing data may yield limited benefits, while increasing data without sufficient model capacity may fail to exploit the additional information.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 0.8042117357254028,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.6864727139472961
              }
            ]
          }
        },
        {
          "subclaim": "Data collection is expensive",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6500,
                "faiss_score": 0.8686016201972961,
                "faiss_rank": 1,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 29,
                "sentence": "Accessing data from memory is often more expensive in terms of time and energy than performing arithmetic operations.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": 3.566348075866699,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.434949696063995
              },
              {
                "id": 1387,
                "faiss_score": 0.8628299832344055,
                "faiss_rank": 2,
                "doc_id": "wiki_Overfitting",
                "file_type": ".txt",
                "position": 37,
                "sentence": "Other negative consequences include: A function that is overfitted is likely to request more information about each item in the validation dataset than does the optimal function; gathering this additional unneeded data can be expensive or error-prone, especially if each individual piece of information must be gathered by human observation and manual data entry.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                "primary_category": "machine learning",
                "rerank_score": 0.6261616945266724,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.4889916777610779
              },
              {
                "id": 6354,
                "faiss_score": 0.8548440337181091,
                "faiss_rank": 10,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 93,
                "sentence": "Transformer-based models can be computationally expensive at inference time, particularly when generating long outputs or processing large batches.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -5.124258518218994,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.269414484500885
              }
            ]
          }
        },
        {
          "subclaim": "Labeling is costly",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 2318,
                "faiss_score": 0.8671115636825562,
                "faiss_rank": 1,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 249,
                "sentence": "This approach directly quantifies predictive performance but may be impractical when labels are delayed or costly to obtain.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": 4.585840702056885,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.452952265739441
              },
              {
                "id": 202,
                "faiss_score": 0.8283368945121765,
                "faiss_rank": 14,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 93,
                "sentence": "In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -0.7225422859191895,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.10579460859298706
              },
              {
                "id": 2426,
                "faiss_score": 0.8386695384979248,
                "faiss_rank": 7,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 21,
                "sentence": "This is an important benefit because unlabeled data is more abundant than the labeled data.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": -3.514857530593872,
                "rerank_rank": 4,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.6761879920959473
              }
            ]
          }
        },
        {
          "subclaim": "Noisy data degrades performance",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5998,
                "faiss_score": 0.9058985114097595,
                "faiss_rank": 1,
                "doc_id": "local_ml_evaluation_and_generalization.txt",
                "file_type": ".txt",
                "position": 24,
                "sentence": "When deployment data differs from training data, performance may degrade unpredictably.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                "primary_category": null,
                "rerank_score": 0.8202033042907715,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.726101815700531
              },
              {
                "id": 4087,
                "faiss_score": 0.8815248608589172,
                "faiss_rank": 7,
                "doc_id": "wiki_Information_theory",
                "file_type": ".txt",
                "position": 81,
                "sentence": "However, channels often fail to produce exact reconstruction of a signal; noise, periods of silence, and other forms of signal corruption often degrade quality.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Information_theory",
                "primary_category": "all articles needing additional references",
                "rerank_score": -0.8287366032600403,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.05278825759887695
              },
              {
                "id": 5808,
                "faiss_score": 0.8804681897163391,
                "faiss_rank": 8,
                "doc_id": "local_math_information_theory_and_learning.txt",
                "file_type": ".txt",
                "position": 56,
                "sentence": "Even with infinite computation and perfect optimization, performance is bounded by noise and ambiguity.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_information_theory_and_learning.txt",
                "primary_category": null,
                "rerank_score": -3.4648706912994385,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.5844025015830994
              }
            ]
          }
        },
        {
          "subclaim": "Returns diminish beyond scale",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6178,
                "faiss_score": 0.8914539217948914,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 54,
                "sentence": "While scaling has delivered consistent gains, it may encounter diminishing returns or external constraints that necessitate new approaches.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.174654960632324,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.066108882427216
              },
              {
                "id": 6133,
                "faiss_score": 0.8767734169960022,
                "faiss_rank": 2,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -8.22350025177002,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -7.346726834774017
              },
              {
                "id": 6186,
                "faiss_score": 0.8617356419563293,
                "faiss_rank": 3,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 62,
                "sentence": "Ultimately, scaling is a powerful but blunt tool.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -9.478944778442383,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -8.617209136486053
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Increasing dataset size improves model generalization",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6137,
                      "faiss_score": 0.9258404970169067,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 5.599934101104736,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.525774598121643
                    },
                    {
                      "id": 1453,
                      "faiss_score": 0.911958634853363,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Regularization_(mathematics)",
                      "file_type": ".txt",
                      "position": 25,
                      "sentence": "By regularizing for time, model complexity can be controlled, improving generalization.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
                      "primary_category": "articles with short description",
                      "rerank_score": 2.2767534255981445,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.1887120604515076
                    },
                    {
                      "id": 5906,
                      "faiss_score": 0.8898252248764038,
                      "faiss_rank": 15,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 1,
                      "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 2.2275302410125732,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.117355465888977
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing dataset size improves training stability",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 2613,
                      "faiss_score": 0.8831011056900024,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 208,
                      "sentence": "Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": 3.4986326694488525,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.381733775138855
                    },
                    {
                      "id": 1785,
                      "faiss_score": 0.8818594813346863,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 53,
                      "sentence": "Cleaned datasets can increase training efficiency and lead to improved downstream performance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 3.4402031898498535,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.32206267118454
                    },
                    {
                      "id": 6133,
                      "faiss_score": 0.8990232944488525,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 2.8726301193237305,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.771653413772583
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing dataset size improves robustness",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6137,
                      "faiss_score": 0.9264270067214966,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 3.0166993141174316,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.9431263208389282
                    },
                    {
                      "id": 6133,
                      "faiss_score": 0.8859886527061462,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 1.7032674551010132,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.5892561078071594
                    },
                    {
                      "id": 6147,
                      "faiss_score": 0.8822609782218933,
                      "faiss_rank": 7,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 23,
                      "sentence": "For example, increasing model size without increasing data may yield limited benefits, while increasing data without sufficient model capacity may fail to exploit the additional information.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 0.8042117357254028,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.6864727139472961
                    }
                  ]
                }
              },
              {
                "subclaim": "Data collection is expensive",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6500,
                      "faiss_score": 0.8686016201972961,
                      "faiss_rank": 1,
                      "doc_id": "local_physics_computation.txt",
                      "file_type": ".txt",
                      "position": 29,
                      "sentence": "Accessing data from memory is often more expensive in terms of time and energy than performing arithmetic operations.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                      "primary_category": null,
                      "rerank_score": 3.566348075866699,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.434949696063995
                    },
                    {
                      "id": 1387,
                      "faiss_score": 0.8628299832344055,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Overfitting",
                      "file_type": ".txt",
                      "position": 37,
                      "sentence": "Other negative consequences include: A function that is overfitted is likely to request more information about each item in the validation dataset than does the optimal function; gathering this additional unneeded data can be expensive or error-prone, especially if each individual piece of information must be gathered by human observation and manual data entry.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                      "primary_category": "machine learning",
                      "rerank_score": 0.6261616945266724,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.4889916777610779
                    },
                    {
                      "id": 6354,
                      "faiss_score": 0.8548440337181091,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 93,
                      "sentence": "Transformer-based models can be computationally expensive at inference time, particularly when generating long outputs or processing large batches.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -5.124258518218994,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -4.269414484500885
                    }
                  ]
                }
              },
              {
                "subclaim": "Labeling is costly",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 2318,
                      "faiss_score": 0.8671115636825562,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Artificial_neural_network",
                      "file_type": ".txt",
                      "position": 249,
                      "sentence": "This approach directly quantifies predictive performance but may be impractical when labels are delayed or costly to obtain.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                      "primary_category": "neural networks",
                      "rerank_score": 4.585840702056885,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.452952265739441
                    },
                    {
                      "id": 202,
                      "faiss_score": 0.8283368945121765,
                      "faiss_rank": 14,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 93,
                      "sentence": "In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -0.7225422859191895,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.10579460859298706
                    },
                    {
                      "id": 2426,
                      "faiss_score": 0.8386695384979248,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "This is an important benefit because unlabeled data is more abundant than the labeled data.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": -3.514857530593872,
                      "rerank_rank": 4,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.6761879920959473
                    }
                  ]
                }
              },
              {
                "subclaim": "Noisy data degrades performance",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5998,
                      "faiss_score": 0.9058985114097595,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_evaluation_and_generalization.txt",
                      "file_type": ".txt",
                      "position": 24,
                      "sentence": "When deployment data differs from training data, performance may degrade unpredictably.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                      "primary_category": null,
                      "rerank_score": 0.8202033042907715,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.726101815700531
                    },
                    {
                      "id": 4087,
                      "faiss_score": 0.8815248608589172,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Information_theory",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "However, channels often fail to produce exact reconstruction of a signal; noise, periods of silence, and other forms of signal corruption often degrade quality.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Information_theory",
                      "primary_category": "all articles needing additional references",
                      "rerank_score": -0.8287366032600403,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.05278825759887695
                    },
                    {
                      "id": 5808,
                      "faiss_score": 0.8804681897163391,
                      "faiss_rank": 8,
                      "doc_id": "local_math_information_theory_and_learning.txt",
                      "file_type": ".txt",
                      "position": 56,
                      "sentence": "Even with infinite computation and perfect optimization, performance is bounded by noise and ambiguity.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_information_theory_and_learning.txt",
                      "primary_category": null,
                      "rerank_score": -3.4648706912994385,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.5844025015830994
                    }
                  ]
                }
              },
              {
                "subclaim": "Returns diminish beyond scale",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6178,
                      "faiss_score": 0.8914539217948914,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 54,
                      "sentence": "While scaling has delivered consistent gains, it may encounter diminishing returns or external constraints that necessitate new approaches.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.174654960632324,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.066108882427216
                    },
                    {
                      "id": 6133,
                      "faiss_score": 0.8767734169960022,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -8.22350025177002,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -7.346726834774017
                    },
                    {
                      "id": 6186,
                      "faiss_score": 0.8617356419563293,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 62,
                      "sentence": "Ultimately, scaling is a powerful but blunt tool.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -9.478944778442383,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -8.617209136486053
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Quantum error correction enables reliable quantum computation and scalability, but requires many physical qubits, introduces large overhead, limits near-term feasibility, and increases system complexity.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Quantum error correction enables reliable quantum computation",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 4795,
                "faiss_score": 0.8979429006576538,
                "faiss_rank": 15,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 5.168917655944824,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.066860556602478
              },
              {
                "id": 4719,
                "faiss_score": 0.9038070440292358,
                "faiss_rank": 9,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.851476669311523,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.755283713340759
              },
              {
                "id": 4862,
                "faiss_score": 0.920432448387146,
                "faiss_rank": 2,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 109,
                "sentence": "Quantum error correction can be applied to quantum metrology.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 4.4757890701293945,
                "rerank_rank": 4,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.3962215185165405
              }
            ]
          }
        },
        {
          "subclaim": "Quantum error correction requires many physical qubits",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6554,
                "faiss_score": 0.9731938242912292,
                "faiss_rank": 1,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 9.974997520446777,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 10.948191344738007
              },
              {
                "id": 4792,
                "faiss_score": 0.9309069514274597,
                "faiss_rank": 2,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 39,
                "sentence": "According to the quantum Hamming bound, encoding a single logical qubit with the ability to correct any single-qubit error requires at least five physical qubits.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 8.700674057006836,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 9.631581008434296
              },
              {
                "id": 765,
                "faiss_score": 0.9033190608024597,
                "faiss_rank": 13,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 169,
                "sentence": "Careful estimates show that at least 3 million physical qubits would factor 2,048-bit integer in 5 months on a fully error-corrected trapped-ion quantum computer.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 6.840010643005371,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 7.743329703807831
              }
            ]
          }
        },
        {
          "subclaim": "Quantum error correction introduces large overhead",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 764,
                "faiss_score": 0.9220078587532043,
                "faiss_rank": 6,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 168,
                "sentence": "However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 5.192421913146973,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 6.114429771900177
              },
              {
                "id": 793,
                "faiss_score": 0.9589597582817078,
                "faiss_rank": 1,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.363793849945068,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 5.322753608226776
              },
              {
                "id": 4795,
                "faiss_score": 0.8939375281333923,
                "faiss_rank": 12,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": -0.6144595146179199,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.2794780135154724
              }
            ]
          }
        },
        {
          "subclaim": "Quantum error correction limits near-term feasibility",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6562,
                "faiss_score": 0.9053820371627808,
                "faiss_rank": 5,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 22,
                "sentence": "This constraint has motivated interest in near-term quantum devices that operate without full error correction, often referred to as noisy intermediate-scale quantum systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 3.418632984161377,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.324015021324158
              },
              {
                "id": 4795,
                "faiss_score": 0.8939117193222046,
                "faiss_rank": 10,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 1.228224277496338,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.1221359968185425
              },
              {
                "id": 4824,
                "faiss_score": 0.8983690738677979,
                "faiss_rank": 8,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 71,
                "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": -0.5382038950920105,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.36016517877578735
              }
            ]
          }
        },
        {
          "subclaim": "Quantum error correction increases system complexity",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 764,
                "faiss_score": 0.9119178056716919,
                "faiss_rank": 7,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 168,
                "sentence": "However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.0019683837890625,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.913886189460754
              },
              {
                "id": 793,
                "faiss_score": 0.9260589480400085,
                "faiss_rank": 4,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 2.7478976249694824,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.673956573009491
              },
              {
                "id": 6553,
                "faiss_score": 0.9282435178756714,
                "faiss_rank": 3,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 2.169891357421875,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.0981348752975464
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Quantum error correction enables reliable quantum computation",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 4795,
                      "faiss_score": 0.8979429006576538,
                      "faiss_rank": 15,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 5.168917655944824,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.066860556602478
                    },
                    {
                      "id": 4719,
                      "faiss_score": 0.9038070440292358,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.851476669311523,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.755283713340759
                    },
                    {
                      "id": 4862,
                      "faiss_score": 0.920432448387146,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 109,
                      "sentence": "Quantum error correction can be applied to quantum metrology.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 4.4757890701293945,
                      "rerank_rank": 4,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.3962215185165405
                    }
                  ]
                }
              },
              {
                "subclaim": "Quantum error correction requires many physical qubits",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6554,
                      "faiss_score": 0.9731938242912292,
                      "faiss_rank": 1,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 9.974997520446777,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 10.948191344738007
                    },
                    {
                      "id": 4792,
                      "faiss_score": 0.9309069514274597,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 39,
                      "sentence": "According to the quantum Hamming bound, encoding a single logical qubit with the ability to correct any single-qubit error requires at least five physical qubits.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 8.700674057006836,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 9.631581008434296
                    },
                    {
                      "id": 765,
                      "faiss_score": 0.9033190608024597,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 169,
                      "sentence": "Careful estimates show that at least 3 million physical qubits would factor 2,048-bit integer in 5 months on a fully error-corrected trapped-ion quantum computer.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 6.840010643005371,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 7.743329703807831
                    }
                  ]
                }
              },
              {
                "subclaim": "Quantum error correction introduces large overhead",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 764,
                      "faiss_score": 0.9220078587532043,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 168,
                      "sentence": "However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 5.192421913146973,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 6.114429771900177
                    },
                    {
                      "id": 793,
                      "faiss_score": 0.9589597582817078,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.363793849945068,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 5.322753608226776
                    },
                    {
                      "id": 4795,
                      "faiss_score": 0.8939375281333923,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": -0.6144595146179199,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.2794780135154724
                    }
                  ]
                }
              },
              {
                "subclaim": "Quantum error correction limits near-term feasibility",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6562,
                      "faiss_score": 0.9053820371627808,
                      "faiss_rank": 5,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 22,
                      "sentence": "This constraint has motivated interest in near-term quantum devices that operate without full error correction, often referred to as noisy intermediate-scale quantum systems.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 3.418632984161377,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.324015021324158
                    },
                    {
                      "id": 4795,
                      "faiss_score": 0.8939117193222046,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 1.228224277496338,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.1221359968185425
                    },
                    {
                      "id": 4824,
                      "faiss_score": 0.8983690738677979,
                      "faiss_rank": 8,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 71,
                      "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": -0.5382038950920105,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.36016517877578735
                    }
                  ]
                }
              },
              {
                "subclaim": "Quantum error correction increases system complexity",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 764,
                      "faiss_score": 0.9119178056716919,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 168,
                      "sentence": "However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.0019683837890625,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.913886189460754
                    },
                    {
                      "id": 793,
                      "faiss_score": 0.9260589480400085,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 2.7478976249694824,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.673956573009491
                    },
                    {
                      "id": 6553,
                      "faiss_score": 0.9282435178756714,
                      "faiss_rank": 3,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 2.169891357421875,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.0981348752975464
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Large language models generate fluent text, perform many tasks, and generalize across domains, but hallucinate facts, encode societal biases, lack grounded reasoning, and require massive datasets.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Large language models generate fluent text",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6040,
                "faiss_score": 0.9094253778457642,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 6.935098171234131,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 7.844523549079895
              },
              {
                "id": 6121,
                "faiss_score": 0.9008355140686035,
                "faiss_rank": 2,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 4.061986923217773,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.962822437286377
              },
              {
                "id": 6042,
                "faiss_score": 0.8820071816444397,
                "faiss_rank": 8,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 2,
                "sentence": "This simple training signal, when combined with large datasets and high model capacity, produces systems that can generate coherent text, answer questions, summarize documents, and perform a wide variety of language-related tasks without explicit task-specific programming.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 3.112881660461426,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.9948888421058655
              }
            ]
          }
        },
        {
          "subclaim": "perform many tasks",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5691,
                "faiss_score": 0.8415981531143188,
                "faiss_rank": 8,
                "doc_id": "local_math_computation_limits.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Many important computational tasks fall into classes that are believed to be hard.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_computation_limits.txt",
                "primary_category": null,
                "rerank_score": -0.4781818985939026,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.36341625452041626
              },
              {
                "id": 2193,
                "faiss_score": 0.8400170803070068,
                "faiss_rank": 12,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 124,
                "sentence": "All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": -3.0649709701538086,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.2249538898468018
              },
              {
                "id": 3881,
                "faiss_score": 0.8591969013214111,
                "faiss_rank": 4,
                "doc_id": "wiki_Latency_(engineering)",
                "file_type": ".txt",
                "position": 50,
                "sentence": "When all of the tasks are done at the same time, however, it is possible to reduce the latency to the length of the longest task.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Latency_(engineering)",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": -3.914613723754883,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.0554168224334717
              }
            ]
          }
        },
        {
          "subclaim": "generalize across domains",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5562,
                "faiss_score": 0.8713772296905518,
                "faiss_rank": 9,
                "doc_id": "local_bio_gene_editing.txt",
                "file_type": ".txt",
                "position": 35,
                "sentence": "These factors complicate efforts to generalize results across systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_gene_editing.txt",
                "primary_category": null,
                "rerank_score": 1.713374137878418,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.5847513675689697
              },
              {
                "id": 6405,
                "faiss_score": 0.8751733899116516,
                "faiss_rank": 6,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 144,
                "sentence": "These representations encode statistical regularities of language and sequence structure, allowing models to generalize across contexts.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 0.37290430068969727,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.2480776906013489
              },
              {
                "id": 6128,
                "faiss_score": 0.875458836555481,
                "faiss_rank": 4,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 4,
                "sentence": "These trends have been observed across different domains and architectures, suggesting that scaling captures general properties of learning systems rather than task-specific quirks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -0.6710819602012634,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.20437687635421753
              }
            ]
          }
        },
        {
          "subclaim": "hallucinate facts",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 2021,
                "faiss_score": 0.8814721703529358,
                "faiss_rank": 3,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 289,
                "sentence": "Hallucinations represent a fundamental challenge, wherein models generate syntactically fluent text that appears factually sound, but is internally inconsistent with training data or factually incorrect.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 2.190915584564209,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.0723877549171448
              },
              {
                "id": 6063,
                "faiss_score": 0.8869999647140503,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 23,
                "sentence": "The tendency to hallucinate is influenced by prompting, context length, and decoding strategies.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.881400227546692,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.768400192260742
              },
              {
                "id": 6060,
                "faiss_score": 0.8803726434707642,
                "faiss_rank": 4,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 20,
                "sentence": "Hallucination is one of the most widely discussed failure modes of large language models.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.5415492057800293,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.4219218492507935
              }
            ]
          }
        },
        {
          "subclaim": "encode societal biases",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 411,
                "faiss_score": 0.9120092988014221,
                "faiss_rank": 1,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 302,
                "sentence": "When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -3.2401061058044434,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.3280968070030212
              },
              {
                "id": 412,
                "faiss_score": 0.911769449710846,
                "faiss_rank": 3,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 303,
                "sentence": "Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -3.7880208492279053,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.8762513995170593
              },
              {
                "id": 2038,
                "faiss_score": 0.8740885257720947,
                "faiss_rank": 7,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 306,
                "sentence": "Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -4.511942386627197,
                "rerank_rank": 5,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.6378538608551025
              }
            ]
          }
        },
        {
          "subclaim": "lack grounded reasoning",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6111,
                "faiss_score": 0.8929279446601868,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 71,
                "sentence": "At the same time, this progress has highlighted fundamental limitations related to grounding, reasoning, and reliability.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -4.8359198570251465,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.9429919123649597
              },
              {
                "id": 2754,
                "faiss_score": 0.8560482859611511,
                "faiss_rank": 5,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 349,
                "sentence": "A main criticism concerns the lack of theory surrounding some methods.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": -7.450339317321777,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -6.594291031360626
              },
              {
                "id": 6187,
                "faiss_score": 0.8757100701332092,
                "faiss_rank": 3,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 63,
                "sentence": "It can unlock capabilities that are difficult to achieve otherwise, but it does not solve fundamental problems related to understanding, grounding, or reasoning.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -7.920139312744141,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -7.044429242610931
              }
            ]
          }
        },
        {
          "subclaim": "require massive datasets",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6309,
                "faiss_score": 0.8829025030136108,
                "faiss_rank": 2,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 48,
                "sentence": "High-capacity models are prone to overfitting when data is scarce, and strong generalization typically requires pretraining on massive corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 1.0930875539779663,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.9759900569915771
              },
              {
                "id": 3968,
                "faiss_score": 0.8802938461303711,
                "faiss_rank": 4,
                "doc_id": "wiki_Throughput",
                "file_type": ".txt",
                "position": 63,
                "sentence": "Large data loads that require processing impose data processing requirements on hardware.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Network_throughput",
                "primary_category": "all articles needing additional references",
                "rerank_score": 0.3394312262535095,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.2197250723838806
              },
              {
                "id": 1394,
                "faiss_score": 0.8828780651092529,
                "faiss_rank": 3,
                "doc_id": "wiki_Overfitting",
                "file_type": ".txt",
                "position": 44,
                "sentence": "The optimal function usually needs verification on bigger or completely new datasets.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                "primary_category": "machine learning",
                "rerank_score": 0.20189504325389862,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 1.0847731083631516
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Large language models generate fluent text",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6040,
                      "faiss_score": 0.9094253778457642,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 6.935098171234131,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 7.844523549079895
                    },
                    {
                      "id": 6121,
                      "faiss_score": 0.9008355140686035,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 4.061986923217773,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.962822437286377
                    },
                    {
                      "id": 6042,
                      "faiss_score": 0.8820071816444397,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "This simple training signal, when combined with large datasets and high model capacity, produces systems that can generate coherent text, answer questions, summarize documents, and perform a wide variety of language-related tasks without explicit task-specific programming.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 3.112881660461426,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.9948888421058655
                    }
                  ]
                }
              },
              {
                "subclaim": "perform many tasks",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5691,
                      "faiss_score": 0.8415981531143188,
                      "faiss_rank": 8,
                      "doc_id": "local_math_computation_limits.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "Many important computational tasks fall into classes that are believed to be hard.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_computation_limits.txt",
                      "primary_category": null,
                      "rerank_score": -0.4781818985939026,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.36341625452041626
                    },
                    {
                      "id": 2193,
                      "faiss_score": 0.8400170803070068,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Artificial_neural_network",
                      "file_type": ".txt",
                      "position": 124,
                      "sentence": "All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                      "primary_category": "neural networks",
                      "rerank_score": -3.0649709701538086,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.2249538898468018
                    },
                    {
                      "id": 3881,
                      "faiss_score": 0.8591969013214111,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Latency_(engineering)",
                      "file_type": ".txt",
                      "position": 50,
                      "sentence": "When all of the tasks are done at the same time, however, it is possible to reduce the latency to the length of the longest task.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Latency_(engineering)",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": -3.914613723754883,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -3.0554168224334717
                    }
                  ]
                }
              },
              {
                "subclaim": "generalize across domains",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5562,
                      "faiss_score": 0.8713772296905518,
                      "faiss_rank": 9,
                      "doc_id": "local_bio_gene_editing.txt",
                      "file_type": ".txt",
                      "position": 35,
                      "sentence": "These factors complicate efforts to generalize results across systems.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_gene_editing.txt",
                      "primary_category": null,
                      "rerank_score": 1.713374137878418,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.5847513675689697
                    },
                    {
                      "id": 6405,
                      "faiss_score": 0.8751733899116516,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 144,
                      "sentence": "These representations encode statistical regularities of language and sequence structure, allowing models to generalize across contexts.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 0.37290430068969727,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.2480776906013489
                    },
                    {
                      "id": 6128,
                      "faiss_score": 0.875458836555481,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 4,
                      "sentence": "These trends have been observed across different domains and architectures, suggesting that scaling captures general properties of learning systems rather than task-specific quirks.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -0.6710819602012634,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.20437687635421753
                    }
                  ]
                }
              },
              {
                "subclaim": "hallucinate facts",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 2021,
                      "faiss_score": 0.8814721703529358,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 289,
                      "sentence": "Hallucinations represent a fundamental challenge, wherein models generate syntactically fluent text that appears factually sound, but is internally inconsistent with training data or factually incorrect.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 2.190915584564209,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.0723877549171448
                    },
                    {
                      "id": 6063,
                      "faiss_score": 0.8869999647140503,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 23,
                      "sentence": "The tendency to hallucinate is influenced by prompting, context length, and decoding strategies.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.881400227546692,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.768400192260742
                    },
                    {
                      "id": 6060,
                      "faiss_score": 0.8803726434707642,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 20,
                      "sentence": "Hallucination is one of the most widely discussed failure modes of large language models.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.5415492057800293,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.4219218492507935
                    }
                  ]
                }
              },
              {
                "subclaim": "encode societal biases",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 411,
                      "faiss_score": 0.9120092988014221,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 302,
                      "sentence": "When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -3.2401061058044434,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.3280968070030212
                    },
                    {
                      "id": 412,
                      "faiss_score": 0.911769449710846,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 303,
                      "sentence": "Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -3.7880208492279053,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.8762513995170593
                    },
                    {
                      "id": 2038,
                      "faiss_score": 0.8740885257720947,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 306,
                      "sentence": "Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -4.511942386627197,
                      "rerank_rank": 5,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -3.6378538608551025
                    }
                  ]
                }
              },
              {
                "subclaim": "lack grounded reasoning",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6111,
                      "faiss_score": 0.8929279446601868,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 71,
                      "sentence": "At the same time, this progress has highlighted fundamental limitations related to grounding, reasoning, and reliability.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -4.8359198570251465,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -3.9429919123649597
                    },
                    {
                      "id": 2754,
                      "faiss_score": 0.8560482859611511,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 349,
                      "sentence": "A main criticism concerns the lack of theory surrounding some methods.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": -7.450339317321777,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -6.594291031360626
                    },
                    {
                      "id": 6187,
                      "faiss_score": 0.8757100701332092,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 63,
                      "sentence": "It can unlock capabilities that are difficult to achieve otherwise, but it does not solve fundamental problems related to understanding, grounding, or reasoning.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -7.920139312744141,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -7.044429242610931
                    }
                  ]
                }
              },
              {
                "subclaim": "require massive datasets",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6309,
                      "faiss_score": 0.8829025030136108,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 48,
                      "sentence": "High-capacity models are prone to overfitting when data is scarce, and strong generalization typically requires pretraining on massive corpora.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 1.0930875539779663,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.9759900569915771
                    },
                    {
                      "id": 3968,
                      "faiss_score": 0.8802938461303711,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Throughput",
                      "file_type": ".txt",
                      "position": 63,
                      "sentence": "Large data loads that require processing impose data processing requirements on hardware.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Network_throughput",
                      "primary_category": "all articles needing additional references",
                      "rerank_score": 0.3394312262535095,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.2197250723838806
                    },
                    {
                      "id": 1394,
                      "faiss_score": 0.8828780651092529,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Overfitting",
                      "file_type": ".txt",
                      "position": 44,
                      "sentence": "The optimal function usually needs verification on bigger or completely new datasets.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                      "primary_category": "machine learning",
                      "rerank_score": 0.20189504325389862,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 1.0847731083631516
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Increasing computational resources improves deep learning training speed, enables larger models, and stabilizes optimization, but increases cost, energy usage, hardware dependence, and environmental impact.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Increasing computational resources improves deep learning training speed",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 2624,
                "faiss_score": 0.8756795525550842,
                "faiss_rank": 12,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 219,
                "sentence": "OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 3.08569598197937,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.9613755345344543
              },
              {
                "id": 2164,
                "faiss_score": 0.8744220733642578,
                "faiss_rank": 13,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 95,
                "sentence": "Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": 3.0568156242370605,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 3.9312376976013184
              },
              {
                "id": 2622,
                "faiss_score": 0.892419695854187,
                "faiss_rank": 2,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 217,
                "sentence": "Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 1.4879298210144043,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.3803495168685913
              }
            ]
          }
        },
        {
          "subclaim": "Increasing computational resources enables larger models",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5907,
                "faiss_score": 0.8947705626487732,
                "faiss_rank": 10,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 2,
                "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 3.712005615234375,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.606776177883148
              },
              {
                "id": 6147,
                "faiss_score": 0.88716059923172,
                "faiss_rank": 19,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 23,
                "sentence": "For example, increasing model size without increasing data may yield limited benefits, while increasing data without sufficient model capacity may fail to exploit the additional information.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 1.6675896644592285,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.5547502636909485
              },
              {
                "id": 1901,
                "faiss_score": 0.8953041434288025,
                "faiss_rank": 9,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 169,
                "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 1.582042932510376,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 2.4773470759391785
              }
            ]
          }
        },
        {
          "subclaim": "Increasing computational resources stabilizes optimization",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5868,
                "faiss_score": 0.8724570274353027,
                "faiss_rank": 19,
                "doc_id": "local_math_optimization_and_convergence.txt",
                "file_type": ".txt",
                "position": 37,
                "sentence": "Regularization can improve generalization and stabilize optimization by smoothing the objective landscape.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                "primary_category": null,
                "rerank_score": -1.1302709579467773,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.2578139305114746
              },
              {
                "id": 1901,
                "faiss_score": 0.8825475573539734,
                "faiss_rank": 5,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 169,
                "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -1.6149537563323975,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.7324061989784241
              },
              {
                "id": 5857,
                "faiss_score": 0.8886821269989014,
                "faiss_rank": 2,
                "doc_id": "local_math_optimization_and_convergence.txt",
                "file_type": ".txt",
                "position": 26,
                "sentence": "Balancing exploration and stability is a recurring theme in optimization theory and practice.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                "primary_category": null,
                "rerank_score": -3.3342981338500977,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.4456160068511963
              }
            ]
          }
        },
        {
          "subclaim": "Increasing computational resources increases cost",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5995,
                "faiss_score": 0.8827309012413025,
                "faiss_rank": 5,
                "doc_id": "local_ml_evaluation_and_generalization.txt",
                "file_type": ".txt",
                "position": 21,
                "sentence": "This technique reduces variance in evaluation estimates but increases computational cost.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                "primary_category": null,
                "rerank_score": 4.090311050415039,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": 4.9730419516563416
              },
              {
                "id": 6651,
                "faiss_score": 0.8786903619766235,
                "faiss_rank": 13,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 54,
                "sentence": "Adding redundancy increases cost and complexity.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": -1.3506048917770386,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.47191452980041504
              },
              {
                "id": 5923,
                "faiss_score": 0.8798081874847412,
                "faiss_rank": 8,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "This technique shifts computational cost from deployment to training.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -2.631453037261963,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.7516448497772217
              }
            ]
          }
        },
        {
          "subclaim": "Increasing computational resources increases energy usage",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 5907,
                "faiss_score": 0.8672863245010376,
                "faiss_rank": 16,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 2,
                "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -1.2218883037567139,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.35460197925567627
              },
              {
                "id": 5432,
                "faiss_score": 0.9075989723205566,
                "faiss_rank": 2,
                "doc_id": "wiki_Landauer's_principle",
                "file_type": ".txt",
                "position": 2,
                "sentence": "As of 2012, modern computers use about a billion times as much energy per operation.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Landauer%27s_principle",
                "primary_category": "all articles containing potentially dated statements",
                "rerank_score": -2.027003526687622,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.1194045543670654
              },
              {
                "id": 5381,
                "faiss_score": 0.8766747713088989,
                "faiss_rank": 11,
                "doc_id": "wiki_Reversible_computing",
                "file_type": ".txt",
                "position": 6,
                "sentence": "Reversible computing proponents argue that a significant portion of this energy consumption is due to architectural overheads.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Reversible_computing",
                "primary_category": "articles with short description",
                "rerank_score": -2.1637558937072754,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.2870811223983765
              }
            ]
          }
        },
        {
          "subclaim": "Increasing computational resources increases hardware dependence",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6530,
                "faiss_score": 0.8772687911987305,
                "faiss_rank": 11,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 59,
                "sentence": "This dependence underscores the importance of considering computation as a layered system.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": -3.2304980754852295,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.353229284286499
              },
              {
                "id": 5944,
                "faiss_score": 0.8837687969207764,
                "faiss_rank": 5,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "Hardware plays a significant role in shaping efficiency strategies.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -5.977171897888184,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.093403100967407
              },
              {
                "id": 5900,
                "faiss_score": 0.8796184062957764,
                "faiss_rank": 9,
                "doc_id": "local_math_optimization_and_convergence.txt",
                "file_type": ".txt",
                "position": 69,
                "sentence": "Communication, memory access, and hardware utilization influence practical convergence speed.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                "primary_category": null,
                "rerank_score": -6.411072731018066,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.53145432472229
              }
            ]
          }
        },
        {
          "subclaim": "Increasing computational resources increases environmental impact",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6094,
                "faiss_score": 0.8690935373306274,
                "faiss_rank": 9,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 54,
                "sentence": "These costs limit participation to well-resourced organizations and raise concerns about environmental impact.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -4.8720855712890625,
                "rerank_rank": 1,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.002992033958435
              },
              {
                "id": 6534,
                "faiss_score": 0.8964979648590088,
                "faiss_rank": 1,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 63,
                "sentence": "Research in computation increasingly emphasizes efficiency and sustainability.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": -5.121312141418457,
                "rerank_rank": 2,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.224814176559448
              },
              {
                "id": 6310,
                "faiss_score": 0.8743864297866821,
                "faiss_rank": 7,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 49,
                "sentence": "This dependence raises concerns about accessibility, reproducibility, and environmental impact.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -5.240941524505615,
                "rerank_rank": 3,
                "probs": {
                  "support": 0.0,
                  "contradict": 0.0,
                  "neutral": 1.0
                },
                "stance_score": 0.0,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.366555094718933
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Increasing computational resources improves deep learning training speed",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 2624,
                      "faiss_score": 0.8756795525550842,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 219,
                      "sentence": "OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": 3.08569598197937,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.9613755345344543
                    },
                    {
                      "id": 2164,
                      "faiss_score": 0.8744220733642578,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Artificial_neural_network",
                      "file_type": ".txt",
                      "position": 95,
                      "sentence": "Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                      "primary_category": "neural networks",
                      "rerank_score": 3.0568156242370605,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 3.9312376976013184
                    },
                    {
                      "id": 2622,
                      "faiss_score": 0.892419695854187,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 217,
                      "sentence": "Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": 1.4879298210144043,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.3803495168685913
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing computational resources enables larger models",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5907,
                      "faiss_score": 0.8947705626487732,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 3.712005615234375,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.606776177883148
                    },
                    {
                      "id": 6147,
                      "faiss_score": 0.88716059923172,
                      "faiss_rank": 19,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 23,
                      "sentence": "For example, increasing model size without increasing data may yield limited benefits, while increasing data without sufficient model capacity may fail to exploit the additional information.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 1.6675896644592285,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.5547502636909485
                    },
                    {
                      "id": 1901,
                      "faiss_score": 0.8953041434288025,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 169,
                      "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 1.582042932510376,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 2.4773470759391785
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing computational resources stabilizes optimization",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5868,
                      "faiss_score": 0.8724570274353027,
                      "faiss_rank": 19,
                      "doc_id": "local_math_optimization_and_convergence.txt",
                      "file_type": ".txt",
                      "position": 37,
                      "sentence": "Regularization can improve generalization and stabilize optimization by smoothing the objective landscape.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                      "primary_category": null,
                      "rerank_score": -1.1302709579467773,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.2578139305114746
                    },
                    {
                      "id": 1901,
                      "faiss_score": 0.8825475573539734,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 169,
                      "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -1.6149537563323975,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.7324061989784241
                    },
                    {
                      "id": 5857,
                      "faiss_score": 0.8886821269989014,
                      "faiss_rank": 2,
                      "doc_id": "local_math_optimization_and_convergence.txt",
                      "file_type": ".txt",
                      "position": 26,
                      "sentence": "Balancing exploration and stability is a recurring theme in optimization theory and practice.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                      "primary_category": null,
                      "rerank_score": -3.3342981338500977,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.4456160068511963
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing computational resources increases cost",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5995,
                      "faiss_score": 0.8827309012413025,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_evaluation_and_generalization.txt",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "This technique reduces variance in evaluation estimates but increases computational cost.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                      "primary_category": null,
                      "rerank_score": 4.090311050415039,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 4.9730419516563416
                    },
                    {
                      "id": 6651,
                      "faiss_score": 0.8786903619766235,
                      "faiss_rank": 13,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 54,
                      "sentence": "Adding redundancy increases cost and complexity.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": -1.3506048917770386,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.47191452980041504
                    },
                    {
                      "id": 5923,
                      "faiss_score": 0.8798081874847412,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "This technique shifts computational cost from deployment to training.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -2.631453037261963,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.7516448497772217
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing computational resources increases energy usage",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5907,
                      "faiss_score": 0.8672863245010376,
                      "faiss_rank": 16,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -1.2218883037567139,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.35460197925567627
                    },
                    {
                      "id": 5432,
                      "faiss_score": 0.9075989723205566,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Landauer's_principle",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "As of 2012, modern computers use about a billion times as much energy per operation.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Landauer%27s_principle",
                      "primary_category": "all articles containing potentially dated statements",
                      "rerank_score": -2.027003526687622,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.1194045543670654
                    },
                    {
                      "id": 5381,
                      "faiss_score": 0.8766747713088989,
                      "faiss_rank": 11,
                      "doc_id": "wiki_Reversible_computing",
                      "file_type": ".txt",
                      "position": 6,
                      "sentence": "Reversible computing proponents argue that a significant portion of this energy consumption is due to architectural overheads.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Reversible_computing",
                      "primary_category": "articles with short description",
                      "rerank_score": -2.1637558937072754,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.2870811223983765
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing computational resources increases hardware dependence",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6530,
                      "faiss_score": 0.8772687911987305,
                      "faiss_rank": 11,
                      "doc_id": "local_physics_computation.txt",
                      "file_type": ".txt",
                      "position": 59,
                      "sentence": "This dependence underscores the importance of considering computation as a layered system.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                      "primary_category": null,
                      "rerank_score": -3.2304980754852295,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.353229284286499
                    },
                    {
                      "id": 5944,
                      "faiss_score": 0.8837687969207764,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 39,
                      "sentence": "Hardware plays a significant role in shaping efficiency strategies.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -5.977171897888184,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -5.093403100967407
                    },
                    {
                      "id": 5900,
                      "faiss_score": 0.8796184062957764,
                      "faiss_rank": 9,
                      "doc_id": "local_math_optimization_and_convergence.txt",
                      "file_type": ".txt",
                      "position": 69,
                      "sentence": "Communication, memory access, and hardware utilization influence practical convergence speed.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                      "primary_category": null,
                      "rerank_score": -6.411072731018066,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -5.53145432472229
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing computational resources increases environmental impact",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6094,
                      "faiss_score": 0.8690935373306274,
                      "faiss_rank": 9,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 54,
                      "sentence": "These costs limit participation to well-resourced organizations and raise concerns about environmental impact.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -4.8720855712890625,
                      "rerank_rank": 1,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -4.002992033958435
                    },
                    {
                      "id": 6534,
                      "faiss_score": 0.8964979648590088,
                      "faiss_rank": 1,
                      "doc_id": "local_physics_computation.txt",
                      "file_type": ".txt",
                      "position": 63,
                      "sentence": "Research in computation increasingly emphasizes efficiency and sustainability.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                      "primary_category": null,
                      "rerank_score": -5.121312141418457,
                      "rerank_rank": 2,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -4.224814176559448
                    },
                    {
                      "id": 6310,
                      "faiss_score": 0.8743864297866821,
                      "faiss_rank": 7,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 49,
                      "sentence": "This dependence raises concerns about accessibility, reproducibility, and environmental impact.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -5.240941524505615,
                      "rerank_rank": 3,
                      "probs": {
                        "support": 0.0,
                        "contradict": 0.0,
                        "neutral": 1.0
                      },
                      "stance_score": 0.0,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -4.366555094718933
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    }
  ],
  "metrics": {
    "final_accuracy": 0.14814814814814814,
    "confusion_matrix": {
      "SUPPORT": {
        "SUPPORT": 0,
        "CONTRADICT": 0,
        "MIXED": 0,
        "INCONCLUSIVE": 6
      },
      "CONTRADICT": {
        "SUPPORT": 0,
        "CONTRADICT": 0,
        "MIXED": 0,
        "INCONCLUSIVE": 6
      },
      "MIXED": {
        "SUPPORT": 0,
        "CONTRADICT": 0,
        "MIXED": 0,
        "INCONCLUSIVE": 11
      },
      "INCONCLUSIVE": {
        "SUPPORT": 0,
        "CONTRADICT": 0,
        "MIXED": 0,
        "INCONCLUSIVE": 4
      }
    },
    "precision_recall_f1": {
      "per_class": {
        "SUPPORT": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "CONTRADICT": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "MIXED": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "INCONCLUSIVE": {
          "precision": 0.14814814814814814,
          "recall": 1.0,
          "f1": 0.25806451612903225
        }
      },
      "macro_f1": 0.06451612903225806
    },
    "subclaim_accuracy": null,
    "decomposition_stats": {
      "avg_subclaims_per_claim": 2.8518518518518516,
      "min_subclaims": 1,
      "max_subclaims": 7
    }
  }
}