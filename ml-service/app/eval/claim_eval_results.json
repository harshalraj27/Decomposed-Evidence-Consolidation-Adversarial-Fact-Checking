{
  "results": [
    {
      "claim": "Scaling laws indicate that increasing model size and data generally improves language model performance.",
      "expected_verdict": "SUPPORT",
      "predicted_verdict": "SUPPORT",
      "subclaims": [
        {
          "subclaim": "Scaling laws indicate that increasing model size improves language model performance.",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 0.7753959966549218,
            "contradict": 0.0,
            "total": 0.7753959966549218
          },
          "evidence": {
            "supporting": [
              {
                "id": 6300,
                "faiss_score": 0.8919124603271484,
                "faiss_rank": 18,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "As transformer models grew larger, their performance continued to improve, reinforcing the idea that scaling was a key driver of success.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 1.1516189575195312,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.019933873787522316,
                  "neutral": 0.28682276606559753,
                  "support": 0.6932433843612671
                },
                "stance_score": 0.6733095105737448,
                "evidence_contribution": 0.7753959966549218,
                "combined_rank_score": 2.0435314178466797
              }
            ],
            "contradicting": [
              {
                "id": 6121,
                "faiss_score": 0.8985731601715088,
                "faiss_rank": 9,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -0.8732558488845825,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.45808807015419006,
                  "neutral": 0.5354201197624207,
                  "support": 0.006491828244179487
                },
                "stance_score": -0.4515962419100106,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.02531731128692627
              }
            ],
            "neutral": [
              {
                "id": 6142,
                "faiss_score": 0.9138842225074768,
                "faiss_rank": 6,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 5.783466339111328,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0020528091117739677,
                  "neutral": 0.9971649050712585,
                  "support": 0.0007822587504051626
                },
                "stance_score": -0.0012705503613688052,
                "evidence_contribution": -0.007348185247122219,
                "combined_rank_score": 6.697350561618805
              },
              {
                "id": 6124,
                "faiss_score": 0.8996810913085938,
                "faiss_rank": 8,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 5.338567733764648,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0030547380447387695,
                  "neutral": 0.9952334761619568,
                  "support": 0.001711796852760017
                },
                "stance_score": -0.0013429411919787526,
                "evidence_contribution": -0.007169382515841205,
                "combined_rank_score": 6.238248825073242
              },
              {
                "id": 6133,
                "faiss_score": 0.8973473310470581,
                "faiss_rank": 10,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.928947925567627,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.007783351466059685,
                  "neutral": 0.9895959496498108,
                  "support": 0.002620717976242304
                },
                "stance_score": -0.005162633489817381,
                "evidence_contribution": -0.025446351630101338,
                "combined_rank_score": 5.826295256614685
              }
            ]
          }
        },
        {
          "subclaim": "Scaling laws indicate that increasing data generally improves language model performance.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 6121,
                "faiss_score": 0.8885934352874756,
                "faiss_rank": 13,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -2.486970901489258,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.11388173699378967,
                  "neutral": 0.8839079141616821,
                  "support": 0.0022103756200522184
                },
                "stance_score": -0.11167136137373745,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.5983774662017822
              }
            ],
            "neutral": [
              {
                "id": 6127,
                "faiss_score": 0.9171009063720703,
                "faiss_rank": 2,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "Empirical observations have shown that, under certain conditions, model performance improves predictably as scale increases.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.851520538330078,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.004043751861900091,
                  "neutral": 0.9890744090080261,
                  "support": 0.006881888955831528
                },
                "stance_score": 0.0028381370939314365,
                "evidence_contribution": 0.013769280401804807,
                "combined_rank_score": 5.768621444702148
              },
              {
                "id": 6142,
                "faiss_score": 0.9079248309135437,
                "faiss_rank": 5,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.8335371017456055,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0016234064241871238,
                  "neutral": 0.997517466545105,
                  "support": 0.0008592000813223422
                },
                "stance_score": -0.0007642063428647816,
                "evidence_contribution": -0.003693819711626245,
                "combined_rank_score": 5.741461932659149
              },
              {
                "id": 6124,
                "faiss_score": 0.8999987840652466,
                "faiss_rank": 9,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.305981636047363,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.002156906295567751,
                  "neutral": 0.9965866804122925,
                  "support": 0.0012564564822241664
                },
                "stance_score": -0.0009004498133435845,
                "evidence_contribution": -0.003877320360439751,
                "combined_rank_score": 5.20598042011261
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is generally supported by the available evidence, with some limitations.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Scaling laws indicate that increasing model size improves language model performance.",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6300,
                      "faiss_score": 0.8919124603271484,
                      "faiss_rank": 18,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 39,
                      "sentence": "As transformer models grew larger, their performance continued to improve, reinforcing the idea that scaling was a key driver of success.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 1.1516189575195312,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.019933873787522316,
                        "neutral": 0.28682276606559753,
                        "support": 0.6932433843612671
                      },
                      "stance_score": 0.6733095105737448,
                      "evidence_contribution": 0.7753959966549218,
                      "combined_rank_score": 2.0435314178466797
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6121,
                      "faiss_score": 0.8985731601715088,
                      "faiss_rank": 9,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -0.8732558488845825,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.45808807015419006,
                        "neutral": 0.5354201197624207,
                        "support": 0.006491828244179487
                      },
                      "stance_score": -0.4515962419100106,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.02531731128692627
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6142,
                      "faiss_score": 0.9138842225074768,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 5.783466339111328,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0020528091117739677,
                        "neutral": 0.9971649050712585,
                        "support": 0.0007822587504051626
                      },
                      "stance_score": -0.0012705503613688052,
                      "evidence_contribution": -0.007348185247122219,
                      "combined_rank_score": 6.697350561618805
                    },
                    {
                      "id": 6124,
                      "faiss_score": 0.8996810913085938,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 5.338567733764648,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0030547380447387695,
                        "neutral": 0.9952334761619568,
                        "support": 0.001711796852760017
                      },
                      "stance_score": -0.0013429411919787526,
                      "evidence_contribution": -0.007169382515841205,
                      "combined_rank_score": 6.238248825073242
                    },
                    {
                      "id": 6133,
                      "faiss_score": 0.8973473310470581,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.928947925567627,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.007783351466059685,
                        "neutral": 0.9895959496498108,
                        "support": 0.002620717976242304
                      },
                      "stance_score": -0.005162633489817381,
                      "evidence_contribution": -0.025446351630101338,
                      "combined_rank_score": 5.826295256614685
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Scaling laws indicate that increasing data generally improves language model performance.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 6121,
                      "faiss_score": 0.8885934352874756,
                      "faiss_rank": 13,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -2.486970901489258,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.11388173699378967,
                        "neutral": 0.8839079141616821,
                        "support": 0.0022103756200522184
                      },
                      "stance_score": -0.11167136137373745,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.5983774662017822
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6127,
                      "faiss_score": 0.9171009063720703,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "Empirical observations have shown that, under certain conditions, model performance improves predictably as scale increases.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.851520538330078,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.004043751861900091,
                        "neutral": 0.9890744090080261,
                        "support": 0.006881888955831528
                      },
                      "stance_score": 0.0028381370939314365,
                      "evidence_contribution": 0.013769280401804807,
                      "combined_rank_score": 5.768621444702148
                    },
                    {
                      "id": 6142,
                      "faiss_score": 0.9079248309135437,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.8335371017456055,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0016234064241871238,
                        "neutral": 0.997517466545105,
                        "support": 0.0008592000813223422
                      },
                      "stance_score": -0.0007642063428647816,
                      "evidence_contribution": -0.003693819711626245,
                      "combined_rank_score": 5.741461932659149
                    },
                    {
                      "id": 6124,
                      "faiss_score": 0.8999987840652466,
                      "faiss_rank": 9,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.305981636047363,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.002156906295567751,
                        "neutral": 0.9965866804122925,
                        "support": 0.0012564564822241664
                      },
                      "stance_score": -0.0009004498133435845,
                      "evidence_contribution": -0.003877320360439751,
                      "combined_rank_score": 5.20598042011261
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Transformer architectures enabled significant improvements in natural language processing tasks.",
      "expected_verdict": "SUPPORT",
      "predicted_verdict": "SUPPORT",
      "subclaims": [
        {
          "subclaim": "Transformer architectures were used in natural language processing tasks.",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 14.348367211160701,
            "contradict": 0.4050704516570928,
            "total": 14.753437662817793
          },
          "evidence": {
            "supporting": [
              {
                "id": 6296,
                "faiss_score": 0.9599642753601074,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 35,
                "sentence": "The transformer architecture proved highly effective across a wide range of natural language processing tasks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 9.242033958435059,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0064346035942435265,
                  "neutral": 0.011724923737347126,
                  "support": 0.9818404316902161
                },
                "stance_score": 0.9754058280959725,
                "evidence_contribution": 9.014733786518448,
                "combined_rank_score": 10.201998233795166
              },
              {
                "id": 2184,
                "faiss_score": 0.9435071349143982,
                "faiss_rank": 2,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 115,
                "sentence": "Transformers have increasingly become the model of choice for natural language processing.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": 4.645409107208252,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.001396269304677844,
                  "neutral": 0.01836763694882393,
                  "support": 0.9802360534667969
                },
                "stance_score": 0.978839784162119,
                "evidence_contribution": 4.547111247844468,
                "combined_rank_score": 5.58891624212265
              },
              {
                "id": 3003,
                "faiss_score": 0.8870973587036133,
                "faiss_rank": 17,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 58,
                "sentence": "Tasks for pretraining and fine-tuning commonly include: language modeling next-sentence prediction question answering reading comprehension sentiment analysis paraphrasing The T5 transformer report documents a large number of natural language pretraining tasks.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": 3.4558331966400146,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.013680233620107174,
                  "neutral": 0.745046854019165,
                  "support": 0.24127286672592163
                },
                "stance_score": 0.22759263310581446,
                "evidence_contribution": 0.7865221767977848,
                "combined_rank_score": 4.342930555343628
              },
              {
                "id": 3025,
                "faiss_score": 0.8943243622779846,
                "faiss_rank": 11,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 80,
                "sentence": "Transformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": -0.1328548789024353,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.003647217759862542,
                  "neutral": 0.8628968596458435,
                  "support": 0.13345587253570557
                },
                "stance_score": 0.12980865477584302,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.7614694833755493
              }
            ],
            "contradicting": [
              {
                "id": 2987,
                "faiss_score": 0.8978081941604614,
                "faiss_rank": 8,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": 0.8487489819526672,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.31608426570892334,
                  "neutral": 0.6735674142837524,
                  "support": 0.010348307900130749
                },
                "stance_score": -0.3057359578087926,
                "evidence_contribution": -0.25949308293653633,
                "combined_rank_score": 1.7465571761131287
              },
              {
                "id": 1807,
                "faiss_score": 0.9014351963996887,
                "faiss_rank": 7,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 75,
                "sentence": "LLMs are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 0.8668583035469055,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.1701473444700241,
                  "neutral": 0.7902719378471375,
                  "support": 0.03958071395754814
                },
                "stance_score": -0.13056663051247597,
                "evidence_contribution": -0.11318276782588055,
                "combined_rank_score": 1.7682934999465942
              },
              {
                "id": 6261,
                "faiss_score": 0.9068849682807922,
                "faiss_rank": 5,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Transformer-based neural network architectures emerged from a broader effort to overcome structural limitations present in earlier approaches to sequence modeling.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 0.053444553166627884,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.6116482615470886,
                  "neutral": 0.38283827900886536,
                  "support": 0.005513511132448912
                },
                "stance_score": -0.6061347504146397,
                "evidence_contribution": -0.03239460089467593,
                "combined_rank_score": 0.9603295214474201
              }
            ],
            "neutral": [
              {
                "id": 6334,
                "faiss_score": 0.8871206045150757,
                "faiss_rank": 16,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 73,
                "sentence": "Rather than training models from scratch for each individual task, practitioners increasingly rely on pretrained transformer backbones that capture broad linguistic or sequential knowledge.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 0.6710541248321533,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.003452342702075839,
                  "neutral": 0.908929169178009,
                  "support": 0.08761847019195557
                },
                "stance_score": 0.08416612748987973,
                "evidence_contribution": 0.05648002702323268,
                "combined_rank_score": 1.558174729347229
              },
              {
                "id": 3083,
                "faiss_score": 0.8949947357177734,
                "faiss_rank": 10,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 138,
                "sentence": "The transformer architecture is constructed to calculate output tokens iteratively.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": 0.5777134895324707,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.06788498163223267,
                  "neutral": 0.9225779175758362,
                  "support": 0.009537145495414734
                },
                "stance_score": -0.05834783613681793,
                "evidence_contribution": -0.03370833202126988,
                "combined_rank_score": 1.4727082252502441
              },
              {
                "id": 6273,
                "faiss_score": 0.8904296159744263,
                "faiss_rank": 15,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 12,
                "sentence": "The transformer architecture generalized this concept by eliminating recurrence entirely and relying solely on attention mechanisms to model relationships within a sequence.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 0.10180939733982086,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.015106219798326492,
                  "neutral": 0.9824913144111633,
                  "support": 0.0024024825543165207
                },
                "stance_score": -0.012703737244009972,
                "evidence_contribution": -0.001293359832776092,
                "combined_rank_score": 0.9922390133142471
              }
            ]
          }
        },
        {
          "subclaim": "These tasks saw significant improvements.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [
              {
                "id": 5586,
                "faiss_score": 0.868844747543335,
                "faiss_rank": 11,
                "doc_id": "local_bio_gene_editing.txt",
                "file_type": ".txt",
                "position": 59,
                "sentence": "Improvements in precision, delivery, and control have expanded the range of possible applications.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_gene_editing.txt",
                "primary_category": null,
                "rerank_score": -1.6070451736450195,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0007949198479764163,
                  "neutral": 0.13771769404411316,
                  "support": 0.8614874482154846
                },
                "stance_score": 0.8606925283675082,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.7382004261016846
              },
              {
                "id": 6046,
                "faiss_score": 0.8739926815032959,
                "faiss_rank": 5,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 6,
                "sentence": "These include improved generalization, better handling of rare or ambiguous inputs, and the ability to adapt to new tasks with minimal additional data.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -2.1880946159362793,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0016440394101664424,
                  "neutral": 0.4812794029712677,
                  "support": 0.5170766115188599
                },
                "stance_score": 0.5154325721086934,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.3141019344329834
              },
              {
                "id": 6399,
                "faiss_score": 0.8873401880264282,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 138,
                "sentence": "Improvements in efficiency, training stability, and integration with other components are expected to yield practical gains.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -2.438533306121826,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0021322567481547594,
                  "neutral": 0.6527662873268127,
                  "support": 0.3451014459133148
                },
                "stance_score": 0.34296918916516006,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.551193118095398
              },
              {
                "id": 5589,
                "faiss_score": 0.8712761402130127,
                "faiss_rank": 6,
                "doc_id": "local_bio_gene_editing.txt",
                "file_type": ".txt",
                "position": 62,
                "sentence": "Small improvements in efficiency or specificity can have meaningful impacts when translated into clinical or agricultural settings.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_gene_editing.txt",
                "primary_category": null,
                "rerank_score": -3.2599258422851562,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.028195634484291077,
                  "neutral": 0.5434474945068359,
                  "support": 0.4283568263053894
                },
                "stance_score": 0.4001611918210983,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.3886497020721436
              },
              {
                "id": 6044,
                "faiss_score": 0.8646043539047241,
                "faiss_rank": 14,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 4,
                "sentence": "Increasing the number of parameters, the amount of training data, and the total compute used during training has been shown to produce consistent improvements in performance across many benchmarks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -4.22540807723999,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.000817636027932167,
                  "neutral": 0.1027408242225647,
                  "support": 0.8964415788650513
                },
                "stance_score": 0.8956239428371191,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.360803723335266
              }
            ],
            "contradicting": [
              {
                "id": 5969,
                "faiss_score": 0.8760039806365967,
                "faiss_rank": 3,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 64,
                "sentence": "Improvements in one area may expose constraints elsewhere.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -5.354757785797119,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.3850768208503723,
                  "neutral": 0.5729495286941528,
                  "support": 0.04197365790605545
                },
                "stance_score": -0.34310316294431686,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.4787538051605225
              },
              {
                "id": 6728,
                "faiss_score": 0.8701443076133728,
                "faiss_rank": 7,
                "doc_id": "local_systems_latency_throughput_tradeoffs.txt",
                "file_type": ".txt",
                "position": 60,
                "sentence": "Assertions that a system improves both metrics simultaneously should be examined carefully, as such improvements usually depend on changing assumptions or workloads.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_latency_throughput_tradeoffs.txt",
                "primary_category": null,
                "rerank_score": -6.235987663269043,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.2782565951347351,
                  "neutral": 0.683676540851593,
                  "support": 0.03806686028838158
                },
                "stance_score": -0.24018973484635353,
                "evidence_contribution": -0.0,
                "combined_rank_score": -5.36584335565567
              }
            ],
            "neutral": [
              {
                "id": 1877,
                "faiss_score": 0.8633498549461365,
                "faiss_rank": 15,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 145,
                "sentence": "Early research demonstrated that inserting intermediate \"scratchpad\" computations could improve performance on such tasks.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -2.0456624031066895,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.00309790694154799,
                  "neutral": 0.9512084126472473,
                  "support": 0.045693691819906235
                },
                "stance_score": 0.042595784878358245,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.182312548160553
              },
              {
                "id": 2232,
                "faiss_score": 0.8614091277122498,
                "faiss_rank": 18,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 163,
                "sentence": "Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": -4.753089427947998,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.003027374157682061,
                  "neutral": 0.9957343935966492,
                  "support": 0.0012382006971165538
                },
                "stance_score": -0.0017891734605655074,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.8916803002357483
              },
              {
                "id": 1785,
                "faiss_score": 0.8570448756217957,
                "faiss_rank": 19,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 53,
                "sentence": "Cleaned datasets can increase training efficiency and lead to improved downstream performance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -7.232579231262207,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.003309796331450343,
                  "neutral": 0.9835056662559509,
                  "support": 0.01318448968231678
                },
                "stance_score": 0.009874693350866437,
                "evidence_contribution": 0.0,
                "combined_rank_score": -6.375534355640411
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is generally supported by the available evidence, with some limitations.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Transformer architectures were used in natural language processing tasks.",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "weak"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6296,
                      "faiss_score": 0.9599642753601074,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 35,
                      "sentence": "The transformer architecture proved highly effective across a wide range of natural language processing tasks.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 9.242033958435059,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0064346035942435265,
                        "neutral": 0.011724923737347126,
                        "support": 0.9818404316902161
                      },
                      "stance_score": 0.9754058280959725,
                      "evidence_contribution": 9.014733786518448,
                      "combined_rank_score": 10.201998233795166
                    },
                    {
                      "id": 2184,
                      "faiss_score": 0.9435071349143982,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Artificial_neural_network",
                      "file_type": ".txt",
                      "position": 115,
                      "sentence": "Transformers have increasingly become the model of choice for natural language processing.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                      "primary_category": "neural networks",
                      "rerank_score": 4.645409107208252,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.001396269304677844,
                        "neutral": 0.01836763694882393,
                        "support": 0.9802360534667969
                      },
                      "stance_score": 0.978839784162119,
                      "evidence_contribution": 4.547111247844468,
                      "combined_rank_score": 5.58891624212265
                    },
                    {
                      "id": 3003,
                      "faiss_score": 0.8870973587036133,
                      "faiss_rank": 17,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 58,
                      "sentence": "Tasks for pretraining and fine-tuning commonly include: language modeling next-sentence prediction question answering reading comprehension sentiment analysis paraphrasing The T5 transformer report documents a large number of natural language pretraining tasks.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": 3.4558331966400146,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.013680233620107174,
                        "neutral": 0.745046854019165,
                        "support": 0.24127286672592163
                      },
                      "stance_score": 0.22759263310581446,
                      "evidence_contribution": 0.7865221767977848,
                      "combined_rank_score": 4.342930555343628
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 1807,
                      "faiss_score": 0.9014351963996887,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "LLMs are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 0.8668583035469055,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.1701473444700241,
                        "neutral": 0.7902719378471375,
                        "support": 0.03958071395754814
                      },
                      "stance_score": -0.13056663051247597,
                      "evidence_contribution": -0.11318276782588055,
                      "combined_rank_score": 1.7682934999465942
                    },
                    {
                      "id": 2987,
                      "faiss_score": 0.8978081941604614,
                      "faiss_rank": 8,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": 0.8487489819526672,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.31608426570892334,
                        "neutral": 0.6735674142837524,
                        "support": 0.010348307900130749
                      },
                      "stance_score": -0.3057359578087926,
                      "evidence_contribution": -0.25949308293653633,
                      "combined_rank_score": 1.7465571761131287
                    },
                    {
                      "id": 6261,
                      "faiss_score": 0.9068849682807922,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Transformer-based neural network architectures emerged from a broader effort to overcome structural limitations present in earlier approaches to sequence modeling.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 0.053444553166627884,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.6116482615470886,
                        "neutral": 0.38283827900886536,
                        "support": 0.005513511132448912
                      },
                      "stance_score": -0.6061347504146397,
                      "evidence_contribution": -0.03239460089467593,
                      "combined_rank_score": 0.9603295214474201
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6334,
                      "faiss_score": 0.8871206045150757,
                      "faiss_rank": 16,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 73,
                      "sentence": "Rather than training models from scratch for each individual task, practitioners increasingly rely on pretrained transformer backbones that capture broad linguistic or sequential knowledge.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 0.6710541248321533,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.003452342702075839,
                        "neutral": 0.908929169178009,
                        "support": 0.08761847019195557
                      },
                      "stance_score": 0.08416612748987973,
                      "evidence_contribution": 0.05648002702323268,
                      "combined_rank_score": 1.558174729347229
                    },
                    {
                      "id": 3083,
                      "faiss_score": 0.8949947357177734,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 138,
                      "sentence": "The transformer architecture is constructed to calculate output tokens iteratively.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": 0.5777134895324707,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.06788498163223267,
                        "neutral": 0.9225779175758362,
                        "support": 0.009537145495414734
                      },
                      "stance_score": -0.05834783613681793,
                      "evidence_contribution": -0.03370833202126988,
                      "combined_rank_score": 1.4727082252502441
                    },
                    {
                      "id": 6273,
                      "faiss_score": 0.8904296159744263,
                      "faiss_rank": 15,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 12,
                      "sentence": "The transformer architecture generalized this concept by eliminating recurrence entirely and relying solely on attention mechanisms to model relationships within a sequence.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 0.10180939733982086,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.015106219798326492,
                        "neutral": 0.9824913144111633,
                        "support": 0.0024024825543165207
                      },
                      "stance_score": -0.012703737244009972,
                      "evidence_contribution": -0.001293359832776092,
                      "combined_rank_score": 0.9922390133142471
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "These tasks saw significant improvements.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 5586,
                      "faiss_score": 0.868844747543335,
                      "faiss_rank": 11,
                      "doc_id": "local_bio_gene_editing.txt",
                      "file_type": ".txt",
                      "position": 59,
                      "sentence": "Improvements in precision, delivery, and control have expanded the range of possible applications.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_gene_editing.txt",
                      "primary_category": null,
                      "rerank_score": -1.6070451736450195,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0007949198479764163,
                        "neutral": 0.13771769404411316,
                        "support": 0.8614874482154846
                      },
                      "stance_score": 0.8606925283675082,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.7382004261016846
                    },
                    {
                      "id": 6046,
                      "faiss_score": 0.8739926815032959,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 6,
                      "sentence": "These include improved generalization, better handling of rare or ambiguous inputs, and the ability to adapt to new tasks with minimal additional data.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -2.1880946159362793,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0016440394101664424,
                        "neutral": 0.4812794029712677,
                        "support": 0.5170766115188599
                      },
                      "stance_score": 0.5154325721086934,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.3141019344329834
                    },
                    {
                      "id": 6399,
                      "faiss_score": 0.8873401880264282,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 138,
                      "sentence": "Improvements in efficiency, training stability, and integration with other components are expected to yield practical gains.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -2.438533306121826,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.0021322567481547594,
                        "neutral": 0.6527662873268127,
                        "support": 0.3451014459133148
                      },
                      "stance_score": 0.34296918916516006,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.551193118095398
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 5969,
                      "faiss_score": 0.8760039806365967,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 64,
                      "sentence": "Improvements in one area may expose constraints elsewhere.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -5.354757785797119,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.3850768208503723,
                        "neutral": 0.5729495286941528,
                        "support": 0.04197365790605545
                      },
                      "stance_score": -0.34310316294431686,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -4.4787538051605225
                    },
                    {
                      "id": 6728,
                      "faiss_score": 0.8701443076133728,
                      "faiss_rank": 7,
                      "doc_id": "local_systems_latency_throughput_tradeoffs.txt",
                      "file_type": ".txt",
                      "position": 60,
                      "sentence": "Assertions that a system improves both metrics simultaneously should be examined carefully, as such improvements usually depend on changing assumptions or workloads.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_latency_throughput_tradeoffs.txt",
                      "primary_category": null,
                      "rerank_score": -6.235987663269043,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.2782565951347351,
                        "neutral": 0.683676540851593,
                        "support": 0.03806686028838158
                      },
                      "stance_score": -0.24018973484635353,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -5.36584335565567
                    }
                  ],
                  "neutral": [
                    {
                      "id": 1877,
                      "faiss_score": 0.8633498549461365,
                      "faiss_rank": 15,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 145,
                      "sentence": "Early research demonstrated that inserting intermediate \"scratchpad\" computations could improve performance on such tasks.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -2.0456624031066895,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.00309790694154799,
                        "neutral": 0.9512084126472473,
                        "support": 0.045693691819906235
                      },
                      "stance_score": 0.042595784878358245,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.182312548160553
                    },
                    {
                      "id": 2232,
                      "faiss_score": 0.8614091277122498,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Artificial_neural_network",
                      "file_type": ".txt",
                      "position": 163,
                      "sentence": "Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                      "primary_category": "neural networks",
                      "rerank_score": -4.753089427947998,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.003027374157682061,
                        "neutral": 0.9957343935966492,
                        "support": 0.0012382006971165538
                      },
                      "stance_score": -0.0017891734605655074,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -3.8916803002357483
                    },
                    {
                      "id": 1785,
                      "faiss_score": 0.8570448756217957,
                      "faiss_rank": 19,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 53,
                      "sentence": "Cleaned datasets can increase training efficiency and lead to improved downstream performance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -7.232579231262207,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.003309796331450343,
                        "neutral": 0.9835056662559509,
                        "support": 0.01318448968231678
                      },
                      "stance_score": 0.009874693350866437,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -6.375534355640411
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Fault tolerance is a fundamental requirement for reliable distributed systems.",
      "expected_verdict": "SUPPORT",
      "predicted_verdict": "SUPPORT",
      "subclaims": [
        {
          "subclaim": "Fault tolerance is a fundamental requirement for reliable distributed systems.",
          "verdict": "MIXED",
          "controversial": true,
          "strengths": {
            "support": 1.7173915604469325,
            "contradict": 1.9065610618419537,
            "total": 3.6239526222888863
          },
          "evidence": {
            "supporting": [
              {
                "id": 3508,
                "faiss_score": 0.9106696844100952,
                "faiss_rank": 13,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 79,
                "sentence": "The basic characteristics of fault tolerance require: No single point of failure \u2013 If a system experiences a failure, it must continue to operate without interruption during the repair process.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 3.6346070766448975,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.0023842875380069017,
                  "neutral": 0.8007035255432129,
                  "support": 0.19691221415996552
                },
                "stance_score": 0.1945279266219586,
                "evidence_contribution": 0.7070325787052301,
                "combined_rank_score": 4.545276761054993
              },
              {
                "id": 6597,
                "faiss_score": 0.9170854091644287,
                "faiss_rank": 10,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Fault tolerance and reliability describe a system\u2019s ability to continue operating correctly in the presence of failures.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": 5.004147529602051,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.002577927429229021,
                  "neutral": 0.8689212799072266,
                  "support": 0.1285007894039154
                },
                "stance_score": 0.12592286197468638,
                "evidence_contribution": 0.6301365786710469,
                "combined_rank_score": 5.9212329387664795
              },
              {
                "id": 6610,
                "faiss_score": 0.9200949668884277,
                "faiss_rank": 9,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Redundancy is a fundamental technique for achieving fault tolerance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": 2.504101514816284,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.004990473855286837,
                  "neutral": 0.8381791710853577,
                  "support": 0.1568303257226944
                },
                "stance_score": 0.15183985186740756,
                "evidence_contribution": 0.38022240307065547,
                "combined_rank_score": 3.424196481704712
              }
            ],
            "contradicting": [
              {
                "id": 6601,
                "faiss_score": 0.9003157615661621,
                "faiss_rank": 16,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 4,
                "sentence": "Fault tolerance focuses on how systems respond to failures, while reliability concerns the probability that a system performs its intended function over time.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": 4.317968368530273,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.45627206563949585,
                  "neutral": 0.5289970636367798,
                  "support": 0.014730836264789104
                },
                "stance_score": -0.44154122937470675,
                "evidence_contribution": -1.9065610618419537,
                "combined_rank_score": 5.2182841300964355
              }
            ],
            "neutral": [
              {
                "id": 568,
                "faiss_score": 0.9223860502243042,
                "faiss_rank": 6,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 144,
                "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 4.329262733459473,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0938965231180191,
                  "neutral": 0.8450294137001038,
                  "support": 0.061074014753103256
                },
                "stance_score": -0.03282250836491585,
                "evidence_contribution": -0.142097262282892,
                "combined_rank_score": 5.251648783683777
              },
              {
                "id": 3525,
                "faiss_score": 0.8951352834701538,
                "faiss_rank": 18,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 96,
                "sentence": "Spare components address the first fundamental characteristic of fault tolerance in three ways: Replication: Providing multiple identical instances of the same system or subsystem, directing tasks or requests to all of them in parallel, and choosing the correct result on the basis of a quorum; Redundancy: Providing multiple identical instances of the same system and switching to one of the remaining instances in case of a failure (failover); Diversity: Providing multiple different implementations of the same specification, and using them like replicated systems to cope with errors in a specific implementation.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 3.751453399658203,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.002626709407195449,
                  "neutral": 0.9082320332527161,
                  "support": 0.08914132416248322
                },
                "stance_score": 0.08651461475528777,
                "evidence_contribution": 0.32455554564384403,
                "combined_rank_score": 4.646588683128357
              },
              {
                "id": 3738,
                "faiss_score": 0.8935599327087402,
                "faiss_rank": 20,
                "doc_id": "wiki_Byzantine_fault",
                "file_type": ".txt",
                "position": 28,
                "sentence": "The objective of Byzantine fault tolerance is to be able to defend against failures of system components with or without symptoms that prevent other components of the system from reaching an agreement among themselves, where such an agreement is needed for the correct operation of the system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Byzantine_fault",
                "primary_category": "all accuracy disputes",
                "rerank_score": 2.5237598419189453,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.007628699764609337,
                  "neutral": 0.9567516446113586,
                  "support": 0.03561968356370926
                },
                "stance_score": 0.027990983799099922,
                "evidence_contribution": 0.07064252084797218,
                "combined_rank_score": 3.4173197746276855
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is generally supported by the available evidence, with some limitations.",
        "sections": [
          {
            "type": "CONTROVERSIAL_ASPECTS",
            "items": [
              {
                "subclaim": "Fault tolerance is a fundamental requirement for reliable distributed systems.",
                "verdict": "MIXED",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6597,
                      "faiss_score": 0.9170854091644287,
                      "faiss_rank": 10,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Fault tolerance and reliability describe a system\u2019s ability to continue operating correctly in the presence of failures.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": 5.004147529602051,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.002577927429229021,
                        "neutral": 0.8689212799072266,
                        "support": 0.1285007894039154
                      },
                      "stance_score": 0.12592286197468638,
                      "evidence_contribution": 0.6301365786710469,
                      "combined_rank_score": 5.9212329387664795
                    },
                    {
                      "id": 3508,
                      "faiss_score": 0.9106696844100952,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Fault_tolerance",
                      "file_type": ".txt",
                      "position": 79,
                      "sentence": "The basic characteristics of fault tolerance require: No single point of failure \u2013 If a system experiences a failure, it must continue to operate without interruption during the repair process.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 3.6346070766448975,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.0023842875380069017,
                        "neutral": 0.8007035255432129,
                        "support": 0.19691221415996552
                      },
                      "stance_score": 0.1945279266219586,
                      "evidence_contribution": 0.7070325787052301,
                      "combined_rank_score": 4.545276761054993
                    },
                    {
                      "id": 6610,
                      "faiss_score": 0.9200949668884277,
                      "faiss_rank": 9,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Redundancy is a fundamental technique for achieving fault tolerance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": 2.504101514816284,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.004990473855286837,
                        "neutral": 0.8381791710853577,
                        "support": 0.1568303257226944
                      },
                      "stance_score": 0.15183985186740756,
                      "evidence_contribution": 0.38022240307065547,
                      "combined_rank_score": 3.424196481704712
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6601,
                      "faiss_score": 0.9003157615661621,
                      "faiss_rank": 16,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 4,
                      "sentence": "Fault tolerance focuses on how systems respond to failures, while reliability concerns the probability that a system performs its intended function over time.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": 4.317968368530273,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.45627206563949585,
                        "neutral": 0.5289970636367798,
                        "support": 0.014730836264789104
                      },
                      "stance_score": -0.44154122937470675,
                      "evidence_contribution": -1.9065610618419537,
                      "combined_rank_score": 5.2182841300964355
                    }
                  ],
                  "neutral": [
                    {
                      "id": 568,
                      "faiss_score": 0.9223860502243042,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 144,
                      "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 4.329262733459473,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0938965231180191,
                        "neutral": 0.8450294137001038,
                        "support": 0.061074014753103256
                      },
                      "stance_score": -0.03282250836491585,
                      "evidence_contribution": -0.142097262282892,
                      "combined_rank_score": 5.251648783683777
                    },
                    {
                      "id": 3525,
                      "faiss_score": 0.8951352834701538,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Fault_tolerance",
                      "file_type": ".txt",
                      "position": 96,
                      "sentence": "Spare components address the first fundamental characteristic of fault tolerance in three ways: Replication: Providing multiple identical instances of the same system or subsystem, directing tasks or requests to all of them in parallel, and choosing the correct result on the basis of a quorum; Redundancy: Providing multiple identical instances of the same system and switching to one of the remaining instances in case of a failure (failover); Diversity: Providing multiple different implementations of the same specification, and using them like replicated systems to cope with errors in a specific implementation.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 3.751453399658203,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.002626709407195449,
                        "neutral": 0.9082320332527161,
                        "support": 0.08914132416248322
                      },
                      "stance_score": 0.08651461475528777,
                      "evidence_contribution": 0.32455554564384403,
                      "combined_rank_score": 4.646588683128357
                    },
                    {
                      "id": 3738,
                      "faiss_score": 0.8935599327087402,
                      "faiss_rank": 20,
                      "doc_id": "wiki_Byzantine_fault",
                      "file_type": ".txt",
                      "position": 28,
                      "sentence": "The objective of Byzantine fault tolerance is to be able to defend against failures of system components with or without symptoms that prevent other components of the system from reaching an agreement among themselves, where such an agreement is needed for the correct operation of the system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Byzantine_fault",
                      "primary_category": "all accuracy disputes",
                      "rerank_score": 2.5237598419189453,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.007628699764609337,
                        "neutral": 0.9567516446113586,
                        "support": 0.03561968356370926
                      },
                      "stance_score": 0.027990983799099922,
                      "evidence_contribution": 0.07064252084797218,
                      "combined_rank_score": 3.4173197746276855
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Increasing computational resources can improve the training performance of deep learning models.",
      "expected_verdict": "SUPPORT",
      "predicted_verdict": "SUPPORT",
      "subclaims": [
        {
          "subclaim": "Increasing computational resources can improve the training performance of deep learning models.",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 6.743168849386467,
            "contradict": 0.0,
            "total": 6.743168849386467
          },
          "evidence": {
            "supporting": [
              {
                "id": 2622,
                "faiss_score": 0.8935152292251587,
                "faiss_rank": 6,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 217,
                "sentence": "Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 2.851893901824951,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0011127914767712355,
                  "neutral": 0.14476646482944489,
                  "support": 0.8541207909584045
                },
                "stance_score": 0.8530079994816333,
                "evidence_contribution": 2.4326883119295712,
                "combined_rank_score": 3.74540913105011
              },
              {
                "id": 5906,
                "faiss_score": 0.8968743085861206,
                "faiss_rank": 4,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 4.011983871459961,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.029276547953486443,
                  "neutral": 0.37700629234313965,
                  "support": 0.5937171578407288
                },
                "stance_score": 0.5644406098872423,
                "evidence_contribution": 2.26452662326464,
                "combined_rank_score": 4.9088581800460815
              },
              {
                "id": 1901,
                "faiss_score": 0.8911174535751343,
                "faiss_rank": 9,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 169,
                "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 3.919332504272461,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.004067023750394583,
                  "neutral": 0.642042338848114,
                  "support": 0.35389062762260437
                },
                "stance_score": 0.3498236038722098,
                "evidence_contribution": 1.3710750214180853,
                "combined_rank_score": 4.810449957847595
              },
              {
                "id": 1416,
                "faiss_score": 0.8915122747421265,
                "faiss_rank": 8,
                "doc_id": "wiki_Overfitting",
                "file_type": ".txt",
                "position": 66,
                "sentence": "Increase the amount of training data: If the model is underfitting due to a lack of data, increasing the amount of training data may help.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                "primary_category": "machine learning",
                "rerank_score": 2.29343318939209,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.008432808332145214,
                  "neutral": 0.6888686418533325,
                  "support": 0.3026985824108124
                },
                "stance_score": 0.29426577407866716,
                "evidence_contribution": 0.6748788927741698,
                "combined_rank_score": 3.1849454641342163
              },
              {
                "id": 1785,
                "faiss_score": 0.8816750049591064,
                "faiss_rank": 17,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 53,
                "sentence": "Cleaned datasets can increase training efficiency and lead to improved downstream performance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -0.03195880353450775,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.005137357860803604,
                  "neutral": 0.752930760383606,
                  "support": 0.24193188548088074
                },
                "stance_score": 0.23679452762007713,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.8497162014245987
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 5907,
                "faiss_score": 0.8959633708000183,
                "faiss_rank": 5,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 2,
                "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 2.638784170150757,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.07758736610412598,
                  "neutral": 0.8997015953063965,
                  "support": 0.022711053490638733
                },
                "stance_score": -0.054876312613487244,
                "evidence_contribution": -0.14480674504071445,
                "combined_rank_score": 3.534747540950775
              },
              {
                "id": 6044,
                "faiss_score": 0.8992515802383423,
                "faiss_rank": 3,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 4,
                "sentence": "Increasing the number of parameters, the amount of training data, and the total compute used during training has been shown to produce consistent improvements in performance across many benchmarks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 2.371992588043213,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0009964535711333156,
                  "neutral": 0.9319743514060974,
                  "support": 0.06702922284603119
                },
                "stance_score": 0.06603276927489787,
                "evidence_contribution": 0.15662923928802536,
                "combined_rank_score": 3.271244168281555
              },
              {
                "id": 6092,
                "faiss_score": 0.8839632868766785,
                "faiss_rank": 12,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 52,
                "sentence": "The computational cost of training large language models is substantial.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 0.24331992864608765,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.10281243920326233,
                  "neutral": 0.8933481574058533,
                  "support": 0.0038394108414649963
                },
                "stance_score": -0.09897302836179733,
                "evidence_contribution": -0.024082110198879736,
                "combined_rank_score": 1.1272832155227661
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is generally supported by the available evidence, with some limitations.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Increasing computational resources can improve the training performance of deep learning models.",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 5906,
                      "faiss_score": 0.8968743085861206,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 1,
                      "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 4.011983871459961,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.029276547953486443,
                        "neutral": 0.37700629234313965,
                        "support": 0.5937171578407288
                      },
                      "stance_score": 0.5644406098872423,
                      "evidence_contribution": 2.26452662326464,
                      "combined_rank_score": 4.9088581800460815
                    },
                    {
                      "id": 1901,
                      "faiss_score": 0.8911174535751343,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 169,
                      "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 3.919332504272461,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.004067023750394583,
                        "neutral": 0.642042338848114,
                        "support": 0.35389062762260437
                      },
                      "stance_score": 0.3498236038722098,
                      "evidence_contribution": 1.3710750214180853,
                      "combined_rank_score": 4.810449957847595
                    },
                    {
                      "id": 2622,
                      "faiss_score": 0.8935152292251587,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 217,
                      "sentence": "Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": 2.851893901824951,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0011127914767712355,
                        "neutral": 0.14476646482944489,
                        "support": 0.8541207909584045
                      },
                      "stance_score": 0.8530079994816333,
                      "evidence_contribution": 2.4326883119295712,
                      "combined_rank_score": 3.74540913105011
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5907,
                      "faiss_score": 0.8959633708000183,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 2.638784170150757,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.07758736610412598,
                        "neutral": 0.8997015953063965,
                        "support": 0.022711053490638733
                      },
                      "stance_score": -0.054876312613487244,
                      "evidence_contribution": -0.14480674504071445,
                      "combined_rank_score": 3.534747540950775
                    },
                    {
                      "id": 6044,
                      "faiss_score": 0.8992515802383423,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 4,
                      "sentence": "Increasing the number of parameters, the amount of training data, and the total compute used during training has been shown to produce consistent improvements in performance across many benchmarks.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 2.371992588043213,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0009964535711333156,
                        "neutral": 0.9319743514060974,
                        "support": 0.06702922284603119
                      },
                      "stance_score": 0.06603276927489787,
                      "evidence_contribution": 0.15662923928802536,
                      "combined_rank_score": 3.271244168281555
                    },
                    {
                      "id": 6092,
                      "faiss_score": 0.8839632868766785,
                      "faiss_rank": 12,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 52,
                      "sentence": "The computational cost of training large language models is substantial.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 0.24331992864608765,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.10281243920326233,
                        "neutral": 0.8933481574058533,
                        "support": 0.0038394108414649963
                      },
                      "stance_score": -0.09897302836179733,
                      "evidence_contribution": -0.024082110198879736,
                      "combined_rank_score": 1.1272832155227661
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Large language models can perform multiple language tasks without task-specific fine-tuning.",
      "expected_verdict": "SUPPORT",
      "predicted_verdict": "SUPPORT",
      "subclaims": [
        {
          "subclaim": "Large language models can perform multiple language tasks",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 12.668652140865667,
            "contradict": 0.0,
            "total": 12.668652140865667
          },
          "evidence": {
            "supporting": [
              {
                "id": 6079,
                "faiss_score": 0.9092576503753662,
                "faiss_rank": 2,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "Large language models are often described as general-purpose systems because they can be adapted to many tasks with minimal fine-tuning.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 6.446345329284668,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0010351998498663306,
                  "neutral": 0.16859209537506104,
                  "support": 0.8303727507591248
                },
                "stance_score": 0.8293375509092584,
                "evidence_contribution": 5.346196247704284,
                "combined_rank_score": 7.355602979660034
              },
              {
                "id": 6042,
                "faiss_score": 0.8853220343589783,
                "faiss_rank": 11,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 2,
                "sentence": "This simple training signal, when combined with large datasets and high model capacity, produces systems that can generate coherent text, answer questions, summarize documents, and perform a wide variety of language-related tasks without explicit task-specific programming.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 4.042299270629883,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0015242607332766056,
                  "neutral": 0.19606323540210724,
                  "support": 0.8024125099182129
                },
                "stance_score": 0.8008882491849363,
                "evidence_contribution": 3.237429985536312,
                "combined_rank_score": 4.927621304988861
              },
              {
                "id": 6040,
                "faiss_score": 0.9093468189239502,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 5.019293785095215,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0017207523342221975,
                  "neutral": 0.6858500242233276,
                  "support": 0.312429279088974
                },
                "stance_score": 0.3107085267547518,
                "evidence_contribution": 1.559537377316216,
                "combined_rank_score": 5.928640604019165
              },
              {
                "id": 6121,
                "faiss_score": 0.9074077606201172,
                "faiss_rank": 3,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 5.14181661605835,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.007413278333842754,
                  "neutral": 0.82762211561203,
                  "support": 0.16496461629867554
                },
                "stance_score": 0.15755133796483278,
                "evidence_contribution": 0.8101000874298019,
                "combined_rank_score": 6.049224376678467
              },
              {
                "id": 6043,
                "faiss_score": 0.8912434577941895,
                "faiss_rank": 6,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 3.5135903358459473,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.002225017175078392,
                  "neutral": 0.8620526790618896,
                  "support": 0.13572227954864502
                },
                "stance_score": 0.13349726237356663,
                "evidence_contribution": 0.4690546909376545,
                "combined_rank_score": 4.404833793640137
              },
              {
                "id": 2185,
                "faiss_score": 0.8812185525894165,
                "faiss_rank": 17,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 116,
                "sentence": "Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": 3.2154648303985596,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.001421264372766018,
                  "neutral": 0.8628906011581421,
                  "support": 0.1356882005929947
                },
                "stance_score": 0.13426693622022867,
                "evidence_contribution": 0.4317306113015118,
                "combined_rank_score": 4.096683382987976
              },
              {
                "id": 6047,
                "faiss_score": 0.883418083190918,
                "faiss_rank": 14,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Despite these capabilities, the behavior of large language models remains fundamentally probabilistic.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 3.7385928630828857,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.0030133984982967377,
                  "neutral": 0.8811923265457153,
                  "support": 0.11579427868127823
                },
                "stance_score": 0.11278088018298149,
                "evidence_contribution": 0.42164179374430066,
                "combined_rank_score": 4.622010946273804
              },
              {
                "id": 6115,
                "faiss_score": 0.8896223306655884,
                "faiss_rank": 7,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 75,
                "sentence": "Large language models also influence how users interact with technology.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 3.2091166973114014,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.0022298081312328577,
                  "neutral": 0.8730888366699219,
                  "support": 0.12468136847019196
                },
                "stance_score": 0.1224515603389591,
                "evidence_contribution": 0.3929613468955882,
                "combined_rank_score": 4.09873902797699
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 3009,
                "faiss_score": 0.8790433406829834,
                "faiss_rank": 19,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 64,
                "sentence": "In general, there are 3 classes of language modelling tasks: \"masked\", \"autoregressive\", and \"prefixLM\".",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": 3.828814744949341,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.005533924791961908,
                  "neutral": 0.9516805410385132,
                  "support": 0.04278552904725075
                },
                "stance_score": 0.03725160425528884,
                "evidence_contribution": 0.14262949164566752,
                "combined_rank_score": 4.707858085632324
              },
              {
                "id": 6099,
                "faiss_score": 0.8944740891456604,
                "faiss_rank": 5,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 59,
                "sentence": "Large language models are increasingly deployed as components within larger systems rather than standalone tools.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 3.510206699371338,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0012955378042533994,
                  "neutral": 0.9265615344047546,
                  "support": 0.07214298099279404
                },
                "stance_score": 0.07084744318854064,
                "evidence_contribution": 0.2486891697137456,
                "combined_rank_score": 4.404680788516998
              }
            ]
          }
        },
        {
          "subclaim": "without task-specific fine-tuning",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 6.715901575618437,
            "contradict": 0.0,
            "total": 6.715901575618437
          },
          "evidence": {
            "supporting": [
              {
                "id": 1833,
                "faiss_score": 0.8788925409317017,
                "faiss_rank": 5,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 101,
                "sentence": "This technique, called few-shot prompting, allows LLMs to be adapted to any task without requiring fine-tuning.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 5.452361583709717,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0010085511021316051,
                  "neutral": 0.02543622814118862,
                  "support": 0.9735552668571472
                },
                "stance_score": 0.9725467157550156,
                "evidence_contribution": 5.302676351345701,
                "combined_rank_score": 6.3312541246414185
              },
              {
                "id": 6079,
                "faiss_score": 0.8675508499145508,
                "faiss_rank": 14,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "Large language models are often described as general-purpose systems because they can be adapted to many tasks with minimal fine-tuning.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 0.734641432762146,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0036333799362182617,
                  "neutral": 0.07261178642511368,
                  "support": 0.9237548112869263
                },
                "stance_score": 0.920121431350708,
                "evidence_contribution": 0.6759593266426407,
                "combined_rank_score": 1.6021922826766968
              },
              {
                "id": 2566,
                "faiss_score": 0.875229001045227,
                "faiss_rank": 7,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 161,
                "sentence": "Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 0.4810207486152649,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0010565657867118716,
                  "neutral": 0.06393749266862869,
                  "support": 0.9350059628486633
                },
                "stance_score": 0.9339493970619515,
                "evidence_contribution": 0.4492490381435152,
                "combined_rank_score": 1.356249749660492
              },
              {
                "id": 6432,
                "faiss_score": 0.8805605173110962,
                "faiss_rank": 4,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 171,
                "sentence": "By framing tasks as variations of a common input-output format, practitioners can leverage pretrained models without extensive task-specific training.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 0.30624258518218994,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0018275665352120996,
                  "neutral": 0.055858906358480453,
                  "support": 0.942313551902771
                },
                "stance_score": 0.9404859853675589,
                "evidence_contribution": 0.2880168594865805,
                "combined_rank_score": 1.1868031024932861
              },
              {
                "id": 287,
                "faiss_score": 0.8709167242050171,
                "faiss_rank": 11,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 178,
                "sentence": "Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -2.324951648712158,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.0012470762012526393,
                  "neutral": 0.12183920294046402,
                  "support": 0.8769136667251587
                },
                "stance_score": 0.875666590523906,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.4540349245071411
              }
            ],
            "contradicting": [
              {
                "id": 6085,
                "faiss_score": 0.8813949823379517,
                "faiss_rank": 3,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 45,
                "sentence": "Fine-tuning is commonly used to adapt large language models to specific domains or behaviors.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -0.16154561936855316,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.9929386973381042,
                  "neutral": 0.005900803487747908,
                  "support": 0.001160479267127812
                },
                "stance_score": -0.9917782180709764,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.7198493629693985
              },
              {
                "id": 1802,
                "faiss_score": 0.8630768656730652,
                "faiss_rank": 18,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 70,
                "sentence": "Instruction fine-tuning is a form of supervised learning used to teach LLMs to follow user instructions.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -0.5821276307106018,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.9954550266265869,
                  "neutral": 0.0036955545656383038,
                  "support": 0.0008494564681313932
                },
                "stance_score": -0.9946055701584555,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.2809492349624634
              },
              {
                "id": 6335,
                "faiss_score": 0.8739376664161682,
                "faiss_rank": 8,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 74,
                "sentence": "These pretrained models are then adapted to downstream tasks through fine-tuning or lightweight adaptation mechanisms.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -0.7192176580429077,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.9972085356712341,
                  "neutral": 0.002253232290968299,
                  "support": 0.0005383074167184532
                },
                "stance_score": -0.9966702282545157,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.1547200083732605
              },
              {
                "id": 6087,
                "faiss_score": 0.8714344501495361,
                "faiss_rank": 10,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 47,
                "sentence": "While fine-tuning can improve performance, it may also introduce overfitting or reduce generality if not carefully managed.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -1.157218337059021,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.9041840434074402,
                  "neutral": 0.08107355237007141,
                  "support": 0.014742383733391762
                },
                "stance_score": -0.8894416596740484,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.28578388690948486
              },
              {
                "id": 6088,
                "faiss_score": 0.8841488361358643,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 48,
                "sentence": "Parameter-efficient fine-tuning methods aim to mitigate these risks by modifying only a subset of parameters.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -2.0797901153564453,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.9841687083244324,
                  "neutral": 0.014658700674772263,
                  "support": 0.001172599266283214
                },
                "stance_score": -0.9829961090581492,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.195641279220581
              }
            ],
            "neutral": []
          }
        }
      ],
      "explanation": {
        "summary": "The claim is generally supported by the available evidence, with some limitations.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Large language models can perform multiple language tasks",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6079,
                      "faiss_score": 0.9092576503753662,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 39,
                      "sentence": "Large language models are often described as general-purpose systems because they can be adapted to many tasks with minimal fine-tuning.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 6.446345329284668,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0010351998498663306,
                        "neutral": 0.16859209537506104,
                        "support": 0.8303727507591248
                      },
                      "stance_score": 0.8293375509092584,
                      "evidence_contribution": 5.346196247704284,
                      "combined_rank_score": 7.355602979660034
                    },
                    {
                      "id": 6121,
                      "faiss_score": 0.9074077606201172,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 5.14181661605835,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.007413278333842754,
                        "neutral": 0.82762211561203,
                        "support": 0.16496461629867554
                      },
                      "stance_score": 0.15755133796483278,
                      "evidence_contribution": 0.8101000874298019,
                      "combined_rank_score": 6.049224376678467
                    },
                    {
                      "id": 6040,
                      "faiss_score": 0.9093468189239502,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 5.019293785095215,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0017207523342221975,
                        "neutral": 0.6858500242233276,
                        "support": 0.312429279088974
                      },
                      "stance_score": 0.3107085267547518,
                      "evidence_contribution": 1.559537377316216,
                      "combined_rank_score": 5.928640604019165
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 3009,
                      "faiss_score": 0.8790433406829834,
                      "faiss_rank": 19,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 64,
                      "sentence": "In general, there are 3 classes of language modelling tasks: \"masked\", \"autoregressive\", and \"prefixLM\".",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": 3.828814744949341,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.005533924791961908,
                        "neutral": 0.9516805410385132,
                        "support": 0.04278552904725075
                      },
                      "stance_score": 0.03725160425528884,
                      "evidence_contribution": 0.14262949164566752,
                      "combined_rank_score": 4.707858085632324
                    },
                    {
                      "id": 6099,
                      "faiss_score": 0.8944740891456604,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 59,
                      "sentence": "Large language models are increasingly deployed as components within larger systems rather than standalone tools.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 3.510206699371338,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.0012955378042533994,
                        "neutral": 0.9265615344047546,
                        "support": 0.07214298099279404
                      },
                      "stance_score": 0.07084744318854064,
                      "evidence_contribution": 0.2486891697137456,
                      "combined_rank_score": 4.404680788516998
                    }
                  ]
                }
              },
              {
                "subclaim": "without task-specific fine-tuning",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 1833,
                      "faiss_score": 0.8788925409317017,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 101,
                      "sentence": "This technique, called few-shot prompting, allows LLMs to be adapted to any task without requiring fine-tuning.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 5.452361583709717,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0010085511021316051,
                        "neutral": 0.02543622814118862,
                        "support": 0.9735552668571472
                      },
                      "stance_score": 0.9725467157550156,
                      "evidence_contribution": 5.302676351345701,
                      "combined_rank_score": 6.3312541246414185
                    },
                    {
                      "id": 6079,
                      "faiss_score": 0.8675508499145508,
                      "faiss_rank": 14,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 39,
                      "sentence": "Large language models are often described as general-purpose systems because they can be adapted to many tasks with minimal fine-tuning.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 0.734641432762146,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0036333799362182617,
                        "neutral": 0.07261178642511368,
                        "support": 0.9237548112869263
                      },
                      "stance_score": 0.920121431350708,
                      "evidence_contribution": 0.6759593266426407,
                      "combined_rank_score": 1.6021922826766968
                    },
                    {
                      "id": 2566,
                      "faiss_score": 0.875229001045227,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 161,
                      "sentence": "Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": 0.4810207486152649,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0010565657867118716,
                        "neutral": 0.06393749266862869,
                        "support": 0.9350059628486633
                      },
                      "stance_score": 0.9339493970619515,
                      "evidence_contribution": 0.4492490381435152,
                      "combined_rank_score": 1.356249749660492
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6085,
                      "faiss_score": 0.8813949823379517,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 45,
                      "sentence": "Fine-tuning is commonly used to adapt large language models to specific domains or behaviors.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -0.16154561936855316,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.9929386973381042,
                        "neutral": 0.005900803487747908,
                        "support": 0.001160479267127812
                      },
                      "stance_score": -0.9917782180709764,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.7198493629693985
                    },
                    {
                      "id": 1802,
                      "faiss_score": 0.8630768656730652,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 70,
                      "sentence": "Instruction fine-tuning is a form of supervised learning used to teach LLMs to follow user instructions.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -0.5821276307106018,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.9954550266265869,
                        "neutral": 0.0036955545656383038,
                        "support": 0.0008494564681313932
                      },
                      "stance_score": -0.9946055701584555,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.2809492349624634
                    },
                    {
                      "id": 6335,
                      "faiss_score": 0.8739376664161682,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 74,
                      "sentence": "These pretrained models are then adapted to downstream tasks through fine-tuning or lightweight adaptation mechanisms.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -0.7192176580429077,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.9972085356712341,
                        "neutral": 0.002253232290968299,
                        "support": 0.0005383074167184532
                      },
                      "stance_score": -0.9966702282545157,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.1547200083732605
                    }
                  ],
                  "neutral": []
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Distributed consensus protocols help systems remain consistent despite node failures.",
      "expected_verdict": "SUPPORT",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Distributed consensus protocols help systems remain consistent",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 1.8874435073039542,
            "total": 1.8874435073039542
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 5618,
                "faiss_score": 0.9274933934211731,
                "faiss_rank": 1,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Consensus is a fundamental problem in distributed systems that captures the difficulty of agreement in the presence of failures.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 2.1761515140533447,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.8703222274780273,
                  "neutral": 0.12668642401695251,
                  "support": 0.002991301706060767
                },
                "stance_score": -0.8673309257719666,
                "evidence_contribution": -1.8874435073039542,
                "combined_rank_score": 3.103644907474518
              },
              {
                "id": 3637,
                "faiss_score": 0.9029970765113831,
                "faiss_rank": 5,
                "doc_id": "wiki_Consensus_algorithm",
                "file_type": ".txt",
                "position": 46,
                "sentence": "The consensus problem may be considered in the case of asynchronous or synchronous systems.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Consensus_(computer_science)",
                "primary_category": "articles with short description",
                "rerank_score": -1.7597379684448242,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.18643596768379211,
                  "neutral": 0.8109642267227173,
                  "support": 0.0025998535566031933
                },
                "stance_score": -0.18383611412718892,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.8567408919334412
              }
            ],
            "neutral": [
              {
                "id": 5612,
                "faiss_score": 0.888638973236084,
                "faiss_rank": 18,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 15,
                "sentence": "Strong consistency models aim to make distributed systems behave as if there were a single shared state, but enforcing such behavior requires coordination and synchronization, which can be expensive or impossible under certain failure conditions.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 3.147623062133789,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.018985481932759285,
                  "neutral": 0.9624131917953491,
                  "support": 0.01860135607421398
                },
                "stance_score": -0.00038412585854530334,
                "evidence_contribution": -0.0012090834111191384,
                "combined_rank_score": 4.036262035369873
              },
              {
                "id": 5610,
                "faiss_score": 0.9084477424621582,
                "faiss_rank": 3,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Consistency is a central concept in distributed systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 1.8636924028396606,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.000766043784096837,
                  "neutral": 0.9977141618728638,
                  "support": 0.0015198341570794582
                },
                "stance_score": 0.0007537903729826212,
                "evidence_contribution": 0.0014048333914613853,
                "combined_rank_score": 2.772140145301819
              },
              {
                "id": 3592,
                "faiss_score": 0.9013365507125854,
                "faiss_rank": 6,
                "doc_id": "wiki_Consensus_algorithm",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Some of the processes (agents) may fail or be unreliable in other ways, so consensus protocols must be fault-tolerant or resilient.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Consensus_(computer_science)",
                "primary_category": "articles with short description",
                "rerank_score": 0.459737092256546,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0038521725218743086,
                  "neutral": 0.950269341468811,
                  "support": 0.045878443866968155
                },
                "stance_score": 0.042026271345093846,
                "evidence_contribution": 0.019321035786578046,
                "combined_rank_score": 1.3610736429691315
              }
            ]
          }
        },
        {
          "subclaim": "Systems remain consistent despite node failures",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 5.206657191986313,
            "contradict": 0.0,
            "total": 5.206657191986313
          },
          "evidence": {
            "supporting": [
              {
                "id": 3474,
                "faiss_score": 0.9279701709747314,
                "faiss_rank": 1,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 45,
                "sentence": "Resilient networks continue to transmit data despite the failure of some links or nodes.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 2.9349451065063477,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.001138089457526803,
                  "neutral": 0.016104355454444885,
                  "support": 0.9827576279640198
                },
                "stance_score": 0.981619538506493,
                "evidence_contribution": 2.880999460990651,
                "combined_rank_score": 3.862915277481079
              },
              {
                "id": 5615,
                "faiss_score": 0.9032431840896606,
                "faiss_rank": 3,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "By maintaining multiple copies of data across different nodes, a system can continue to operate even if some replicas fail.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 2.553046703338623,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0009699473739601672,
                  "neutral": 0.08712581545114517,
                  "support": 0.9119042158126831
                },
                "stance_score": 0.9109342684387229,
                "evidence_contribution": 2.325657730995662,
                "combined_rank_score": 3.4562898874282837
              },
              {
                "id": 3463,
                "faiss_score": 0.9054452180862427,
                "faiss_rank": 2,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 34,
                "sentence": "A highly fault-tolerant system might continue at the same level of performance even though one or more components have failed.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": -2.004009246826172,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.00400190707296133,
                  "neutral": 0.46762439608573914,
                  "support": 0.5283737182617188
                },
                "stance_score": 0.5243718111887574,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.0985640287399292
              },
              {
                "id": 430,
                "faiss_score": 0.8767738342285156,
                "faiss_rank": 17,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 6,
                "sentence": "Other typical properties of distributed systems are: The system must tolerate failures in individual computers.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": -4.0686445236206055,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.034422557801008224,
                  "neutral": 0.791019082069397,
                  "support": 0.17455832660198212
                },
                "stance_score": 0.1401357688009739,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.19187068939209
              }
            ],
            "contradicting": [
              {
                "id": 6603,
                "faiss_score": 0.8794848322868347,
                "faiss_rank": 11,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 6,
                "sentence": "A system may tolerate certain failures gracefully yet still exhibit low overall reliability if failures occur frequently.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": -0.1449195146560669,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.7329874634742737,
                  "neutral": 0.24647438526153564,
                  "support": 0.020538147538900375
                },
                "stance_score": -0.7124493159353733,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.7345653176307678
              },
              {
                "id": 6605,
                "faiss_score": 0.8778896331787109,
                "faiss_rank": 15,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 8,
                "sentence": "Failures in computing systems take many forms.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": -2.2687134742736816,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.8227906823158264,
                  "neutral": 0.1751573234796524,
                  "support": 0.0020519725512713194
                },
                "stance_score": -0.8207387097645551,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.3908238410949707
              },
              {
                "id": 6659,
                "faiss_score": 0.878364622592926,
                "faiss_rank": 14,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 62,
                "sentence": "In large-scale systems, failures are often correlated rather than independent.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": -3.2208199501037598,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.4171348512172699,
                  "neutral": 0.5790106058120728,
                  "support": 0.0038546037394553423
                },
                "stance_score": -0.41328024747781456,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.3424553275108337
              },
              {
                "id": 5605,
                "faiss_score": 0.8787782192230225,
                "faiss_rank": 13,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 8,
                "sentence": "Failures are another fundamental aspect of distributed systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": -3.792255401611328,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.11729293316602707,
                  "neutral": 0.876907467842102,
                  "support": 0.005799655802547932
                },
                "stance_score": -0.11149327736347914,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.9134771823883057
              }
            ],
            "neutral": []
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Systems remain consistent despite node failures",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 3474,
                      "faiss_score": 0.9279701709747314,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Fault_tolerance",
                      "file_type": ".txt",
                      "position": 45,
                      "sentence": "Resilient networks continue to transmit data despite the failure of some links or nodes.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 2.9349451065063477,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.001138089457526803,
                        "neutral": 0.016104355454444885,
                        "support": 0.9827576279640198
                      },
                      "stance_score": 0.981619538506493,
                      "evidence_contribution": 2.880999460990651,
                      "combined_rank_score": 3.862915277481079
                    },
                    {
                      "id": 5615,
                      "faiss_score": 0.9032431840896606,
                      "faiss_rank": 3,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "By maintaining multiple copies of data across different nodes, a system can continue to operate even if some replicas fail.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 2.553046703338623,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0009699473739601672,
                        "neutral": 0.08712581545114517,
                        "support": 0.9119042158126831
                      },
                      "stance_score": 0.9109342684387229,
                      "evidence_contribution": 2.325657730995662,
                      "combined_rank_score": 3.4562898874282837
                    },
                    {
                      "id": 3463,
                      "faiss_score": 0.9054452180862427,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Fault_tolerance",
                      "file_type": ".txt",
                      "position": 34,
                      "sentence": "A highly fault-tolerant system might continue at the same level of performance even though one or more components have failed.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": -2.004009246826172,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.00400190707296133,
                        "neutral": 0.46762439608573914,
                        "support": 0.5283737182617188
                      },
                      "stance_score": 0.5243718111887574,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.0985640287399292
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6603,
                      "faiss_score": 0.8794848322868347,
                      "faiss_rank": 11,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 6,
                      "sentence": "A system may tolerate certain failures gracefully yet still exhibit low overall reliability if failures occur frequently.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": -0.1449195146560669,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.7329874634742737,
                        "neutral": 0.24647438526153564,
                        "support": 0.020538147538900375
                      },
                      "stance_score": -0.7124493159353733,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.7345653176307678
                    },
                    {
                      "id": 6605,
                      "faiss_score": 0.8778896331787109,
                      "faiss_rank": 15,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 8,
                      "sentence": "Failures in computing systems take many forms.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": -2.2687134742736816,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.8227906823158264,
                        "neutral": 0.1751573234796524,
                        "support": 0.0020519725512713194
                      },
                      "stance_score": -0.8207387097645551,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.3908238410949707
                    },
                    {
                      "id": 6659,
                      "faiss_score": 0.878364622592926,
                      "faiss_rank": 14,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 62,
                      "sentence": "In large-scale systems, failures are often correlated rather than independent.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": -3.2208199501037598,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.4171348512172699,
                        "neutral": 0.5790106058120728,
                        "support": 0.0038546037394553423
                      },
                      "stance_score": -0.41328024747781456,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -2.3424553275108337
                    }
                  ],
                  "neutral": []
                }
              }
            ]
          },
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Distributed consensus protocols help systems remain consistent",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 5618,
                      "faiss_score": 0.9274933934211731,
                      "faiss_rank": 1,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Consensus is a fundamental problem in distributed systems that captures the difficulty of agreement in the presence of failures.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 2.1761515140533447,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.8703222274780273,
                        "neutral": 0.12668642401695251,
                        "support": 0.002991301706060767
                      },
                      "stance_score": -0.8673309257719666,
                      "evidence_contribution": -1.8874435073039542,
                      "combined_rank_score": 3.103644907474518
                    },
                    {
                      "id": 3637,
                      "faiss_score": 0.9029970765113831,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Consensus_algorithm",
                      "file_type": ".txt",
                      "position": 46,
                      "sentence": "The consensus problem may be considered in the case of asynchronous or synchronous systems.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Consensus_(computer_science)",
                      "primary_category": "articles with short description",
                      "rerank_score": -1.7597379684448242,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.18643596768379211,
                        "neutral": 0.8109642267227173,
                        "support": 0.0025998535566031933
                      },
                      "stance_score": -0.18383611412718892,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.8567408919334412
                    }
                  ],
                  "neutral": [
                    {
                      "id": 5612,
                      "faiss_score": 0.888638973236084,
                      "faiss_rank": 18,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 15,
                      "sentence": "Strong consistency models aim to make distributed systems behave as if there were a single shared state, but enforcing such behavior requires coordination and synchronization, which can be expensive or impossible under certain failure conditions.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 3.147623062133789,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.018985481932759285,
                        "neutral": 0.9624131917953491,
                        "support": 0.01860135607421398
                      },
                      "stance_score": -0.00038412585854530334,
                      "evidence_contribution": -0.0012090834111191384,
                      "combined_rank_score": 4.036262035369873
                    },
                    {
                      "id": 5610,
                      "faiss_score": 0.9084477424621582,
                      "faiss_rank": 3,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Consistency is a central concept in distributed systems.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 1.8636924028396606,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.000766043784096837,
                        "neutral": 0.9977141618728638,
                        "support": 0.0015198341570794582
                      },
                      "stance_score": 0.0007537903729826212,
                      "evidence_contribution": 0.0014048333914613853,
                      "combined_rank_score": 2.772140145301819
                    },
                    {
                      "id": 3592,
                      "faiss_score": 0.9013365507125854,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Consensus_algorithm",
                      "file_type": ".txt",
                      "position": 1,
                      "sentence": "Some of the processes (agents) may fail or be unreliable in other ways, so consensus protocols must be fault-tolerant or resilient.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Consensus_(computer_science)",
                      "primary_category": "articles with short description",
                      "rerank_score": 0.459737092256546,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.0038521725218743086,
                        "neutral": 0.950269341468811,
                        "support": 0.045878443866968155
                      },
                      "stance_score": 0.042026271345093846,
                      "evidence_contribution": 0.019321035786578046,
                      "combined_rank_score": 1.3610736429691315
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Increasing model size always guarantees better generalization performance.",
      "expected_verdict": "CONTRADICT",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Increasing model size always guarantees better generalization performance.",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 9.356575939596496,
            "total": 9.356575939596496
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 6133,
                "faiss_score": 0.8974666595458984,
                "faiss_rank": 15,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 5.871306896209717,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.904425859451294,
                  "neutral": 0.09184230118989944,
                  "support": 0.003731856355443597
                },
                "stance_score": -0.9006940030958503,
                "evidence_contribution": -5.288250911751402,
                "combined_rank_score": 6.768773555755615
              },
              {
                "id": 5941,
                "faiss_score": 0.901360034942627,
                "faiss_rank": 9,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 36,
                "sentence": "Smaller or compressed models may generalize better due to implicit regularization, but excessive compression can harm performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 2.947385549545288,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.9887085556983948,
                  "neutral": 0.010507587343454361,
                  "support": 0.0007837332668714225
                },
                "stance_score": -0.9879248224315234,
                "evidence_contribution": -2.9117953456717665,
                "combined_rank_score": 3.848745584487915
              },
              {
                "id": 1453,
                "faiss_score": 0.9143708348274231,
                "faiss_rank": 4,
                "doc_id": "wiki_Regularization_(mathematics)",
                "file_type": ".txt",
                "position": 25,
                "sentence": "By regularizing for time, model complexity can be controlled, improving generalization.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
                "primary_category": "articles with short description",
                "rerank_score": 1.1829490661621094,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.9792255163192749,
                  "neutral": 0.019215479493141174,
                  "support": 0.0015590087277814746
                },
                "stance_score": -0.9776665075914934,
                "evidence_contribution": -1.156529682173328,
                "combined_rank_score": 2.0973199009895325
              },
              {
                "id": 5922,
                "faiss_score": 0.8979805111885071,
                "faiss_rank": 14,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 17,
                "sentence": "Distilled models often achieve better performance than models trained directly on the same data, given similar size constraints.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -1.044982671737671,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.7945719957351685,
                  "neutral": 0.20232698321342468,
                  "support": 0.0031010955572128296
                },
                "stance_score": -0.7914709001779556,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.14700216054916382
              },
              {
                "id": 5906,
                "faiss_score": 0.895328938961029,
                "faiss_rank": 18,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -1.0923975706100464,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.8033847808837891,
                  "neutral": 0.19272951781749725,
                  "support": 0.0038856947794556618
                },
                "stance_score": -0.7994990861043334,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.19706863164901733
              },
              {
                "id": 6221,
                "faiss_score": 0.900503396987915,
                "faiss_rank": 10,
                "doc_id": "local_ml_training_dynamics.txt",
                "file_type": ".txt",
                "position": 29,
                "sentence": "Surprisingly, such models can still generalize well.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                "primary_category": null,
                "rerank_score": -1.5820221900939941,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.26446568965911865,
                  "neutral": 0.7334563732147217,
                  "support": 0.002077879384160042
                },
                "stance_score": -0.2623878102749586,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.6815187931060791
              },
              {
                "id": 6211,
                "faiss_score": 0.9146316051483154,
                "faiss_rank": 3,
                "doc_id": "local_ml_training_dynamics.txt",
                "file_type": ".txt",
                "position": 19,
                "sentence": "Batch size influences both optimization efficiency and generalization.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                "primary_category": null,
                "rerank_score": -3.0013480186462402,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.12615004181861877,
                  "neutral": 0.8723376989364624,
                  "support": 0.0015121976612135768
                },
                "stance_score": -0.1246378441574052,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.086716413497925
              },
              {
                "id": 1443,
                "faiss_score": 0.8999907970428467,
                "faiss_rank": 11,
                "doc_id": "wiki_Regularization_(mathematics)",
                "file_type": ".txt",
                "position": 15,
                "sentence": "Regularization can be motivated as a technique to improve the generalizability of a learned model.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
                "primary_category": "articles with short description",
                "rerank_score": -3.3324716091156006,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.5267986059188843,
                  "neutral": 0.4700230360031128,
                  "support": 0.0031784214079380035
                },
                "stance_score": -0.5236201845109463,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.432480812072754
              }
            ],
            "neutral": [
              {
                "id": 6137,
                "faiss_score": 0.9333542585372925,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 3.651726245880127,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.08033034205436707,
                  "neutral": 0.8913941979408264,
                  "support": 0.028275374323129654
                },
                "stance_score": -0.05205496773123741,
                "evidence_contribution": -0.19009049189260274,
                "combined_rank_score": 4.585080504417419
              },
              {
                "id": 6127,
                "faiss_score": 0.9018120765686035,
                "faiss_rank": 8,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "Empirical observations have shown that, under certain conditions, model performance improves predictably as scale increases.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 2.1988959312438965,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.02673305943608284,
                  "neutral": 0.8649839162826538,
                  "support": 0.10828303545713425
                },
                "stance_score": 0.0815499760210514,
                "evidence_contribution": 0.17931991046572726,
                "combined_rank_score": 3.1007080078125
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Increasing model size always guarantees better generalization performance.",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 6133,
                      "faiss_score": 0.8974666595458984,
                      "faiss_rank": 15,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 5.871306896209717,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.904425859451294,
                        "neutral": 0.09184230118989944,
                        "support": 0.003731856355443597
                      },
                      "stance_score": -0.9006940030958503,
                      "evidence_contribution": -5.288250911751402,
                      "combined_rank_score": 6.768773555755615
                    },
                    {
                      "id": 5941,
                      "faiss_score": 0.901360034942627,
                      "faiss_rank": 9,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 36,
                      "sentence": "Smaller or compressed models may generalize better due to implicit regularization, but excessive compression can harm performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 2.947385549545288,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.9887085556983948,
                        "neutral": 0.010507587343454361,
                        "support": 0.0007837332668714225
                      },
                      "stance_score": -0.9879248224315234,
                      "evidence_contribution": -2.9117953456717665,
                      "combined_rank_score": 3.848745584487915
                    },
                    {
                      "id": 1453,
                      "faiss_score": 0.9143708348274231,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Regularization_(mathematics)",
                      "file_type": ".txt",
                      "position": 25,
                      "sentence": "By regularizing for time, model complexity can be controlled, improving generalization.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
                      "primary_category": "articles with short description",
                      "rerank_score": 1.1829490661621094,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.9792255163192749,
                        "neutral": 0.019215479493141174,
                        "support": 0.0015590087277814746
                      },
                      "stance_score": -0.9776665075914934,
                      "evidence_contribution": -1.156529682173328,
                      "combined_rank_score": 2.0973199009895325
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6137,
                      "faiss_score": 0.9333542585372925,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 3.651726245880127,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.08033034205436707,
                        "neutral": 0.8913941979408264,
                        "support": 0.028275374323129654
                      },
                      "stance_score": -0.05205496773123741,
                      "evidence_contribution": -0.19009049189260274,
                      "combined_rank_score": 4.585080504417419
                    },
                    {
                      "id": 6127,
                      "faiss_score": 0.9018120765686035,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "Empirical observations have shown that, under certain conditions, model performance improves predictably as scale increases.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 2.1988959312438965,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.02673305943608284,
                        "neutral": 0.8649839162826538,
                        "support": 0.10828303545713425
                      },
                      "stance_score": 0.0815499760210514,
                      "evidence_contribution": 0.17931991046572726,
                      "combined_rank_score": 3.1007080078125
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Distributed systems do not need fault tolerance mechanisms.",
      "expected_verdict": "CONTRADICT",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Distributed systems do not need fault tolerance mechanisms.",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.29039730421702314,
            "contradict": 2.4744526756058063,
            "total": 2.7648499798228294
          },
          "evidence": {
            "supporting": [
              {
                "id": 3487,
                "faiss_score": 0.8769917488098145,
                "faiss_rank": 17,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 58,
                "sentence": "Some components, like the drive shaft in a car, are not likely to fail, so no fault tolerance is needed.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 0.529964804649353,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.023633461445569992,
                  "neutral": 0.4047772288322449,
                  "support": 0.5715892910957336
                },
                "stance_score": 0.5479558296501637,
                "evidence_contribution": 0.29039730421702314,
                "combined_rank_score": 1.4069565534591675
              }
            ],
            "contradicting": [
              {
                "id": 430,
                "faiss_score": 0.8796877861022949,
                "faiss_rank": 13,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 6,
                "sentence": "Other typical properties of distributed systems are: The system must tolerate failures in individual computers.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.483339786529541,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.9988288283348083,
                  "neutral": 0.0008527741301804781,
                  "support": 0.0003183769586030394
                },
                "stance_score": -0.9985104513762053,
                "evidence_contribution": -1.481130279791896,
                "combined_rank_score": 2.363027572631836
              },
              {
                "id": 3809,
                "faiss_score": 0.9038978815078735,
                "faiss_rank": 4,
                "doc_id": "wiki_CAP_theorem",
                "file_type": ".txt",
                "position": 0,
                "sentence": "No distributed system is safe from network failures, thus network partitioning generally has to be tolerated.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/CAP_theorem",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 0.8771834373474121,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.9990100860595703,
                  "neutral": 0.0007420883048325777,
                  "support": 0.0002478054666426033
                },
                "stance_score": -0.9987622805929277,
                "evidence_contribution": -0.8760977303834449,
                "combined_rank_score": 1.7810813188552856
              },
              {
                "id": 3482,
                "faiss_score": 0.9055246114730835,
                "faiss_rank": 3,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 53,
                "sentence": "Providing fault-tolerant design for every component is normally not an option.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 0.31907758116722107,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.4294501543045044,
                  "neutral": 0.508485734462738,
                  "support": 0.06206406280398369
                },
                "stance_score": -0.3673860915005207,
                "evidence_contribution": -0.1172246654304655,
                "combined_rank_score": 1.2246021926403046
              },
              {
                "id": 3508,
                "faiss_score": 0.8738317489624023,
                "faiss_rank": 20,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 79,
                "sentence": "The basic characteristics of fault tolerance require: No single point of failure \u2013 If a system experiences a failure, it must continue to operate without interruption during the repair process.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": -0.11146597564220428,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.8368813395500183,
                  "neutral": 0.15880845487117767,
                  "support": 0.004310184624046087
                },
                "stance_score": -0.8325711549259722,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.7623657733201981
              },
              {
                "id": 3588,
                "faiss_score": 0.9097967147827148,
                "faiss_rank": 2,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 159,
                "sentence": "There is a difference between fault tolerance and systems that rarely have problems.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": -0.7256827354431152,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.19584187865257263,
                  "neutral": 0.7686963081359863,
                  "support": 0.03546185418963432
                },
                "stance_score": -0.1603800244629383,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.1841139793395996
              },
              {
                "id": 6665,
                "faiss_score": 0.9101653099060059,
                "faiss_rank": 1,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 68,
                "sentence": "Ultimately, fault tolerance and reliability are not properties that can be added as afterthoughts.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": -1.6267772912979126,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.5150806307792664,
                  "neutral": 0.46868276596069336,
                  "support": 0.01623660698533058
                },
                "stance_score": -0.4988440237939358,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.7166119813919067
              }
            ],
            "neutral": [
              {
                "id": 3456,
                "faiss_score": 0.893700361251831,
                "faiss_rank": 9,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 27,
                "sentence": "It is helpful if the time between failures is as long as possible, but this is not specifically required in a fault-tolerant system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 0.8421114683151245,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.051901448518037796,
                  "neutral": 0.8763694167137146,
                  "support": 0.07172917574644089
                },
                "stance_score": 0.01982772722840309,
                "evidence_contribution": 0.0166971564896623,
                "combined_rank_score": 1.7358118295669556
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Distributed systems do not need fault tolerance mechanisms.",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 3487,
                      "faiss_score": 0.8769917488098145,
                      "faiss_rank": 17,
                      "doc_id": "wiki_Fault_tolerance",
                      "file_type": ".txt",
                      "position": 58,
                      "sentence": "Some components, like the drive shaft in a car, are not likely to fail, so no fault tolerance is needed.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 0.529964804649353,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.023633461445569992,
                        "neutral": 0.4047772288322449,
                        "support": 0.5715892910957336
                      },
                      "stance_score": 0.5479558296501637,
                      "evidence_contribution": 0.29039730421702314,
                      "combined_rank_score": 1.4069565534591675
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 430,
                      "faiss_score": 0.8796877861022949,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 6,
                      "sentence": "Other typical properties of distributed systems are: The system must tolerate failures in individual computers.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.483339786529541,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.9988288283348083,
                        "neutral": 0.0008527741301804781,
                        "support": 0.0003183769586030394
                      },
                      "stance_score": -0.9985104513762053,
                      "evidence_contribution": -1.481130279791896,
                      "combined_rank_score": 2.363027572631836
                    },
                    {
                      "id": 3809,
                      "faiss_score": 0.9038978815078735,
                      "faiss_rank": 4,
                      "doc_id": "wiki_CAP_theorem",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "No distributed system is safe from network failures, thus network partitioning generally has to be tolerated.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/CAP_theorem",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 0.8771834373474121,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.9990100860595703,
                        "neutral": 0.0007420883048325777,
                        "support": 0.0002478054666426033
                      },
                      "stance_score": -0.9987622805929277,
                      "evidence_contribution": -0.8760977303834449,
                      "combined_rank_score": 1.7810813188552856
                    },
                    {
                      "id": 3482,
                      "faiss_score": 0.9055246114730835,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Fault_tolerance",
                      "file_type": ".txt",
                      "position": 53,
                      "sentence": "Providing fault-tolerant design for every component is normally not an option.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 0.31907758116722107,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.4294501543045044,
                        "neutral": 0.508485734462738,
                        "support": 0.06206406280398369
                      },
                      "stance_score": -0.3673860915005207,
                      "evidence_contribution": -0.1172246654304655,
                      "combined_rank_score": 1.2246021926403046
                    }
                  ],
                  "neutral": [
                    {
                      "id": 3456,
                      "faiss_score": 0.893700361251831,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Fault_tolerance",
                      "file_type": ".txt",
                      "position": 27,
                      "sentence": "It is helpful if the time between failures is as long as possible, but this is not specifically required in a fault-tolerant system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 0.8421114683151245,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.051901448518037796,
                        "neutral": 0.8763694167137146,
                        "support": 0.07172917574644089
                      },
                      "stance_score": 0.01982772722840309,
                      "evidence_contribution": 0.0166971564896623,
                      "combined_rank_score": 1.7358118295669556
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Quantum computers can function reliably without error correction.",
      "expected_verdict": "CONTRADICT",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Quantum computers can function reliably without error correction.",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 21.481415764491658,
            "total": 21.481415764491658
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 6553,
                "faiss_score": 0.9091418981552124,
                "faiss_rank": 1,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 4.795175075531006,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.9906208515167236,
                  "neutral": 0.008175135590136051,
                  "support": 0.001204083557240665
                },
                "stance_score": -0.989416767959483,
                "evidence_contribution": -4.744426625031758,
                "combined_rank_score": 5.704316973686218
              },
              {
                "id": 6561,
                "faiss_score": 0.8931423425674438,
                "faiss_rank": 16,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Small errors accumulate quickly in quantum circuits, limiting the depth of computations that can be performed reliably.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 4.629236221313477,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.9992790818214417,
                  "neutral": 0.000551443372387439,
                  "support": 0.00016951408179011196
                },
                "stance_score": -0.9991095677396515,
                "evidence_contribution": -4.625114200041246,
                "combined_rank_score": 5.52237856388092
              },
              {
                "id": 671,
                "faiss_score": 0.9006523489952087,
                "faiss_rank": 2,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 75,
                "sentence": "Scientists at Harvard University successfully created \"quantum circuits\" that correct errors more efficiently than alternative methods, which may potentially remove a major obstacle to practical quantum computers.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 3.718876361846924,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.998163640499115,
                  "neutral": 0.0014617514098063111,
                  "support": 0.00037455931305885315
                },
                "stance_score": -0.9977890811860561,
                "evidence_contribution": -3.7106542281317854,
                "combined_rank_score": 4.619528710842133
              },
              {
                "id": 793,
                "faiss_score": 0.8939269781112671,
                "faiss_rank": 13,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.389732837677002,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.7898147106170654,
                  "neutral": 0.20260053873062134,
                  "support": 0.007584744598716497
                },
                "stance_score": -0.7822299660183489,
                "evidence_contribution": -3.4337805684457114,
                "combined_rank_score": 5.283659815788269
              },
              {
                "id": 4824,
                "faiss_score": 0.8938266038894653,
                "faiss_rank": 15,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 71,
                "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 2.4530763626098633,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.976203203201294,
                  "neutral": 0.022158455103635788,
                  "support": 0.0016382795292884111
                },
                "stance_score": -0.9745649236720055,
                "evidence_contribution": -2.3906821780884826,
                "combined_rank_score": 3.3469029664993286
              },
              {
                "id": 4862,
                "faiss_score": 0.9004368782043457,
                "faiss_rank": 4,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 109,
                "sentence": "Quantum error correction can be applied to quantum metrology.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 1.515055537223816,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.9318203926086426,
                  "neutral": 0.066192626953125,
                  "support": 0.0019869431853294373
                },
                "stance_score": -0.9298334494233131,
                "evidence_contribution": -1.4087493162447116,
                "combined_rank_score": 2.4154924154281616
              },
              {
                "id": 6554,
                "faiss_score": 0.8949320316314697,
                "faiss_rank": 10,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 1.2565362453460693,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.9315681457519531,
                  "neutral": 0.06641007959842682,
                  "support": 0.002021821215748787
                },
                "stance_score": -0.9295463245362043,
                "evidence_contribution": -1.168008648507961,
                "combined_rank_score": 2.151468276977539
              }
            ],
            "neutral": [
              {
                "id": 4930,
                "faiss_score": 0.8965480923652649,
                "faiss_rank": 6,
                "doc_id": "wiki_Computational_complexity",
                "file_type": ".txt",
                "position": 60,
                "sentence": "A quantum computer is a computer whose model of computation is based on quantum mechanics.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Computational_complexity",
                "primary_category": "all articles containing potentially dated statements",
                "rerank_score": -1.024182677268982,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.010156561620533466,
                  "neutral": 0.9860877394676208,
                  "support": 0.0037557336036115885
                },
                "stance_score": -0.006400828016921878,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.12763458490371704
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Quantum computers can function reliably without error correction.",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 6553,
                      "faiss_score": 0.9091418981552124,
                      "faiss_rank": 1,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 4.795175075531006,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.9906208515167236,
                        "neutral": 0.008175135590136051,
                        "support": 0.001204083557240665
                      },
                      "stance_score": -0.989416767959483,
                      "evidence_contribution": -4.744426625031758,
                      "combined_rank_score": 5.704316973686218
                    },
                    {
                      "id": 6561,
                      "faiss_score": 0.8931423425674438,
                      "faiss_rank": 16,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Small errors accumulate quickly in quantum circuits, limiting the depth of computations that can be performed reliably.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 4.629236221313477,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.9992790818214417,
                        "neutral": 0.000551443372387439,
                        "support": 0.00016951408179011196
                      },
                      "stance_score": -0.9991095677396515,
                      "evidence_contribution": -4.625114200041246,
                      "combined_rank_score": 5.52237856388092
                    },
                    {
                      "id": 793,
                      "faiss_score": 0.8939269781112671,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.389732837677002,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.7898147106170654,
                        "neutral": 0.20260053873062134,
                        "support": 0.007584744598716497
                      },
                      "stance_score": -0.7822299660183489,
                      "evidence_contribution": -3.4337805684457114,
                      "combined_rank_score": 5.283659815788269
                    }
                  ],
                  "neutral": [
                    {
                      "id": 4930,
                      "faiss_score": 0.8965480923652649,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Computational_complexity",
                      "file_type": ".txt",
                      "position": 60,
                      "sentence": "A quantum computer is a computer whose model of computation is based on quantum mechanics.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Computational_complexity",
                      "primary_category": "all articles containing potentially dated statements",
                      "rerank_score": -1.024182677268982,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.010156561620533466,
                        "neutral": 0.9860877394676208,
                        "support": 0.0037557336036115885
                      },
                      "stance_score": -0.006400828016921878,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.12763458490371704
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Scaling neural networks has no impact on performance improvements.",
      "expected_verdict": "CONTRADICT",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Scaling neural networks has no impact on performance improvements.",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.7321178700026582,
            "total": 0.7321178700026582
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 6131,
                "faiss_score": 0.8818397521972656,
                "faiss_rank": 2,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 0.9271154403686523,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.7417044043540955,
                  "neutral": 0.25547850131988525,
                  "support": 0.002817087108269334
                },
                "stance_score": -0.7388873172458261,
                "evidence_contribution": -0.6850338405111762,
                "combined_rank_score": 1.808955192565918
              },
              {
                "id": 6124,
                "faiss_score": 0.8662250638008118,
                "faiss_rank": 8,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 0.04794492572546005,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.9827069044113159,
                  "neutral": 0.016630327329039574,
                  "support": 0.0006628449191339314
                },
                "stance_score": -0.982044059492182,
                "evidence_contribution": -0.04708402949148194,
                "combined_rank_score": 0.9141699895262718
              },
              {
                "id": 6125,
                "faiss_score": 0.8836263418197632,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Over the past decade, scaling has emerged as one of the most reliable drivers of progress in machine learning systems, particularly in deep learning.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -1.5403499603271484,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.9991558790206909,
                  "neutral": 0.0007174470811150968,
                  "support": 0.0001267673069378361
                },
                "stance_score": -0.9990291117137531,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.6567236185073853
              },
              {
                "id": 6161,
                "faiss_score": 0.8589440584182739,
                "faiss_rank": 15,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 37,
                "sentence": "Scaling affects robustness and generalization in nontrivial ways.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -2.0357778072357178,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.8270083069801331,
                  "neutral": 0.17144151031970978,
                  "support": 0.001550139975734055
                },
                "stance_score": -0.825458167004399,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.1768337488174438
              },
              {
                "id": 2393,
                "faiss_score": 0.8614013195037842,
                "faiss_rank": 13,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 324,
                "sentence": "Large and effective neural networks require considerable computing resources.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": -4.691199779510498,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.9643079042434692,
                  "neutral": 0.03442241623997688,
                  "support": 0.0012696814956143498
                },
                "stance_score": -0.9630382227478549,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.829798460006714
              },
              {
                "id": 6142,
                "faiss_score": 0.8561908602714539,
                "faiss_rank": 20,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -4.71311092376709,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.8800991773605347,
                  "neutral": 0.11783106625080109,
                  "support": 0.0020697445143014193
                },
                "stance_score": -0.8780294328462332,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.856920063495636
              },
              {
                "id": 6128,
                "faiss_score": 0.8699010610580444,
                "faiss_rank": 4,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 4,
                "sentence": "These trends have been observed across different domains and architectures, suggesting that scaling captures general properties of learning systems rather than task-specific quirks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -4.945844650268555,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.7391960620880127,
                  "neutral": 0.25634506344795227,
                  "support": 0.004458927549421787
                },
                "stance_score": -0.7347371345385909,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.07594358921051
              },
              {
                "id": 6137,
                "faiss_score": 0.8579269051551819,
                "faiss_rank": 17,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -5.073082447052002,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.8592372536659241,
                  "neutral": 0.1386318802833557,
                  "support": 0.0021308623254299164
                },
                "stance_score": -0.8571063913404942,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.21515554189682
              },
              {
                "id": 6135,
                "faiss_score": 0.867847204208374,
                "faiss_rank": 6,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 11,
                "sentence": "Data scaling plays an equally important role.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -5.08654260635376,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.9858559370040894,
                  "neutral": 0.013569191098213196,
                  "support": 0.0005748308030888438
                },
                "stance_score": -0.9852811062010005,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.218695402145386
              },
              {
                "id": 2985,
                "faiss_score": 0.8640903234481812,
                "faiss_rank": 11,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 40,
                "sentence": "Its parallelizability was an important factor to its widespread use in large neural networks.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": -5.778621673583984,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.9947782754898071,
                  "neutral": 0.0048780133947730064,
                  "support": 0.0003436481347307563
                },
                "stance_score": -0.9944346273550764,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.914531350135803
              }
            ],
            "neutral": []
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Scaling neural networks has no impact on performance improvements.",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 6131,
                      "faiss_score": 0.8818397521972656,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 0.9271154403686523,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.7417044043540955,
                        "neutral": 0.25547850131988525,
                        "support": 0.002817087108269334
                      },
                      "stance_score": -0.7388873172458261,
                      "evidence_contribution": -0.6850338405111762,
                      "combined_rank_score": 1.808955192565918
                    },
                    {
                      "id": 6124,
                      "faiss_score": 0.8662250638008118,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 0.04794492572546005,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.9827069044113159,
                        "neutral": 0.016630327329039574,
                        "support": 0.0006628449191339314
                      },
                      "stance_score": -0.982044059492182,
                      "evidence_contribution": -0.04708402949148194,
                      "combined_rank_score": 0.9141699895262718
                    },
                    {
                      "id": 6125,
                      "faiss_score": 0.8836263418197632,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 1,
                      "sentence": "Over the past decade, scaling has emerged as one of the most reliable drivers of progress in machine learning systems, particularly in deep learning.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -1.5403499603271484,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.9991558790206909,
                        "neutral": 0.0007174470811150968,
                        "support": 0.0001267673069378361
                      },
                      "stance_score": -0.9990291117137531,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.6567236185073853
                    }
                  ],
                  "neutral": []
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Larger datasets always reduce overfitting in machine learning models.",
      "expected_verdict": "CONTRADICT",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Larger datasets reduce overfitting in machine learning models.",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 3.4076481726903425,
            "total": 3.4076481726903425
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 6136,
                "faiss_score": 0.912011444568634,
                "faiss_rank": 5,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 12,
                "sentence": "Training large models on insufficient or low-quality data can lead to overfitting or wasted capacity.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 3.8103489875793457,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.8988689184188843,
                  "neutral": 0.0965760350227356,
                  "support": 0.004554989747703075
                },
                "stance_score": -0.8943139286711812,
                "evidence_contribution": -3.4076481726903425,
                "combined_rank_score": 4.72236043214798
              }
            ],
            "neutral": [
              {
                "id": 2613,
                "faiss_score": 0.8993738889694214,
                "faiss_rank": 10,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 208,
                "sentence": "Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 4.625890731811523,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.008702030405402184,
                  "neutral": 0.91930091381073,
                  "support": 0.0719970092177391
                },
                "stance_score": 0.06329497881233692,
                "evidence_contribution": 0.2927956558581961,
                "combined_rank_score": 5.525264620780945
              },
              {
                "id": 6309,
                "faiss_score": 0.901115894317627,
                "faiss_rank": 8,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 48,
                "sentence": "High-capacity models are prone to overfitting when data is scarce, and strong generalization typically requires pretraining on massive corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 1.2675398588180542,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0038825219962745905,
                  "neutral": 0.929321825504303,
                  "support": 0.06679567694664001
                },
                "stance_score": 0.06291315495036542,
                "evidence_contribution": 0.07974493154358456,
                "combined_rank_score": 2.168655753135681
              },
              {
                "id": 1341,
                "faiss_score": 0.8920928239822388,
                "faiss_rank": 18,
                "doc_id": "wiki_Bias\u2013variance_tradeoff",
                "file_type": ".txt",
                "position": 18,
                "sentence": "The limiting case where only a finite number of data points are selected over a broad sample space may result in improved precision and lower variance overall, but may also result in an overreliance on the training data (overfitting).",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff",
                "primary_category": "machine learning",
                "rerank_score": 1.2561531066894531,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.00855045486241579,
                  "neutral": 0.9845532178878784,
                  "support": 0.0068963379599153996
                },
                "stance_score": -0.001654116902500391,
                "evidence_contribution": -0.0020778240859034014,
                "combined_rank_score": 2.148245930671692
              }
            ]
          }
        },
        {
          "subclaim": "Machine learning models can suffer from overfitting.",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 18.904998382540064,
            "contradict": 0.0,
            "total": 18.904998382540064
          },
          "evidence": {
            "supporting": [
              {
                "id": 1218,
                "faiss_score": 0.9458591341972351,
                "faiss_rank": 1,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 243,
                "sentence": "Overfitting is something to watch out for when training a machine learning model.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": 5.892265319824219,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0004957018536515534,
                  "neutral": 0.013847441412508488,
                  "support": 0.9856568574905396
                },
                "stance_score": 0.985161155636888,
                "evidence_contribution": 5.804830911797184,
                "combined_rank_score": 6.838124454021454
              },
              {
                "id": 1311,
                "faiss_score": 0.9410011172294617,
                "faiss_rank": 3,
                "doc_id": "wiki_Statistical_learning_theory",
                "file_type": ".txt",
                "position": 21,
                "sentence": "In machine learning problems, a major problem that arises is that of overfitting.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Statistical_learning_theory",
                "primary_category": "machine learning",
                "rerank_score": 5.9156341552734375,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0006119771278463304,
                  "neutral": 0.030699515715241432,
                  "support": 0.9686884880065918
                },
                "stance_score": 0.9680765108787455,
                "evidence_contribution": 5.726786472672244,
                "combined_rank_score": 6.856635272502899
              },
              {
                "id": 2607,
                "faiss_score": 0.9083086252212524,
                "faiss_rank": 16,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 202,
                "sentence": "DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 3.5356929302215576,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.0009529117960482836,
                  "neutral": 0.06537891179323196,
                  "support": 0.9336680769920349
                },
                "stance_score": 0.9327151651959866,
                "evidence_contribution": 3.297794415493882,
                "combined_rank_score": 4.44400155544281
              },
              {
                "id": 5988,
                "faiss_score": 0.9075533747673035,
                "faiss_rank": 18,
                "doc_id": "local_ml_evaluation_and_generalization.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Overfitting occurs when a model learns patterns specific to the training data that do not generalize.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                "primary_category": null,
                "rerank_score": 4.331396102905273,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0010803007753565907,
                  "neutral": 0.7501620054244995,
                  "support": 0.24875766038894653
                },
                "stance_score": 0.24767735961358994,
                "evidence_contribution": 1.0727887502081714,
                "combined_rank_score": 5.238949477672577
              },
              {
                "id": 1377,
                "faiss_score": 0.9115912914276123,
                "faiss_rank": 13,
                "doc_id": "wiki_Overfitting",
                "file_type": ".txt",
                "position": 27,
                "sentence": "Overfitting is especially likely in cases where learning was performed too long or where training examples are rare, causing the learner to adjust to very specific random features of the training data that have no causal relation to the target function.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                "primary_category": "machine learning",
                "rerank_score": 2.787229061126709,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.0014486738946288824,
                  "neutral": 0.6907151937484741,
                  "support": 0.3078361749649048
                },
                "stance_score": 0.3063875010702759,
                "evidence_contribution": 0.8539721469490636,
                "combined_rank_score": 3.6988203525543213
              },
              {
                "id": 6136,
                "faiss_score": 0.9201747179031372,
                "faiss_rank": 8,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 12,
                "sentence": "Training large models on insufficient or low-quality data can lead to overfitting or wasted capacity.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 3.697188377380371,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.0011870772577822208,
                  "neutral": 0.7740406394004822,
                  "support": 0.22477230429649353
                },
                "stance_score": 0.2235852270387113,
                "evidence_contribution": 0.8266367027614749,
                "combined_rank_score": 4.617363095283508
              },
              {
                "id": 6309,
                "faiss_score": 0.9066196084022522,
                "faiss_rank": 19,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 48,
                "sentence": "High-capacity models are prone to overfitting when data is scarce, and strong generalization typically requires pretraining on massive corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 3.5030198097229004,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0012985646026208997,
                  "neutral": 0.762039303779602,
                  "support": 0.2366621494293213
                },
                "stance_score": 0.2353635848267004,
                "evidence_contribution": 0.8244833001353278,
                "combined_rank_score": 4.409639418125153
              },
              {
                "id": 1359,
                "faiss_score": 0.9295470118522644,
                "faiss_rank": 4,
                "doc_id": "wiki_Overfitting",
                "file_type": ".txt",
                "position": 9,
                "sentence": "With so many candidate models, overfitting is a real danger.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                "primary_category": "machine learning",
                "rerank_score": 3.042015552520752,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.0013157170033082366,
                  "neutral": 0.8337580561637878,
                  "support": 0.164926216006279
                },
                "stance_score": 0.16361049900297076,
                "evidence_contribution": 0.497705682522718,
                "combined_rank_score": 3.9715625643730164
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 5990,
                "faiss_score": 0.9263968467712402,
                "faiss_rank": 6,
                "doc_id": "local_ml_evaluation_and_generalization.txt",
                "file_type": ".txt",
                "position": 16,
                "sentence": "Overfitting can arise from excessive model capacity, insufficient data, or overly aggressive optimization.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                "primary_category": null,
                "rerank_score": 3.749330997467041,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.001319977454841137,
                  "neutral": 0.9183411002159119,
                  "support": 0.08033887296915054
                },
                "stance_score": 0.0790188955143094,
                "evidence_contribution": 0.2962679943374096,
                "combined_rank_score": 4.675727844238281
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Machine learning models can suffer from overfitting.",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 1311,
                      "faiss_score": 0.9410011172294617,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Statistical_learning_theory",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "In machine learning problems, a major problem that arises is that of overfitting.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Statistical_learning_theory",
                      "primary_category": "machine learning",
                      "rerank_score": 5.9156341552734375,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0006119771278463304,
                        "neutral": 0.030699515715241432,
                        "support": 0.9686884880065918
                      },
                      "stance_score": 0.9680765108787455,
                      "evidence_contribution": 5.726786472672244,
                      "combined_rank_score": 6.856635272502899
                    },
                    {
                      "id": 1218,
                      "faiss_score": 0.9458591341972351,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 243,
                      "sentence": "Overfitting is something to watch out for when training a machine learning model.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": 5.892265319824219,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0004957018536515534,
                        "neutral": 0.013847441412508488,
                        "support": 0.9856568574905396
                      },
                      "stance_score": 0.985161155636888,
                      "evidence_contribution": 5.804830911797184,
                      "combined_rank_score": 6.838124454021454
                    },
                    {
                      "id": 5988,
                      "faiss_score": 0.9075533747673035,
                      "faiss_rank": 18,
                      "doc_id": "local_ml_evaluation_and_generalization.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "Overfitting occurs when a model learns patterns specific to the training data that do not generalize.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                      "primary_category": null,
                      "rerank_score": 4.331396102905273,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.0010803007753565907,
                        "neutral": 0.7501620054244995,
                        "support": 0.24875766038894653
                      },
                      "stance_score": 0.24767735961358994,
                      "evidence_contribution": 1.0727887502081714,
                      "combined_rank_score": 5.238949477672577
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5990,
                      "faiss_score": 0.9263968467712402,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_evaluation_and_generalization.txt",
                      "file_type": ".txt",
                      "position": 16,
                      "sentence": "Overfitting can arise from excessive model capacity, insufficient data, or overly aggressive optimization.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                      "primary_category": null,
                      "rerank_score": 3.749330997467041,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.001319977454841137,
                        "neutral": 0.9183411002159119,
                        "support": 0.08033887296915054
                      },
                      "stance_score": 0.0790188955143094,
                      "evidence_contribution": 0.2962679943374096,
                      "combined_rank_score": 4.675727844238281
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Larger datasets reduce overfitting in machine learning models.",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 6136,
                      "faiss_score": 0.912011444568634,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 12,
                      "sentence": "Training large models on insufficient or low-quality data can lead to overfitting or wasted capacity.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 3.8103489875793457,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.8988689184188843,
                        "neutral": 0.0965760350227356,
                        "support": 0.004554989747703075
                      },
                      "stance_score": -0.8943139286711812,
                      "evidence_contribution": -3.4076481726903425,
                      "combined_rank_score": 4.72236043214798
                    }
                  ],
                  "neutral": [
                    {
                      "id": 2613,
                      "faiss_score": 0.8993738889694214,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 208,
                      "sentence": "Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": 4.625890731811523,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.008702030405402184,
                        "neutral": 0.91930091381073,
                        "support": 0.0719970092177391
                      },
                      "stance_score": 0.06329497881233692,
                      "evidence_contribution": 0.2927956558581961,
                      "combined_rank_score": 5.525264620780945
                    },
                    {
                      "id": 6309,
                      "faiss_score": 0.901115894317627,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 48,
                      "sentence": "High-capacity models are prone to overfitting when data is scarce, and strong generalization typically requires pretraining on massive corpora.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 1.2675398588180542,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0038825219962745905,
                        "neutral": 0.929321825504303,
                        "support": 0.06679567694664001
                      },
                      "stance_score": 0.06291315495036542,
                      "evidence_contribution": 0.07974493154358456,
                      "combined_rank_score": 2.168655753135681
                    },
                    {
                      "id": 1341,
                      "faiss_score": 0.8920928239822388,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Bias\u2013variance_tradeoff",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "The limiting case where only a finite number of data points are selected over a broad sample space may result in improved precision and lower variance overall, but may also result in an overreliance on the training data (overfitting).",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff",
                      "primary_category": "machine learning",
                      "rerank_score": 1.2561531066894531,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.00855045486241579,
                        "neutral": 0.9845532178878784,
                        "support": 0.0068963379599153996
                      },
                      "stance_score": -0.001654116902500391,
                      "evidence_contribution": -0.0020778240859034014,
                      "combined_rank_score": 2.148245930671692
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Transformer models eliminate the need for optimization techniques.",
      "expected_verdict": "CONTRADICT",
      "predicted_verdict": "SUPPORT",
      "subclaims": [
        {
          "subclaim": "Transformer models exist",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 19.16384684320196,
            "contradict": 0.0,
            "total": 19.16384684320196
          },
          "evidence": {
            "supporting": [
              {
                "id": 6392,
                "faiss_score": 0.8948999643325806,
                "faiss_rank": 8,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 131,
                "sentence": "As transformer-based models become more capable, concerns about misuse and unintended consequences grow.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 4.196916580200195,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.018157165497541428,
                  "neutral": 0.15676067769527435,
                  "support": 0.8250821828842163
                },
                "stance_score": 0.8069250173866749,
                "evidence_contribution": 3.3865969844484667,
                "combined_rank_score": 5.091816544532776
              },
              {
                "id": 1759,
                "faiss_score": 0.9132465124130249,
                "faiss_rank": 1,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 27,
                "sentence": "As of 2024, the largest and most capable models are all based on the transformer architecture.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 4.139178276062012,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.011396590620279312,
                  "neutral": 0.17400501668453217,
                  "support": 0.8145983815193176
                },
                "stance_score": 0.8032017908990383,
                "evidence_contribution": 3.324595404183402,
                "combined_rank_score": 5.052424788475037
              },
              {
                "id": 2987,
                "faiss_score": 0.8926093578338623,
                "faiss_rank": 12,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": 4.5255537033081055,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.022291269153356552,
                  "neutral": 0.24094286561012268,
                  "support": 0.7367658615112305
                },
                "stance_score": 0.7144745923578739,
                "evidence_contribution": 3.2333931373647253,
                "combined_rank_score": 5.418163061141968
              },
              {
                "id": 6300,
                "faiss_score": 0.8872029781341553,
                "faiss_rank": 15,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "As transformer models grew larger, their performance continued to improve, reinforcing the idea that scaling was a key driver of success.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 2.884159564971924,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.04400254786014557,
                  "neutral": 0.12755577266216278,
                  "support": 0.8284416794776917
                },
                "stance_score": 0.7844391316175461,
                "evidence_contribution": 2.2624476245930154,
                "combined_rank_score": 3.771362543106079
              },
              {
                "id": 6363,
                "faiss_score": 0.8928086161613464,
                "faiss_rank": 11,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 102,
                "sentence": "These hybrid models attempt to balance efficiency and flexibility, though they often sacrifice the simplicity of the original transformer design.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 3.275681972503662,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.060536615550518036,
                  "neutral": 0.2372356504201889,
                  "support": 0.7022277116775513
                },
                "stance_score": 0.6416910961270332,
                "evidence_contribution": 2.1019759554994373,
                "combined_rank_score": 4.1684905886650085
              },
              {
                "id": 6367,
                "faiss_score": 0.8867349624633789,
                "faiss_rank": 17,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 106,
                "sentence": "Interpretability remains a challenging aspect of transformer-based models.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 2.800401210784912,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.021283458918333054,
                  "neutral": 0.21903814375400543,
                  "support": 0.7596783638000488
                },
                "stance_score": 0.7383949048817158,
                "evidence_contribution": 2.067801985668167,
                "combined_rank_score": 3.687136173248291
              },
              {
                "id": 3066,
                "faiss_score": 0.8990744352340698,
                "faiss_rank": 5,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 121,
                "sentence": "These feed-forward layers contain most of the parameters in a transformer model.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": 2.170119285583496,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.006744684651494026,
                  "neutral": 0.3021673262119293,
                  "support": 0.6910879611968994
                },
                "stance_score": 0.6843432765454054,
                "evidence_contribution": 1.485106542390584,
                "combined_rank_score": 3.069193720817566
              },
              {
                "id": 3010,
                "faiss_score": 0.902931809425354,
                "faiss_rank": 4,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 65,
                "sentence": "These classes are independent of a specific modeling architecture such as transformer, but they are often discussed in the context of transformer.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": 1.9379291534423828,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.031252454966306686,
                  "neutral": 0.3576362133026123,
                  "support": 0.6111113429069519
                },
                "stance_score": 0.5798588879406452,
                "evidence_contribution": 1.123725443822856,
                "combined_rank_score": 2.840860962867737
              },
              {
                "id": 6304,
                "faiss_score": 0.9101985692977905,
                "faiss_rank": 2,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 43,
                "sentence": "To address these issues, numerous variants of the transformer architecture have been proposed.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 0.21942584216594696,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.004565074574202299,
                  "neutral": 0.17873318493366241,
                  "support": 0.8167017102241516
                },
                "stance_score": 0.8121366356499493,
                "evidence_contribution": 0.17820376523130896,
                "combined_rank_score": 1.1296244114637375
              },
              {
                "id": 6298,
                "faiss_score": 0.8935939073562622,
                "faiss_rank": 9,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 37,
                "sentence": "Models based on transformers achieved state-of-the-art performance in translation, summarization, language modeling, and many other tasks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -0.21881964802742004,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.0069143869914114475,
                  "neutral": 0.1628517508506775,
                  "support": 0.8302338123321533
                },
                "stance_score": 0.8233194253407419,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.6747742593288422
              }
            ],
            "contradicting": [],
            "neutral": []
          }
        },
        {
          "subclaim": "Optimization techniques are needed",
          "verdict": "MIXED",
          "controversial": true,
          "strengths": {
            "support": 1.7940795439702546,
            "contradict": 1.0483818562035632,
            "total": 2.8424614001738178
          },
          "evidence": {
            "supporting": [
              {
                "id": 1636,
                "faiss_score": 0.8969464898109436,
                "faiss_rank": 4,
                "doc_id": "wiki_Optimization_(mathematics)",
                "file_type": ".txt",
                "position": 85,
                "sentence": "Many optimization algorithms need to start from a feasible point.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                "primary_category": "all articles with style issues",
                "rerank_score": 4.919436454772949,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.005357774440199137,
                  "neutral": 0.7723470330238342,
                  "support": 0.2222951352596283
                },
                "stance_score": 0.21693736081942916,
                "evidence_contribution": 1.0672095612173327,
                "combined_rank_score": 5.816382944583893
              },
              {
                "id": 5951,
                "faiss_score": 0.8863611221313477,
                "faiss_rank": 16,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 46,
                "sentence": "Effective optimization requires understanding how model behavior interacts with system architecture.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 2.5880959033966064,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.00443139998242259,
                  "neutral": 0.7683227062225342,
                  "support": 0.22724591195583344
                },
                "stance_score": 0.22281451197341084,
                "evidence_contribution": 0.5766653256556987,
                "combined_rank_score": 3.474457025527954
              },
              {
                "id": 5899,
                "faiss_score": 0.8871976137161255,
                "faiss_rank": 14,
                "doc_id": "local_math_optimization_and_convergence.txt",
                "file_type": ".txt",
                "position": 68,
                "sentence": "From a systems perspective, optimization must be efficient not only in iteration count but also in wall-clock time.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                "primary_category": null,
                "rerank_score": 1.000978708267212,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0027748181018978357,
                  "neutral": 0.8443925976753235,
                  "support": 0.15283261239528656
                },
                "stance_score": 0.15005779429338872,
                "evidence_contribution": 0.15020465709722325,
                "combined_rank_score": 1.8881763219833374
              }
            ],
            "contradicting": [
              {
                "id": 1632,
                "faiss_score": 0.8906309604644775,
                "faiss_rank": 9,
                "doc_id": "wiki_Optimization_(mathematics)",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Classical optimization techniques due to their iterative approach do not perform satisfactorily when they are used to obtain multiple solutions, since it is not guaranteed that different solutions will be obtained even with different starting points in multiple runs of the algorithm.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                "primary_category": "all articles with style issues",
                "rerank_score": 3.218745708465576,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.2238139808177948,
                  "neutral": 0.7673050165176392,
                  "support": 0.008881048299372196
                },
                "stance_score": -0.2149329325184226,
                "evidence_contribution": -0.691814454151594,
                "combined_rank_score": 4.109376668930054
              },
              {
                "id": 6229,
                "faiss_score": 0.8993328809738159,
                "faiss_rank": 3,
                "doc_id": "local_ml_training_dynamics.txt",
                "file_type": ".txt",
                "position": 37,
                "sentence": "These techniques alter the optimization path as well as the final solution.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                "primary_category": null,
                "rerank_score": 2.6126317977905273,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.1437188684940338,
                  "neutral": 0.8490404486656189,
                  "support": 0.007240623701363802
                },
                "stance_score": -0.13647824479267,
                "evidence_contribution": -0.35656740205196913,
                "combined_rank_score": 3.5119646787643433
              }
            ],
            "neutral": [
              {
                "id": 5957,
                "faiss_score": 0.892740786075592,
                "faiss_rank": 6,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 52,
                "sentence": "Implementing advanced compression or optimization techniques may require specialized expertise and tooling.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 5.0751824378967285,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.006101752631366253,
                  "neutral": 0.921724796295166,
                  "support": 0.07217340916395187
                },
                "stance_score": 0.06607165653258562,
                "evidence_contribution": 0.3353257108769232,
                "combined_rank_score": 5.967923223972321
              },
              {
                "id": 1719,
                "faiss_score": 0.885511040687561,
                "faiss_rank": 18,
                "doc_id": "wiki_Optimization_(mathematics)",
                "file_type": ".txt",
                "position": 168,
                "sentence": "Another field that uses optimization techniques extensively is operations research.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                "primary_category": "all articles with style issues",
                "rerank_score": 3.6928787231445312,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0008080819970928133,
                  "neutral": 0.9960877895355225,
                  "support": 0.003104150528088212
                },
                "stance_score": 0.0022960685309953988,
                "evidence_contribution": 0.008479102624994628,
                "combined_rank_score": 4.578389763832092
              },
              {
                "id": 6174,
                "faiss_score": 0.8904021978378296,
                "faiss_rank": 10,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 50,
                "sentence": "Approaches include better architectures, improved training objectives, and more effective optimization methods.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 1.238701581954956,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.002908584661781788,
                  "neutral": 0.9507206678390503,
                  "support": 0.046370696276426315
                },
                "stance_score": 0.04346211161464453,
                "evidence_contribution": 0.053836586412163046,
                "combined_rank_score": 2.1291037797927856
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is generally supported by the available evidence, with some limitations.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Transformer models exist",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 2987,
                      "faiss_score": 0.8926093578338623,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": 4.5255537033081055,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.022291269153356552,
                        "neutral": 0.24094286561012268,
                        "support": 0.7367658615112305
                      },
                      "stance_score": 0.7144745923578739,
                      "evidence_contribution": 3.2333931373647253,
                      "combined_rank_score": 5.418163061141968
                    },
                    {
                      "id": 6392,
                      "faiss_score": 0.8948999643325806,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 131,
                      "sentence": "As transformer-based models become more capable, concerns about misuse and unintended consequences grow.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 4.196916580200195,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.018157165497541428,
                        "neutral": 0.15676067769527435,
                        "support": 0.8250821828842163
                      },
                      "stance_score": 0.8069250173866749,
                      "evidence_contribution": 3.3865969844484667,
                      "combined_rank_score": 5.091816544532776
                    },
                    {
                      "id": 1759,
                      "faiss_score": 0.9132465124130249,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 27,
                      "sentence": "As of 2024, the largest and most capable models are all based on the transformer architecture.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 4.139178276062012,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.011396590620279312,
                        "neutral": 0.17400501668453217,
                        "support": 0.8145983815193176
                      },
                      "stance_score": 0.8032017908990383,
                      "evidence_contribution": 3.324595404183402,
                      "combined_rank_score": 5.052424788475037
                    }
                  ],
                  "contradicting": [],
                  "neutral": []
                }
              }
            ]
          },
          {
            "type": "CONTROVERSIAL_ASPECTS",
            "items": [
              {
                "subclaim": "Optimization techniques are needed",
                "verdict": "MIXED",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 1636,
                      "faiss_score": 0.8969464898109436,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Optimization_(mathematics)",
                      "file_type": ".txt",
                      "position": 85,
                      "sentence": "Many optimization algorithms need to start from a feasible point.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                      "primary_category": "all articles with style issues",
                      "rerank_score": 4.919436454772949,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.005357774440199137,
                        "neutral": 0.7723470330238342,
                        "support": 0.2222951352596283
                      },
                      "stance_score": 0.21693736081942916,
                      "evidence_contribution": 1.0672095612173327,
                      "combined_rank_score": 5.816382944583893
                    },
                    {
                      "id": 5951,
                      "faiss_score": 0.8863611221313477,
                      "faiss_rank": 16,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 46,
                      "sentence": "Effective optimization requires understanding how model behavior interacts with system architecture.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 2.5880959033966064,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.00443139998242259,
                        "neutral": 0.7683227062225342,
                        "support": 0.22724591195583344
                      },
                      "stance_score": 0.22281451197341084,
                      "evidence_contribution": 0.5766653256556987,
                      "combined_rank_score": 3.474457025527954
                    },
                    {
                      "id": 5899,
                      "faiss_score": 0.8871976137161255,
                      "faiss_rank": 14,
                      "doc_id": "local_math_optimization_and_convergence.txt",
                      "file_type": ".txt",
                      "position": 68,
                      "sentence": "From a systems perspective, optimization must be efficient not only in iteration count but also in wall-clock time.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                      "primary_category": null,
                      "rerank_score": 1.000978708267212,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.0027748181018978357,
                        "neutral": 0.8443925976753235,
                        "support": 0.15283261239528656
                      },
                      "stance_score": 0.15005779429338872,
                      "evidence_contribution": 0.15020465709722325,
                      "combined_rank_score": 1.8881763219833374
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 1632,
                      "faiss_score": 0.8906309604644775,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Optimization_(mathematics)",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Classical optimization techniques due to their iterative approach do not perform satisfactorily when they are used to obtain multiple solutions, since it is not guaranteed that different solutions will be obtained even with different starting points in multiple runs of the algorithm.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                      "primary_category": "all articles with style issues",
                      "rerank_score": 3.218745708465576,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.2238139808177948,
                        "neutral": 0.7673050165176392,
                        "support": 0.008881048299372196
                      },
                      "stance_score": -0.2149329325184226,
                      "evidence_contribution": -0.691814454151594,
                      "combined_rank_score": 4.109376668930054
                    },
                    {
                      "id": 6229,
                      "faiss_score": 0.8993328809738159,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_training_dynamics.txt",
                      "file_type": ".txt",
                      "position": 37,
                      "sentence": "These techniques alter the optimization path as well as the final solution.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                      "primary_category": null,
                      "rerank_score": 2.6126317977905273,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.1437188684940338,
                        "neutral": 0.8490404486656189,
                        "support": 0.007240623701363802
                      },
                      "stance_score": -0.13647824479267,
                      "evidence_contribution": -0.35656740205196913,
                      "combined_rank_score": 3.5119646787643433
                    }
                  ],
                  "neutral": [
                    {
                      "id": 5957,
                      "faiss_score": 0.892740786075592,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 52,
                      "sentence": "Implementing advanced compression or optimization techniques may require specialized expertise and tooling.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 5.0751824378967285,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.006101752631366253,
                        "neutral": 0.921724796295166,
                        "support": 0.07217340916395187
                      },
                      "stance_score": 0.06607165653258562,
                      "evidence_contribution": 0.3353257108769232,
                      "combined_rank_score": 5.967923223972321
                    },
                    {
                      "id": 1719,
                      "faiss_score": 0.885511040687561,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Optimization_(mathematics)",
                      "file_type": ".txt",
                      "position": 168,
                      "sentence": "Another field that uses optimization techniques extensively is operations research.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                      "primary_category": "all articles with style issues",
                      "rerank_score": 3.6928787231445312,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0008080819970928133,
                        "neutral": 0.9960877895355225,
                        "support": 0.003104150528088212
                      },
                      "stance_score": 0.0022960685309953988,
                      "evidence_contribution": 0.008479102624994628,
                      "combined_rank_score": 4.578389763832092
                    },
                    {
                      "id": 6174,
                      "faiss_score": 0.8904021978378296,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 50,
                      "sentence": "Approaches include better architectures, improved training objectives, and more effective optimization methods.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 1.238701581954956,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.002908584661781788,
                        "neutral": 0.9507206678390503,
                        "support": 0.046370696276426315
                      },
                      "stance_score": 0.04346211161464453,
                      "evidence_contribution": 0.053836586412163046,
                      "combined_rank_score": 2.1291037797927856
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Scaling model size improves performance but introduces efficiency and stability challenges.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "MIXED",
      "subclaims": [
        {
          "subclaim": "Scaling model size improves performance",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 24.978872569263523,
            "contradict": 0.0,
            "total": 24.978872569263523
          },
          "evidence": {
            "supporting": [
              {
                "id": 6300,
                "faiss_score": 0.8937974572181702,
                "faiss_rank": 13,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "As transformer models grew larger, their performance continued to improve, reinforcing the idea that scaling was a key driver of success.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 6.097304344177246,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0006018584244884551,
                  "neutral": 0.04180032014846802,
                  "support": 0.9575978517532349
                },
                "stance_score": 0.9569959933287464,
                "evidence_contribution": 5.835095827483585,
                "combined_rank_score": 6.991101801395416
              },
              {
                "id": 6127,
                "faiss_score": 0.9101240634918213,
                "faiss_rank": 4,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "Empirical observations have shown that, under certain conditions, model performance improves predictably as scale increases.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 6.25351619720459,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.000981850316748023,
                  "neutral": 0.09745568037033081,
                  "support": 0.901562511920929
                },
                "stance_score": 0.9005806616041809,
                "evidence_contribution": 5.631795754230971,
                "combined_rank_score": 7.163640260696411
              },
              {
                "id": 6124,
                "faiss_score": 0.9064903259277344,
                "faiss_rank": 6,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 7.801340579986572,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0015502498717978597,
                  "neutral": 0.32961592078208923,
                  "support": 0.66883385181427
                },
                "stance_score": 0.6672836019424722,
                "evidence_contribution": 5.205706642193415,
                "combined_rank_score": 8.707830905914307
              },
              {
                "id": 6137,
                "faiss_score": 0.9351097345352173,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 5.056931018829346,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.00106658018194139,
                  "neutral": 0.08652021735906601,
                  "support": 0.9124131798744202
                },
                "stance_score": 0.9113465996924788,
                "evidence_contribution": 4.6086168888895465,
                "combined_rank_score": 5.992040753364563
              },
              {
                "id": 6133,
                "faiss_score": 0.9184768795967102,
                "faiss_rank": 2,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 7.172950744628906,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.1632184386253357,
                  "neutral": 0.1580629199743271,
                  "support": 0.678718626499176
                },
                "stance_score": 0.5155001878738403,
                "evidence_contribution": 3.697657456466004,
                "combined_rank_score": 8.091427624225616
              }
            ],
            "contradicting": [
              {
                "id": 6349,
                "faiss_score": 0.8993617296218872,
                "faiss_rank": 10,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 88,
                "sentence": "As models scale, training efficiency becomes a primary concern.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -1.0130536556243896,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.26972973346710205,
                  "neutral": 0.7104320526123047,
                  "support": 0.019838299602270126
                },
                "stance_score": -0.24989143386483192,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.11369192600250244
              }
            ],
            "neutral": [
              {
                "id": 6142,
                "faiss_score": 0.9155328869819641,
                "faiss_rank": 3,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.632805347442627,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.003673245431855321,
                  "neutral": 0.966631293296814,
                  "support": 0.029695525765419006
                },
                "stance_score": 0.026022280333563685,
                "evidence_contribution": 0.12055615948198495,
                "combined_rank_score": 5.548338234424591
              },
              {
                "id": 6131,
                "faiss_score": 0.9074476361274719,
                "faiss_rank": 5,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 2.8586673736572266,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.0011911247856914997,
                  "neutral": 0.995375394821167,
                  "support": 0.0034334997180849314
                },
                "stance_score": 0.0022423749323934317,
                "evidence_contribution": 0.006410204058739932,
                "combined_rank_score": 3.7661150097846985
              },
              {
                "id": 6045,
                "faiss_score": 0.8938406705856323,
                "faiss_rank": 12,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 5,
                "sentence": "As models scale, they appear to acquire new capabilities that were not present in smaller versions.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 0.17277783155441284,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.002974959323182702,
                  "neutral": 0.9193781018257141,
                  "support": 0.07764695584774017
                },
                "stance_score": 0.07467199652455747,
                "evidence_contribution": 0.012901665637351692,
                "combined_rank_score": 1.0666185021400452
              }
            ]
          }
        },
        {
          "subclaim": "Scaling model size introduces efficiency challenges",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 1.9101070100796687,
            "contradict": 0.0,
            "total": 1.9101070100796687
          },
          "evidence": {
            "supporting": [
              {
                "id": 6349,
                "faiss_score": 0.9152157306671143,
                "faiss_rank": 3,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 88,
                "sentence": "As models scale, training efficiency becomes a primary concern.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 1.1722049713134766,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0020032706670463085,
                  "neutral": 0.1527910679578781,
                  "support": 0.8452056646347046
                },
                "stance_score": 0.8432023939676583,
                "evidence_contribution": 0.9884060380323136,
                "combined_rank_score": 2.087420701980591
              },
              {
                "id": 6149,
                "faiss_score": 0.9049237966537476,
                "faiss_rank": 4,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 25,
                "sentence": "Scaling also introduces engineering challenges.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 2.976050615310669,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.003267299383878708,
                  "neutral": 0.6837592720985413,
                  "support": 0.31297338008880615
                },
                "stance_score": 0.30970608070492744,
                "evidence_contribution": 0.921700972047355,
                "combined_rank_score": 3.8809744119644165
              },
              {
                "id": 6157,
                "faiss_score": 0.9232777953147888,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 33,
                "sentence": "Inference efficiency is another scaling concern.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -0.5988900661468506,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0029566814191639423,
                  "neutral": 0.8427377939224243,
                  "support": 0.15430548787117004
                },
                "stance_score": 0.1513488064520061,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.32438772916793823
              },
              {
                "id": 6173,
                "faiss_score": 0.8956167697906494,
                "faiss_rank": 10,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 49,
                "sentence": "Efficiency-oriented research aims to counterbalance brute-force scaling by achieving comparable performance with fewer resources.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -0.9426184892654419,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0026571243070065975,
                  "neutral": 0.8857571482658386,
                  "support": 0.11158574372529984
                },
                "stance_score": 0.10892861941829324,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.04700171947479248
              },
              {
                "id": 5905,
                "faiss_score": 0.9015026092529297,
                "faiss_rank": 5,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "As machine learning models have grown larger and more capable, efficiency has become a central concern.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -2.95443058013916,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0009848137851804495,
                  "neutral": 0.09008853882551193,
                  "support": 0.9089266657829285
                },
                "stance_score": 0.907941851997748,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.0529279708862305
              },
              {
                "id": 6211,
                "faiss_score": 0.8909123539924622,
                "faiss_rank": 16,
                "doc_id": "local_ml_training_dynamics.txt",
                "file_type": ".txt",
                "position": 19,
                "sentence": "Batch size influences both optimization efficiency and generalization.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                "primary_category": null,
                "rerank_score": -5.489606857299805,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.005222289822995663,
                  "neutral": 0.8444947600364685,
                  "support": 0.15028297901153564
                },
                "stance_score": 0.14506068918853998,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.5986945033073425
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 6124,
                "faiss_score": 0.8902746438980103,
                "faiss_rank": 18,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 0.3690628707408905,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0213742908090353,
                  "neutral": 0.974280059337616,
                  "support": 0.004345625638961792
                },
                "stance_score": -0.01702866517007351,
                "evidence_contribution": -0.006284648052552744,
                "combined_rank_score": 1.2593375146389008
              },
              {
                "id": 6142,
                "faiss_score": 0.8960381746292114,
                "faiss_rank": 9,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -1.240119218826294,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.013231821358203888,
                  "neutral": 0.9757226705551147,
                  "support": 0.011045468039810658
                },
                "stance_score": -0.0021863533183932304,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.3440810441970825
              },
              {
                "id": 6131,
                "faiss_score": 0.8946009874343872,
                "faiss_rank": 11,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -2.0848493576049805,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.0008040505927056074,
                  "neutral": 0.9974097609519958,
                  "support": 0.0017861495725810528
                },
                "stance_score": 0.0009820989798754454,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.1902483701705933
              }
            ]
          }
        },
        {
          "subclaim": "Scaling model size introduces stability challenges",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 0.3388324719993685,
            "contradict": 0.0,
            "total": 0.3388324719993685
          },
          "evidence": {
            "supporting": [
              {
                "id": 6149,
                "faiss_score": 0.8830956220626831,
                "faiss_rank": 8,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 25,
                "sentence": "Scaling also introduces engineering challenges.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 2.749152183532715,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.010862507857382298,
                  "neutral": 0.8550251126289368,
                  "support": 0.13411231338977814
                },
                "stance_score": 0.12324980553239584,
                "evidence_contribution": 0.3388324719993685,
                "combined_rank_score": 3.632247805595398
              },
              {
                "id": 6133,
                "faiss_score": 0.9155011177062988,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -1.6219263076782227,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.004420503508299589,
                  "neutral": 0.3866603672504425,
                  "support": 0.608919084072113
                },
                "stance_score": 0.6044985805638134,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.7064251899719238
              },
              {
                "id": 5968,
                "faiss_score": 0.8963637351989746,
                "faiss_rank": 2,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 63,
                "sentence": "As models continue to scale, new bottlenecks emerge.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -6.650618076324463,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0010524292010813951,
                  "neutral": 0.18886996805667877,
                  "support": 0.8100776076316833
                },
                "stance_score": 0.809025178430602,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.754254341125488
              },
              {
                "id": 6349,
                "faiss_score": 0.8802475929260254,
                "faiss_rank": 12,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 88,
                "sentence": "As models scale, training efficiency becomes a primary concern.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -7.273441791534424,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.0099205132573843,
                  "neutral": 0.7918288707733154,
                  "support": 0.1982506662607193
                },
                "stance_score": 0.188330153003335,
                "evidence_contribution": 0.0,
                "combined_rank_score": -6.393194198608398
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 6131,
                "faiss_score": 0.8920418620109558,
                "faiss_rank": 3,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -2.5046567916870117,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0010439755860716105,
                  "neutral": 0.9971585273742676,
                  "support": 0.001797515549696982
                },
                "stance_score": 0.0007535399636253715,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.612614929676056
              },
              {
                "id": 6142,
                "faiss_score": 0.8803155422210693,
                "faiss_rank": 11,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -2.551440715789795,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.01365591213107109,
                  "neutral": 0.980880081653595,
                  "support": 0.00546405790373683
                },
                "stance_score": -0.008191854227334261,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.6711251735687256
              },
              {
                "id": 5919,
                "faiss_score": 0.878303587436676,
                "faiss_rank": 14,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "The challenge lies in maintaining numerical stability and avoiding excessive loss of accuracy.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -5.721813201904297,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0014828282874077559,
                  "neutral": 0.9933990240097046,
                  "support": 0.005118160974234343
                },
                "stance_score": 0.0036353326868265867,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.843509614467621
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The evidence presents both benefits and limitations.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Scaling model size improves performance",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6124,
                      "faiss_score": 0.9064903259277344,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 7.801340579986572,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0015502498717978597,
                        "neutral": 0.32961592078208923,
                        "support": 0.66883385181427
                      },
                      "stance_score": 0.6672836019424722,
                      "evidence_contribution": 5.205706642193415,
                      "combined_rank_score": 8.707830905914307
                    },
                    {
                      "id": 6133,
                      "faiss_score": 0.9184768795967102,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 7.172950744628906,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.1632184386253357,
                        "neutral": 0.1580629199743271,
                        "support": 0.678718626499176
                      },
                      "stance_score": 0.5155001878738403,
                      "evidence_contribution": 3.697657456466004,
                      "combined_rank_score": 8.091427624225616
                    },
                    {
                      "id": 6127,
                      "faiss_score": 0.9101240634918213,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "Empirical observations have shown that, under certain conditions, model performance improves predictably as scale increases.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 6.25351619720459,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.000981850316748023,
                        "neutral": 0.09745568037033081,
                        "support": 0.901562511920929
                      },
                      "stance_score": 0.9005806616041809,
                      "evidence_contribution": 5.631795754230971,
                      "combined_rank_score": 7.163640260696411
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6349,
                      "faiss_score": 0.8993617296218872,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 88,
                      "sentence": "As models scale, training efficiency becomes a primary concern.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -1.0130536556243896,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.26972973346710205,
                        "neutral": 0.7104320526123047,
                        "support": 0.019838299602270126
                      },
                      "stance_score": -0.24989143386483192,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.11369192600250244
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6142,
                      "faiss_score": 0.9155328869819641,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.632805347442627,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.003673245431855321,
                        "neutral": 0.966631293296814,
                        "support": 0.029695525765419006
                      },
                      "stance_score": 0.026022280333563685,
                      "evidence_contribution": 0.12055615948198495,
                      "combined_rank_score": 5.548338234424591
                    },
                    {
                      "id": 6131,
                      "faiss_score": 0.9074476361274719,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 2.8586673736572266,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.0011911247856914997,
                        "neutral": 0.995375394821167,
                        "support": 0.0034334997180849314
                      },
                      "stance_score": 0.0022423749323934317,
                      "evidence_contribution": 0.006410204058739932,
                      "combined_rank_score": 3.7661150097846985
                    },
                    {
                      "id": 6045,
                      "faiss_score": 0.8938406705856323,
                      "faiss_rank": 12,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 5,
                      "sentence": "As models scale, they appear to acquire new capabilities that were not present in smaller versions.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 0.17277783155441284,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.002974959323182702,
                        "neutral": 0.9193781018257141,
                        "support": 0.07764695584774017
                      },
                      "stance_score": 0.07467199652455747,
                      "evidence_contribution": 0.012901665637351692,
                      "combined_rank_score": 1.0666185021400452
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling model size introduces efficiency challenges",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6149,
                      "faiss_score": 0.9049237966537476,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 25,
                      "sentence": "Scaling also introduces engineering challenges.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 2.976050615310669,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.003267299383878708,
                        "neutral": 0.6837592720985413,
                        "support": 0.31297338008880615
                      },
                      "stance_score": 0.30970608070492744,
                      "evidence_contribution": 0.921700972047355,
                      "combined_rank_score": 3.8809744119644165
                    },
                    {
                      "id": 6349,
                      "faiss_score": 0.9152157306671143,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 88,
                      "sentence": "As models scale, training efficiency becomes a primary concern.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 1.1722049713134766,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0020032706670463085,
                        "neutral": 0.1527910679578781,
                        "support": 0.8452056646347046
                      },
                      "stance_score": 0.8432023939676583,
                      "evidence_contribution": 0.9884060380323136,
                      "combined_rank_score": 2.087420701980591
                    },
                    {
                      "id": 6157,
                      "faiss_score": 0.9232777953147888,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 33,
                      "sentence": "Inference efficiency is another scaling concern.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -0.5988900661468506,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.0029566814191639423,
                        "neutral": 0.8427377939224243,
                        "support": 0.15430548787117004
                      },
                      "stance_score": 0.1513488064520061,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.32438772916793823
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6124,
                      "faiss_score": 0.8902746438980103,
                      "faiss_rank": 18,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 0.3690628707408905,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0213742908090353,
                        "neutral": 0.974280059337616,
                        "support": 0.004345625638961792
                      },
                      "stance_score": -0.01702866517007351,
                      "evidence_contribution": -0.006284648052552744,
                      "combined_rank_score": 1.2593375146389008
                    },
                    {
                      "id": 6142,
                      "faiss_score": 0.8960381746292114,
                      "faiss_rank": 9,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -1.240119218826294,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.013231821358203888,
                        "neutral": 0.9757226705551147,
                        "support": 0.011045468039810658
                      },
                      "stance_score": -0.0021863533183932304,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.3440810441970825
                    },
                    {
                      "id": 6131,
                      "faiss_score": 0.8946009874343872,
                      "faiss_rank": 11,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -2.0848493576049805,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.0008040505927056074,
                        "neutral": 0.9974097609519958,
                        "support": 0.0017861495725810528
                      },
                      "stance_score": 0.0009820989798754454,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.1902483701705933
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling model size introduces stability challenges",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "weak",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6149,
                      "faiss_score": 0.8830956220626831,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 25,
                      "sentence": "Scaling also introduces engineering challenges.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 2.749152183532715,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.010862507857382298,
                        "neutral": 0.8550251126289368,
                        "support": 0.13411231338977814
                      },
                      "stance_score": 0.12324980553239584,
                      "evidence_contribution": 0.3388324719993685,
                      "combined_rank_score": 3.632247805595398
                    },
                    {
                      "id": 6133,
                      "faiss_score": 0.9155011177062988,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -1.6219263076782227,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.004420503508299589,
                        "neutral": 0.3866603672504425,
                        "support": 0.608919084072113
                      },
                      "stance_score": 0.6044985805638134,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.7064251899719238
                    },
                    {
                      "id": 5968,
                      "faiss_score": 0.8963637351989746,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 63,
                      "sentence": "As models continue to scale, new bottlenecks emerge.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -6.650618076324463,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.0010524292010813951,
                        "neutral": 0.18886996805667877,
                        "support": 0.8100776076316833
                      },
                      "stance_score": 0.809025178430602,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -5.754254341125488
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6131,
                      "faiss_score": 0.8920418620109558,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -2.5046567916870117,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0010439755860716105,
                        "neutral": 0.9971585273742676,
                        "support": 0.001797515549696982
                      },
                      "stance_score": 0.0007535399636253715,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.612614929676056
                    },
                    {
                      "id": 6142,
                      "faiss_score": 0.8803155422210693,
                      "faiss_rank": 11,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "Scaling laws attempt to formalize the relationship between model size, data size, compute, and performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -2.551440715789795,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.01365591213107109,
                        "neutral": 0.980880081653595,
                        "support": 0.00546405790373683
                      },
                      "stance_score": -0.008191854227334261,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.6711251735687256
                    },
                    {
                      "id": 5919,
                      "faiss_score": 0.878303587436676,
                      "faiss_rank": 14,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "The challenge lies in maintaining numerical stability and avoiding excessive loss of accuracy.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -5.721813201904297,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0014828282874077559,
                        "neutral": 0.9933990240097046,
                        "support": 0.005118160974234343
                      },
                      "stance_score": 0.0036353326868265867,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -4.843509614467621
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Distributed systems improve scalability but increase system complexity.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Distributed systems improve scalability",
          "verdict": "CONTRADICT",
          "controversial": true,
          "strengths": {
            "support": 1.0321325065354696,
            "contradict": 2.1855973309983234,
            "total": 3.217729837533793
          },
          "evidence": {
            "supporting": [
              {
                "id": 499,
                "faiss_score": 0.9017831683158875,
                "faiss_rank": 10,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 75,
                "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.7927249670028687,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0015425255987793207,
                  "neutral": 0.4211810231208801,
                  "support": 0.5772764086723328
                },
                "stance_score": 0.5757338830735534,
                "evidence_contribution": 1.0321325065354696,
                "combined_rank_score": 2.694508135318756
              }
            ],
            "contradicting": [
              {
                "id": 5658,
                "faiss_score": 0.9028811454772949,
                "faiss_rank": 9,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 61,
                "sentence": "Eventually consistent systems illustrate how relaxing guarantees can improve scalability.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 2.7626805305480957,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.8435043692588806,
                  "neutral": 0.10410597920417786,
                  "support": 0.052389614284038544
                },
                "stance_score": -0.7911147549748421,
                "evidence_contribution": -2.1855973309983234,
                "combined_rank_score": 3.6655616760253906
              }
            ],
            "neutral": [
              {
                "id": 445,
                "faiss_score": 0.9056973457336426,
                "faiss_rank": 4,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.2563084363937378,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.000767619232647121,
                  "neutral": 0.9977124929428101,
                  "support": 0.001519894110970199
                },
                "stance_score": 0.0007522748783230782,
                "evidence_contribution": 0.0009450892761243557,
                "combined_rank_score": 2.1620057821273804
              },
              {
                "id": 5597,
                "faiss_score": 0.9030466675758362,
                "faiss_rank": 8,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 1.2206794023513794,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0007695626700296998,
                  "neutral": 0.9976724982261658,
                  "support": 0.0015579789178445935
                },
                "stance_score": 0.0007884162478148937,
                "evidence_contribution": 0.0009624034741868015,
                "combined_rank_score": 2.1237260699272156
              },
              {
                "id": 5646,
                "faiss_score": 0.9050657749176025,
                "faiss_rank": 7,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 49,
                "sentence": "The evolution of distributed systems has been driven by practical needs.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": -0.1592615842819214,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.0006361278356052935,
                  "neutral": 0.9965994954109192,
                  "support": 0.002764445496723056
                },
                "stance_score": 0.0021283176611177623,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.7458041906356812
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems increase system complexity",
          "verdict": "MIXED",
          "controversial": true,
          "strengths": {
            "support": 1.6687459316599744,
            "contradict": 1.3767160541470862,
            "total": 3.0454619858070604
          },
          "evidence": {
            "supporting": [
              {
                "id": 6234,
                "faiss_score": 0.8951437473297119,
                "faiss_rank": 15,
                "doc_id": "local_ml_training_dynamics.txt",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Distributed training introduces additional complexity into training dynamics.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                "primary_category": null,
                "rerank_score": 0.9794656038284302,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.0011164408642798662,
                  "neutral": 0.06910634785890579,
                  "support": 0.929777204990387
                },
                "stance_score": 0.9286607641261071,
                "evidence_contribution": 0.9095912760865489,
                "combined_rank_score": 1.874609351158142
              },
              {
                "id": 5674,
                "faiss_score": 0.9308851957321167,
                "faiss_rank": 1,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 77,
                "sentence": "Distributed systems exemplify the broader theme that complexity emerges from interaction.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 6.873047828674316,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.13098503649234772,
                  "neutral": 0.6275760531425476,
                  "support": 0.24143889546394348
                },
                "stance_score": 0.11045385897159576,
                "evidence_contribution": 0.7591546555734254,
                "combined_rank_score": 7.803933024406433
              }
            ],
            "contradicting": [
              {
                "id": 499,
                "faiss_score": 0.9024401903152466,
                "faiss_rank": 8,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 75,
                "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.669622778892517,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.8282386660575867,
                  "neutral": 0.16808989644050598,
                  "support": 0.0036715413443744183
                },
                "stance_score": -0.8245671247132123,
                "evidence_contribution": -1.3767160541470862,
                "combined_rank_score": 2.5720629692077637
              }
            ],
            "neutral": [
              {
                "id": 5597,
                "faiss_score": 0.9027834534645081,
                "faiss_rank": 7,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 1.5515655279159546,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.04560400918126106,
                  "neutral": 0.9508848190307617,
                  "support": 0.0035111713223159313
                },
                "stance_score": -0.04209283785894513,
                "evidence_contribution": -0.06530979619409488,
                "combined_rank_score": 2.4543489813804626
              },
              {
                "id": 445,
                "faiss_score": 0.9034885168075562,
                "faiss_rank": 3,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.5492687225341797,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.03827379271388054,
                  "neutral": 0.9580088257789612,
                  "support": 0.003717323299497366
                },
                "stance_score": -0.03455646941438317,
                "evidence_contribution": -0.05353725722491287,
                "combined_rank_score": 2.452757239341736
              },
              {
                "id": 5650,
                "faiss_score": 0.9090577363967896,
                "faiss_rank": 2,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 53,
                "sentence": "Distributed systems also intersect with security concerns.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 0.526348888874054,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.0072384984232485294,
                  "neutral": 0.9825186729431152,
                  "support": 0.010242801159620285
                },
                "stance_score": 0.0030043027363717556,
                "evidence_contribution": 0.0015813114071305534,
                "combined_rank_score": 1.4354066252708435
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Distributed systems improve scalability",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 499,
                      "faiss_score": 0.9017831683158875,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.7927249670028687,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0015425255987793207,
                        "neutral": 0.4211810231208801,
                        "support": 0.5772764086723328
                      },
                      "stance_score": 0.5757338830735534,
                      "evidence_contribution": 1.0321325065354696,
                      "combined_rank_score": 2.694508135318756
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 5658,
                      "faiss_score": 0.9028811454772949,
                      "faiss_rank": 9,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 61,
                      "sentence": "Eventually consistent systems illustrate how relaxing guarantees can improve scalability.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 2.7626805305480957,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.8435043692588806,
                        "neutral": 0.10410597920417786,
                        "support": 0.052389614284038544
                      },
                      "stance_score": -0.7911147549748421,
                      "evidence_contribution": -2.1855973309983234,
                      "combined_rank_score": 3.6655616760253906
                    }
                  ],
                  "neutral": [
                    {
                      "id": 445,
                      "faiss_score": 0.9056973457336426,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.2563084363937378,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.000767619232647121,
                        "neutral": 0.9977124929428101,
                        "support": 0.001519894110970199
                      },
                      "stance_score": 0.0007522748783230782,
                      "evidence_contribution": 0.0009450892761243557,
                      "combined_rank_score": 2.1620057821273804
                    },
                    {
                      "id": 5597,
                      "faiss_score": 0.9030466675758362,
                      "faiss_rank": 8,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 1.2206794023513794,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.0007695626700296998,
                        "neutral": 0.9976724982261658,
                        "support": 0.0015579789178445935
                      },
                      "stance_score": 0.0007884162478148937,
                      "evidence_contribution": 0.0009624034741868015,
                      "combined_rank_score": 2.1237260699272156
                    },
                    {
                      "id": 5646,
                      "faiss_score": 0.9050657749176025,
                      "faiss_rank": 7,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 49,
                      "sentence": "The evolution of distributed systems has been driven by practical needs.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": -0.1592615842819214,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.0006361278356052935,
                        "neutral": 0.9965994954109192,
                        "support": 0.002764445496723056
                      },
                      "stance_score": 0.0021283176611177623,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.7458041906356812
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "CONTROVERSIAL_ASPECTS",
            "items": [
              {
                "subclaim": "Distributed systems increase system complexity",
                "verdict": "MIXED",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 5674,
                      "faiss_score": 0.9308851957321167,
                      "faiss_rank": 1,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 77,
                      "sentence": "Distributed systems exemplify the broader theme that complexity emerges from interaction.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 6.873047828674316,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.13098503649234772,
                        "neutral": 0.6275760531425476,
                        "support": 0.24143889546394348
                      },
                      "stance_score": 0.11045385897159576,
                      "evidence_contribution": 0.7591546555734254,
                      "combined_rank_score": 7.803933024406433
                    },
                    {
                      "id": 6234,
                      "faiss_score": 0.8951437473297119,
                      "faiss_rank": 15,
                      "doc_id": "local_ml_training_dynamics.txt",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Distributed training introduces additional complexity into training dynamics.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                      "primary_category": null,
                      "rerank_score": 0.9794656038284302,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.0011164408642798662,
                        "neutral": 0.06910634785890579,
                        "support": 0.929777204990387
                      },
                      "stance_score": 0.9286607641261071,
                      "evidence_contribution": 0.9095912760865489,
                      "combined_rank_score": 1.874609351158142
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 499,
                      "faiss_score": 0.9024401903152466,
                      "faiss_rank": 8,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.669622778892517,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.8282386660575867,
                        "neutral": 0.16808989644050598,
                        "support": 0.0036715413443744183
                      },
                      "stance_score": -0.8245671247132123,
                      "evidence_contribution": -1.3767160541470862,
                      "combined_rank_score": 2.5720629692077637
                    }
                  ],
                  "neutral": [
                    {
                      "id": 5597,
                      "faiss_score": 0.9027834534645081,
                      "faiss_rank": 7,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 1.5515655279159546,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.04560400918126106,
                        "neutral": 0.9508848190307617,
                        "support": 0.0035111713223159313
                      },
                      "stance_score": -0.04209283785894513,
                      "evidence_contribution": -0.06530979619409488,
                      "combined_rank_score": 2.4543489813804626
                    },
                    {
                      "id": 445,
                      "faiss_score": 0.9034885168075562,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.5492687225341797,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.03827379271388054,
                        "neutral": 0.9580088257789612,
                        "support": 0.003717323299497366
                      },
                      "stance_score": -0.03455646941438317,
                      "evidence_contribution": -0.05353725722491287,
                      "combined_rank_score": 2.452757239341736
                    },
                    {
                      "id": 5650,
                      "faiss_score": 0.9090577363967896,
                      "faiss_rank": 2,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 53,
                      "sentence": "Distributed systems also intersect with security concerns.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 0.526348888874054,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.0072384984232485294,
                        "neutral": 0.9825186729431152,
                        "support": 0.010242801159620285
                      },
                      "stance_score": 0.0030043027363717556,
                      "evidence_contribution": 0.0015813114071305534,
                      "combined_rank_score": 1.4354066252708435
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Large language models are powerful but can produce incorrect or misleading outputs.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "MIXED",
      "subclaims": [
        {
          "subclaim": "Large language models are powerful",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 7.616252655345128,
            "contradict": 0.9706514910640749,
            "total": 8.586904146409204
          },
          "evidence": {
            "supporting": [
              {
                "id": 6121,
                "faiss_score": 0.9278501868247986,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 9.14767837524414,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.007072408217936754,
                  "neutral": 0.2285161018371582,
                  "support": 0.764411449432373
                },
                "stance_score": 0.7573390412144363,
                "evidence_contribution": 6.92789397004543,
                "combined_rank_score": 10.07552856206894
              },
              {
                "id": 6043,
                "faiss_score": 0.9241077899932861,
                "faiss_rank": 3,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 5.496891975402832,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.005367050878703594,
                  "neutral": 0.8640390634536743,
                  "support": 0.130593940615654
                },
                "stance_score": 0.1252268897369504,
                "evidence_contribution": 0.6883586852996979,
                "combined_rank_score": 6.420999765396118
              }
            ],
            "contradicting": [
              {
                "id": 2020,
                "faiss_score": 0.9012563228607178,
                "faiss_rank": 10,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 6.476112365722656,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.2466990351676941,
                  "neutral": 0.6564837694168091,
                  "support": 0.09681721776723862
                },
                "stance_score": -0.14988181740045547,
                "evidence_contribution": -0.9706514910640749,
                "combined_rank_score": 7.377368688583374
              }
            ],
            "neutral": [
              {
                "id": 6040,
                "faiss_score": 0.925839900970459,
                "faiss_rank": 2,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 6.383889675140381,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0008157117408700287,
                  "neutral": 0.9963352680206299,
                  "support": 0.002849036827683449
                },
                "stance_score": 0.00203332508681342,
                "evidence_contribution": 0.012980523027912111,
                "combined_rank_score": 7.30972957611084
              },
              {
                "id": 6079,
                "faiss_score": 0.8906152248382568,
                "faiss_rank": 14,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "Large language models are often described as general-purpose systems because they can be adapted to many tasks with minimal fine-tuning.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 6.289165019989014,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0016379584558308125,
                  "neutral": 0.9570803046226501,
                  "support": 0.04128176346421242
                },
                "stance_score": 0.039643805008381605,
                "evidence_contribution": 0.24932643171797886,
                "combined_rank_score": 7.1797802448272705
              },
              {
                "id": 6047,
                "faiss_score": 0.9033905267715454,
                "faiss_rank": 9,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Despite these capabilities, the behavior of large language models remains fundamentally probabilistic.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 5.861764430999756,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.013952930457890034,
                  "neutral": 0.9331933259963989,
                  "support": 0.052853744477033615
                },
                "stance_score": 0.03890081401914358,
                "evidence_contribution": 0.2280274079543525,
                "combined_rank_score": 6.765154957771301
              }
            ]
          }
        },
        {
          "subclaim": "can produce incorrect or misleading outputs",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 3.027372747354259,
            "contradict": 0.0,
            "total": 3.027372747354259
          },
          "evidence": {
            "supporting": [
              {
                "id": 6606,
                "faiss_score": 0.863052248954773,
                "faiss_rank": 1,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "Components may crash completely, producing no output, or they may continue running while producing incorrect results.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": 3.4065425395965576,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0009414730593562126,
                  "neutral": 0.20099835097789764,
                  "support": 0.7980602383613586
                },
                "stance_score": 0.7971187653020024,
                "evidence_contribution": 2.715418983111956,
                "combined_rank_score": 4.269594788551331
              },
              {
                "id": 3535,
                "faiss_score": 0.8539549112319946,
                "faiss_rank": 4,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 106,
                "sentence": "In this case, the voting circuit can output the correct result, and discard the erroneous version.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 1.0036264657974243,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.11982972174882889,
                  "neutral": 0.4495139420032501,
                  "support": 0.4306562840938568
                },
                "stance_score": 0.3108265623450279,
                "evidence_contribution": 0.31195376424230314,
                "combined_rank_score": 1.857581377029419
              },
              {
                "id": 892,
                "faiss_score": 0.8531967401504517,
                "faiss_rank": 5,
                "doc_id": "wiki_Error_correction",
                "file_type": ".txt",
                "position": 65,
                "sentence": "of errors in the output.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Error_detection_and_correction",
                "primary_category": "all articles needing additional references",
                "rerank_score": -0.670259952545166,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0012530958047136664,
                  "neutral": 0.3292033076286316,
                  "support": 0.6695435643196106
                },
                "stance_score": 0.6682904685148969,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.18293678760528564
              },
              {
                "id": 2227,
                "faiss_score": 0.8592409491539001,
                "faiss_rank": 2,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 158,
                "sentence": "The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": -0.9176287651062012,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.007495637983083725,
                  "neutral": 0.5303006172180176,
                  "support": 0.4622037410736084
                },
                "stance_score": 0.4547081030905247,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.058387815952301025
              },
              {
                "id": 2969,
                "faiss_score": 0.8526178598403931,
                "faiss_rank": 6,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 24,
                "sentence": "If the input is long, then the output vector would not be able to contain all relevant information, degrading the output.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": -6.412653923034668,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.002237267093732953,
                  "neutral": 0.8547115921974182,
                  "support": 0.1430511325597763
                },
                "stance_score": 0.14081386546604335,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.560036063194275
              },
              {
                "id": 6428,
                "faiss_score": 0.843582034111023,
                "faiss_rank": 13,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 167,
                "sentence": "Small changes in phrasing, ordering, or context can lead to significant differences in output.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -6.489630699157715,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.0022780755534768105,
                  "neutral": 0.7048073410987854,
                  "support": 0.29291456937789917
                },
                "stance_score": 0.29063649382442236,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.646048665046692
              },
              {
                "id": 6360,
                "faiss_score": 0.8480008840560913,
                "faiss_rank": 9,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 99,
                "sentence": "However, approximations can introduce errors or biases that affect model behavior in subtle ways.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -7.292495250701904,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0016916384920477867,
                  "neutral": 0.5194053053855896,
                  "support": 0.4789030849933624
                },
                "stance_score": 0.47721144650131464,
                "evidence_contribution": 0.0,
                "combined_rank_score": -6.444494366645813
              },
              {
                "id": 1219,
                "faiss_score": 0.8399705290794373,
                "faiss_rank": 16,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 244,
                "sentence": "Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -7.369985580444336,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.0005253414856269956,
                  "neutral": 0.025972124189138412,
                  "support": 0.973502516746521
                },
                "stance_score": 0.972977175260894,
                "evidence_contribution": 0.0,
                "combined_rank_score": -6.530015051364899
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 1310,
                "faiss_score": 0.8482428789138794,
                "faiss_rank": 8,
                "doc_id": "wiki_Statistical_learning_theory",
                "file_type": ".txt",
                "position": 20,
                "sentence": "It takes the value 0 if the predicted output is the same as the actual output, and it takes the value 1 if the predicted output is different from the actual output.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Statistical_learning_theory",
                "primary_category": "machine learning",
                "rerank_score": -6.68577766418457,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.004643861670047045,
                  "neutral": 0.9214097261428833,
                  "support": 0.07394646108150482
                },
                "stance_score": 0.06930259941145778,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.837534785270691
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The evidence presents both benefits and limitations.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Large language models are powerful",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6121,
                      "faiss_score": 0.9278501868247986,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 9.14767837524414,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.007072408217936754,
                        "neutral": 0.2285161018371582,
                        "support": 0.764411449432373
                      },
                      "stance_score": 0.7573390412144363,
                      "evidence_contribution": 6.92789397004543,
                      "combined_rank_score": 10.07552856206894
                    },
                    {
                      "id": 6043,
                      "faiss_score": 0.9241077899932861,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 5.496891975402832,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.005367050878703594,
                        "neutral": 0.8640390634536743,
                        "support": 0.130593940615654
                      },
                      "stance_score": 0.1252268897369504,
                      "evidence_contribution": 0.6883586852996979,
                      "combined_rank_score": 6.420999765396118
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 2020,
                      "faiss_score": 0.9012563228607178,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 6.476112365722656,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.2466990351676941,
                        "neutral": 0.6564837694168091,
                        "support": 0.09681721776723862
                      },
                      "stance_score": -0.14988181740045547,
                      "evidence_contribution": -0.9706514910640749,
                      "combined_rank_score": 7.377368688583374
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6040,
                      "faiss_score": 0.925839900970459,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 6.383889675140381,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0008157117408700287,
                        "neutral": 0.9963352680206299,
                        "support": 0.002849036827683449
                      },
                      "stance_score": 0.00203332508681342,
                      "evidence_contribution": 0.012980523027912111,
                      "combined_rank_score": 7.30972957611084
                    },
                    {
                      "id": 6079,
                      "faiss_score": 0.8906152248382568,
                      "faiss_rank": 14,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 39,
                      "sentence": "Large language models are often described as general-purpose systems because they can be adapted to many tasks with minimal fine-tuning.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 6.289165019989014,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.0016379584558308125,
                        "neutral": 0.9570803046226501,
                        "support": 0.04128176346421242
                      },
                      "stance_score": 0.039643805008381605,
                      "evidence_contribution": 0.24932643171797886,
                      "combined_rank_score": 7.1797802448272705
                    },
                    {
                      "id": 6047,
                      "faiss_score": 0.9033905267715454,
                      "faiss_rank": 9,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "Despite these capabilities, the behavior of large language models remains fundamentally probabilistic.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 5.861764430999756,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.013952930457890034,
                        "neutral": 0.9331933259963989,
                        "support": 0.052853744477033615
                      },
                      "stance_score": 0.03890081401914358,
                      "evidence_contribution": 0.2280274079543525,
                      "combined_rank_score": 6.765154957771301
                    }
                  ]
                }
              },
              {
                "subclaim": "can produce incorrect or misleading outputs",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6606,
                      "faiss_score": 0.863052248954773,
                      "faiss_rank": 1,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "Components may crash completely, producing no output, or they may continue running while producing incorrect results.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": 3.4065425395965576,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0009414730593562126,
                        "neutral": 0.20099835097789764,
                        "support": 0.7980602383613586
                      },
                      "stance_score": 0.7971187653020024,
                      "evidence_contribution": 2.715418983111956,
                      "combined_rank_score": 4.269594788551331
                    },
                    {
                      "id": 3535,
                      "faiss_score": 0.8539549112319946,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Fault_tolerance",
                      "file_type": ".txt",
                      "position": 106,
                      "sentence": "In this case, the voting circuit can output the correct result, and discard the erroneous version.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 1.0036264657974243,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.11982972174882889,
                        "neutral": 0.4495139420032501,
                        "support": 0.4306562840938568
                      },
                      "stance_score": 0.3108265623450279,
                      "evidence_contribution": 0.31195376424230314,
                      "combined_rank_score": 1.857581377029419
                    },
                    {
                      "id": 892,
                      "faiss_score": 0.8531967401504517,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Error_correction",
                      "file_type": ".txt",
                      "position": 65,
                      "sentence": "of errors in the output.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Error_detection_and_correction",
                      "primary_category": "all articles needing additional references",
                      "rerank_score": -0.670259952545166,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0012530958047136664,
                        "neutral": 0.3292033076286316,
                        "support": 0.6695435643196106
                      },
                      "stance_score": 0.6682904685148969,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.18293678760528564
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 1310,
                      "faiss_score": 0.8482428789138794,
                      "faiss_rank": 8,
                      "doc_id": "wiki_Statistical_learning_theory",
                      "file_type": ".txt",
                      "position": 20,
                      "sentence": "It takes the value 0 if the predicted output is the same as the actual output, and it takes the value 1 if the predicted output is different from the actual output.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Statistical_learning_theory",
                      "primary_category": "machine learning",
                      "rerank_score": -6.68577766418457,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.004643861670047045,
                        "neutral": 0.9214097261428833,
                        "support": 0.07394646108150482
                      },
                      "stance_score": 0.06930259941145778,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -5.837534785270691
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Quantum error correction enables scaling but adds significant overhead.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "MIXED",
      "subclaims": [
        {
          "subclaim": "Quantum error correction enables scaling",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 10.564751768059006,
            "contradict": 0.0,
            "total": 10.564751768059006
          },
          "evidence": {
            "supporting": [
              {
                "id": 6553,
                "faiss_score": 0.9426121711730957,
                "faiss_rank": 3,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 6.97873592376709,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.009470941498875618,
                  "neutral": 0.13609865307807922,
                  "support": 0.8544303774833679
                },
                "stance_score": 0.8449594359844923,
                "evidence_contribution": 5.896748770030955,
                "combined_rank_score": 7.9213480949401855
              },
              {
                "id": 4795,
                "faiss_score": 0.9083656072616577,
                "faiss_rank": 9,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 4.952396392822266,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.020467771217226982,
                  "neutral": 0.22572223842144012,
                  "support": 0.7538099884986877
                },
                "stance_score": 0.7333422172814608,
                "evidence_contribution": 3.6318013515689884,
                "combined_rank_score": 5.860762000083923
              },
              {
                "id": 4719,
                "faiss_score": 0.9455520510673523,
                "faiss_rank": 1,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 6.933759689331055,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.1649482250213623,
                  "neutral": 0.5206605792045593,
                  "support": 0.31439119577407837
                },
                "stance_score": 0.14944297075271606,
                "evidence_contribution": 1.0362016464590624,
                "combined_rank_score": 7.879311740398407
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 4862,
                "faiss_score": 0.9224804043769836,
                "faiss_rank": 6,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 109,
                "sentence": "Quantum error correction can be applied to quantum metrology.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 1.7925801277160645,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0023149496410042048,
                  "neutral": 0.9899492263793945,
                  "support": 0.007735862862318754
                },
                "stance_score": 0.0054209132213145494,
                "evidence_contribution": 0.009717421314601737,
                "combined_rank_score": 2.715060532093048
              },
              {
                "id": 756,
                "faiss_score": 0.9023146629333496,
                "faiss_rank": 11,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 160,
                "sentence": "As described by the threshold theorem, if the error rate is small enough, it is thought to be possible to use quantum error correction to suppress errors and decoherence.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 1.5890746116638184,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.002483957214280963,
                  "neutral": 0.988044798374176,
                  "support": 0.009471235796809196
                },
                "stance_score": 0.0069872785825282335,
                "evidence_contribution": 0.011103307000117968,
                "combined_rank_score": 2.491389274597168
              },
              {
                "id": 4824,
                "faiss_score": 0.9028662443161011,
                "faiss_rank": 10,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 71,
                "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 1.4517815113067627,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.001554966322146356,
                  "neutral": 0.9931433200836182,
                  "support": 0.0053017293103039265
                },
                "stance_score": 0.0037467629881575704,
                "evidence_contribution": 0.00543948123345564,
                "combined_rank_score": 2.3546477556228638
              }
            ]
          }
        },
        {
          "subclaim": "Quantum error correction adds significant overhead",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 8.773727150002745,
            "contradict": 0.0,
            "total": 8.773727150002745
          },
          "evidence": {
            "supporting": [
              {
                "id": 764,
                "faiss_score": 0.9263245463371277,
                "faiss_rank": 6,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 168,
                "sentence": "However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 5.299180507659912,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.001764934859238565,
                  "neutral": 0.11494402587413788,
                  "support": 0.8832909464836121
                },
                "stance_score": 0.8815260116243735,
                "evidence_contribution": 4.671365457795065,
                "combined_rank_score": 6.22550505399704
              },
              {
                "id": 4719,
                "faiss_score": 0.9596387147903442,
                "faiss_rank": 1,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.731478214263916,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.001737573416903615,
                  "neutral": 0.12948888540267944,
                  "support": 0.8687735199928284
                },
                "stance_score": 0.8670359465759248,
                "evidence_contribution": 4.102361692207681,
                "combined_rank_score": 5.69111692905426
              },
              {
                "id": 6553,
                "faiss_score": 0.9569774270057678,
                "faiss_rank": 3,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": -0.2708771228790283,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0009398495312780142,
                  "neutral": 0.05072243511676788,
                  "support": 0.9483376741409302
                },
                "stance_score": 0.9473978246096522,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.6861003041267395
              }
            ],
            "contradicting": [
              {
                "id": 4795,
                "faiss_score": 0.89435875415802,
                "faiss_rank": 11,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": -0.4065723419189453,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.14968205988407135,
                  "neutral": 0.841672420501709,
                  "support": 0.008645503781735897
                },
                "stance_score": -0.14103655610233545,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.4877864122390747
              },
              {
                "id": 4824,
                "faiss_score": 0.8923410177230835,
                "faiss_rank": 14,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 71,
                "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": -0.7281641960144043,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.20826101303100586,
                  "neutral": 0.7879799008369446,
                  "support": 0.003759042825549841
                },
                "stance_score": -0.20450197020545602,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.1641768217086792
              }
            ],
            "neutral": [
              {
                "id": 4862,
                "faiss_score": 0.894229531288147,
                "faiss_rank": 12,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 109,
                "sentence": "Quantum error correction can be applied to quantum metrology.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": -0.7784135341644287,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.00450995983555913,
                  "neutral": 0.9947214126586914,
                  "support": 0.0007686226163059473
                },
                "stance_score": -0.0037413372192531824,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.11581599712371826
              },
              {
                "id": 6555,
                "faiss_score": 0.9089707136154175,
                "faiss_rank": 9,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 15,
                "sentence": "This overhead means that a useful, fault-tolerant quantum computer would need orders of magnitude more qubits than are currently available.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": -0.9043731689453125,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.008294355124235153,
                  "neutral": 0.9852702617645264,
                  "support": 0.006435340270400047
                },
                "stance_score": -0.001859014853835106,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.0045975446701049805
              },
              {
                "id": 6554,
                "faiss_score": 0.915757417678833,
                "faiss_rank": 8,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": -0.9889041781425476,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.0015190973645076156,
                  "neutral": 0.9730349183082581,
                  "support": 0.025445954874157906
                },
                "stance_score": 0.02392685750965029,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.0731467604637146
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The evidence presents both benefits and limitations.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Quantum error correction enables scaling",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6553,
                      "faiss_score": 0.9426121711730957,
                      "faiss_rank": 3,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 6.97873592376709,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.009470941498875618,
                        "neutral": 0.13609865307807922,
                        "support": 0.8544303774833679
                      },
                      "stance_score": 0.8449594359844923,
                      "evidence_contribution": 5.896748770030955,
                      "combined_rank_score": 7.9213480949401855
                    },
                    {
                      "id": 4719,
                      "faiss_score": 0.9455520510673523,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 6.933759689331055,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.1649482250213623,
                        "neutral": 0.5206605792045593,
                        "support": 0.31439119577407837
                      },
                      "stance_score": 0.14944297075271606,
                      "evidence_contribution": 1.0362016464590624,
                      "combined_rank_score": 7.879311740398407
                    },
                    {
                      "id": 4795,
                      "faiss_score": 0.9083656072616577,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 4.952396392822266,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.020467771217226982,
                        "neutral": 0.22572223842144012,
                        "support": 0.7538099884986877
                      },
                      "stance_score": 0.7333422172814608,
                      "evidence_contribution": 3.6318013515689884,
                      "combined_rank_score": 5.860762000083923
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 4862,
                      "faiss_score": 0.9224804043769836,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 109,
                      "sentence": "Quantum error correction can be applied to quantum metrology.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 1.7925801277160645,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0023149496410042048,
                        "neutral": 0.9899492263793945,
                        "support": 0.007735862862318754
                      },
                      "stance_score": 0.0054209132213145494,
                      "evidence_contribution": 0.009717421314601737,
                      "combined_rank_score": 2.715060532093048
                    },
                    {
                      "id": 756,
                      "faiss_score": 0.9023146629333496,
                      "faiss_rank": 11,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 160,
                      "sentence": "As described by the threshold theorem, if the error rate is small enough, it is thought to be possible to use quantum error correction to suppress errors and decoherence.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 1.5890746116638184,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.002483957214280963,
                        "neutral": 0.988044798374176,
                        "support": 0.009471235796809196
                      },
                      "stance_score": 0.0069872785825282335,
                      "evidence_contribution": 0.011103307000117968,
                      "combined_rank_score": 2.491389274597168
                    },
                    {
                      "id": 4824,
                      "faiss_score": 0.9028662443161011,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 71,
                      "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 1.4517815113067627,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.001554966322146356,
                        "neutral": 0.9931433200836182,
                        "support": 0.0053017293103039265
                      },
                      "stance_score": 0.0037467629881575704,
                      "evidence_contribution": 0.00543948123345564,
                      "combined_rank_score": 2.3546477556228638
                    }
                  ]
                }
              },
              {
                "subclaim": "Quantum error correction adds significant overhead",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 764,
                      "faiss_score": 0.9263245463371277,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 168,
                      "sentence": "However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 5.299180507659912,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.001764934859238565,
                        "neutral": 0.11494402587413788,
                        "support": 0.8832909464836121
                      },
                      "stance_score": 0.8815260116243735,
                      "evidence_contribution": 4.671365457795065,
                      "combined_rank_score": 6.22550505399704
                    },
                    {
                      "id": 4719,
                      "faiss_score": 0.9596387147903442,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.731478214263916,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.001737573416903615,
                        "neutral": 0.12948888540267944,
                        "support": 0.8687735199928284
                      },
                      "stance_score": 0.8670359465759248,
                      "evidence_contribution": 4.102361692207681,
                      "combined_rank_score": 5.69111692905426
                    },
                    {
                      "id": 6553,
                      "faiss_score": 0.9569774270057678,
                      "faiss_rank": 3,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": -0.2708771228790283,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0009398495312780142,
                        "neutral": 0.05072243511676788,
                        "support": 0.9483376741409302
                      },
                      "stance_score": 0.9473978246096522,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.6861003041267395
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 4795,
                      "faiss_score": 0.89435875415802,
                      "faiss_rank": 11,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": -0.4065723419189453,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.14968205988407135,
                        "neutral": 0.841672420501709,
                        "support": 0.008645503781735897
                      },
                      "stance_score": -0.14103655610233545,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.4877864122390747
                    },
                    {
                      "id": 4824,
                      "faiss_score": 0.8923410177230835,
                      "faiss_rank": 14,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 71,
                      "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": -0.7281641960144043,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.20826101303100586,
                        "neutral": 0.7879799008369446,
                        "support": 0.003759042825549841
                      },
                      "stance_score": -0.20450197020545602,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.1641768217086792
                    }
                  ],
                  "neutral": [
                    {
                      "id": 4862,
                      "faiss_score": 0.894229531288147,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 109,
                      "sentence": "Quantum error correction can be applied to quantum metrology.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": -0.7784135341644287,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.00450995983555913,
                        "neutral": 0.9947214126586914,
                        "support": 0.0007686226163059473
                      },
                      "stance_score": -0.0037413372192531824,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.11581599712371826
                    },
                    {
                      "id": 6555,
                      "faiss_score": 0.9089707136154175,
                      "faiss_rank": 9,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 15,
                      "sentence": "This overhead means that a useful, fault-tolerant quantum computer would need orders of magnitude more qubits than are currently available.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": -0.9043731689453125,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.008294355124235153,
                        "neutral": 0.9852702617645264,
                        "support": 0.006435340270400047
                      },
                      "stance_score": -0.001859014853835106,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.0045975446701049805
                    },
                    {
                      "id": 6554,
                      "faiss_score": 0.915757417678833,
                      "faiss_rank": 8,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": -0.9889041781425476,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.0015190973645076156,
                        "neutral": 0.9730349183082581,
                        "support": 0.025445954874157906
                      },
                      "stance_score": 0.02392685750965029,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.0731467604637146
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Future architectures will eliminate the need for large datasets in machine learning.",
      "expected_verdict": "INCONCLUSIVE",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "Future architectures will eliminate the need for large datasets in machine learning.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 6124,
                "faiss_score": 0.8617883920669556,
                "faiss_rank": 16,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -1.6935912370681763,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.10562246292829514,
                  "neutral": 0.893295168876648,
                  "support": 0.0010824142955243587
                },
                "stance_score": -0.10454004863277078,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.8318028450012207
              },
              {
                "id": 5905,
                "faiss_score": 0.9030225276947021,
                "faiss_rank": 1,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "As machine learning models have grown larger and more capable, efficiency has become a central concern.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -2.905686140060425,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.11705312877893448,
                  "neutral": 0.8816056251525879,
                  "support": 0.0013412677217274904
                },
                "stance_score": -0.11571186105720699,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.0026636123657227
              },
              {
                "id": 6125,
                "faiss_score": 0.8606041669845581,
                "faiss_rank": 18,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Over the past decade, scaling has emerged as one of the most reliable drivers of progress in machine learning systems, particularly in deep learning.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -4.260554313659668,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.22337916493415833,
                  "neutral": 0.7750259637832642,
                  "support": 0.0015949285589158535
                },
                "stance_score": -0.22178423637524247,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.39995014667511
              },
              {
                "id": 6165,
                "faiss_score": 0.8752605319023132,
                "faiss_rank": 4,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 41,
                "sentence": "From a systems perspective, scaling reshapes the entire machine learning pipeline.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -5.4250335693359375,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.1764662116765976,
                  "neutral": 0.8213576674461365,
                  "support": 0.002176115522161126
                },
                "stance_score": -0.17429009615443647,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.549773037433624
              }
            ],
            "neutral": [
              {
                "id": 2944,
                "faiss_score": 0.8668415546417236,
                "faiss_rank": 9,
                "doc_id": "wiki_Self-supervised_learning",
                "file_type": ".txt",
                "position": 53,
                "sentence": "Its ability to leverage unlabeled data effectively opens new possibilities for advancement in machine learning, especially in data-driven application domains.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Self-supervised_learning",
                "primary_category": "machine learning",
                "rerank_score": -2.1371984481811523,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.02333276905119419,
                  "neutral": 0.9753410816192627,
                  "support": 0.0013262011343613267
                },
                "stance_score": -0.022006567916832864,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.2703568935394287
              },
              {
                "id": 349,
                "faiss_score": 0.8643662333488464,
                "faiss_rank": 13,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 240,
                "sentence": "Typically, machine learning models require a high quantity of reliable data to perform accurate predictions.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -2.83878231048584,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.01353005226701498,
                  "neutral": 0.9858565330505371,
                  "support": 0.0006134053110145032
                },
                "stance_score": -0.012916646956000477,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.9744160771369934
              },
              {
                "id": 6126,
                "faiss_score": 0.872998833656311,
                "faiss_rank": 5,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 2,
                "sentence": "Rather than relying on narrowly optimized architectures or handcrafted features, many modern systems achieve strong performance by training large models on vast amounts of data using substantial compute.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -3.1559386253356934,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.0634174570441246,
                  "neutral": 0.935447633266449,
                  "support": 0.0011348612606525421
                },
                "stance_score": -0.06228259578347206,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.2829397916793823
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Future architectures will eliminate the need for large datasets in machine learning.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 6124,
                      "faiss_score": 0.8617883920669556,
                      "faiss_rank": 16,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -1.6935912370681763,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.10562246292829514,
                        "neutral": 0.893295168876648,
                        "support": 0.0010824142955243587
                      },
                      "stance_score": -0.10454004863277078,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.8318028450012207
                    },
                    {
                      "id": 5905,
                      "faiss_score": 0.9030225276947021,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "As machine learning models have grown larger and more capable, efficiency has become a central concern.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -2.905686140060425,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.11705312877893448,
                        "neutral": 0.8816056251525879,
                        "support": 0.0013412677217274904
                      },
                      "stance_score": -0.11571186105720699,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -2.0026636123657227
                    },
                    {
                      "id": 6125,
                      "faiss_score": 0.8606041669845581,
                      "faiss_rank": 18,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 1,
                      "sentence": "Over the past decade, scaling has emerged as one of the most reliable drivers of progress in machine learning systems, particularly in deep learning.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -4.260554313659668,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.22337916493415833,
                        "neutral": 0.7750259637832642,
                        "support": 0.0015949285589158535
                      },
                      "stance_score": -0.22178423637524247,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -3.39995014667511
                    }
                  ],
                  "neutral": [
                    {
                      "id": 2944,
                      "faiss_score": 0.8668415546417236,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Self-supervised_learning",
                      "file_type": ".txt",
                      "position": 53,
                      "sentence": "Its ability to leverage unlabeled data effectively opens new possibilities for advancement in machine learning, especially in data-driven application domains.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Self-supervised_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -2.1371984481811523,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.02333276905119419,
                        "neutral": 0.9753410816192627,
                        "support": 0.0013262011343613267
                      },
                      "stance_score": -0.022006567916832864,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.2703568935394287
                    },
                    {
                      "id": 349,
                      "faiss_score": 0.8643662333488464,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 240,
                      "sentence": "Typically, machine learning models require a high quantity of reliable data to perform accurate predictions.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -2.83878231048584,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.01353005226701498,
                        "neutral": 0.9858565330505371,
                        "support": 0.0006134053110145032
                      },
                      "stance_score": -0.012916646956000477,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.9744160771369934
                    },
                    {
                      "id": 6126,
                      "faiss_score": 0.872998833656311,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "Rather than relying on narrowly optimized architectures or handcrafted features, many modern systems achieve strong performance by training large models on vast amounts of data using substantial compute.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -3.1559386253356934,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.0634174570441246,
                        "neutral": 0.935447633266449,
                        "support": 0.0011348612606525421
                      },
                      "stance_score": -0.06228259578347206,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -2.2829397916793823
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "A single algorithm can optimally solve all machine learning problems.",
      "expected_verdict": "INCONCLUSIVE",
      "predicted_verdict": "INCONCLUSIVE",
      "subclaims": [
        {
          "subclaim": "A single algorithm can optimally solve all machine learning problems.",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [
              {
                "id": 267,
                "faiss_score": 0.8847181797027588,
                "faiss_rank": 9,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 158,
                "sentence": "This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -0.3544308841228485,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.04583118110895157,
                  "neutral": 0.5406124591827393,
                  "support": 0.41355639696121216
                },
                "stance_score": 0.3677252158522606,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.5302872955799103
              }
            ],
            "contradicting": [
              {
                "id": 1558,
                "faiss_score": 0.8794730305671692,
                "faiss_rank": 14,
                "doc_id": "wiki_Optimization_(mathematics)",
                "file_type": ".txt",
                "position": 7,
                "sentence": "In machine learning, it is always necessary to continuously evaluate the quality of a data model by using a cost function where a minimum implies a set of possibly optimal parameters with an optimal (lowest) error.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                "primary_category": "all articles with style issues",
                "rerank_score": -0.5462067127227783,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.8922678232192993,
                  "neutral": 0.10589844733476639,
                  "support": 0.0018336758948862553
                },
                "stance_score": -0.8904341473244131,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.33326631784439087
              },
              {
                "id": 5840,
                "faiss_score": 0.8876022100448608,
                "faiss_rank": 4,
                "doc_id": "local_math_optimization_and_convergence.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "Gradient-based methods are the most widely used optimization techniques in machine learning.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                "primary_category": null,
                "rerank_score": -0.7626746296882629,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.8216201663017273,
                  "neutral": 0.17583926022052765,
                  "support": 0.0025405713822692633
                },
                "stance_score": -0.819079594919458,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.1249275803565979
              },
              {
                "id": 338,
                "faiss_score": 0.8796346187591553,
                "faiss_rank": 12,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 229,
                "sentence": "In machine learning, genetic algorithms were used in the 1980s and 1990s.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -1.477332592010498,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.12754929065704346,
                  "neutral": 0.8693177700042725,
                  "support": 0.0031329584307968616
                },
                "stance_score": -0.1244163322262466,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.5976979732513428
              },
              {
                "id": 2303,
                "faiss_score": 0.8766059875488281,
                "faiss_rank": 17,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 234,
                "sentence": "Learning algorithm: Numerous trade-offs exist between learning algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": -2.7333645820617676,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.9910756349563599,
                  "neutral": 0.008461421355605125,
                  "support": 0.00046292017214000225
                },
                "stance_score": -0.9906127147842199,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.8567585945129395
              }
            ],
            "neutral": [
              {
                "id": 331,
                "faiss_score": 0.8863186836242676,
                "faiss_rank": 5,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 222,
                "sentence": "Efficient algorithms exist that perform inference and learning.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -0.2585304081439972,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.029771577566862106,
                  "neutral": 0.967642605304718,
                  "support": 0.0025857884902507067
                },
                "stance_score": -0.0271857890766114,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.6277882754802704
              },
              {
                "id": 5831,
                "faiss_score": 0.8993061780929565,
                "faiss_rank": 1,
                "doc_id": "local_math_optimization_and_convergence.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Optimization lies at the core of modern machine learning and computational systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                "primary_category": null,
                "rerank_score": -1.4539523124694824,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.07188953459262848,
                  "neutral": 0.9265451431274414,
                  "support": 0.001565277692861855
                },
                "stance_score": -0.07032425689976662,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.5546461343765259
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "There is not enough strong evidence to reach a clear conclusion.",
        "sections": [
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "A single algorithm can optimally solve all machine learning problems.",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 267,
                      "faiss_score": 0.8847181797027588,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 158,
                      "sentence": "This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -0.3544308841228485,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.04583118110895157,
                        "neutral": 0.5406124591827393,
                        "support": 0.41355639696121216
                      },
                      "stance_score": 0.3677252158522606,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.5302872955799103
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 1558,
                      "faiss_score": 0.8794730305671692,
                      "faiss_rank": 14,
                      "doc_id": "wiki_Optimization_(mathematics)",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "In machine learning, it is always necessary to continuously evaluate the quality of a data model by using a cost function where a minimum implies a set of possibly optimal parameters with an optimal (lowest) error.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
                      "primary_category": "all articles with style issues",
                      "rerank_score": -0.5462067127227783,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.8922678232192993,
                        "neutral": 0.10589844733476639,
                        "support": 0.0018336758948862553
                      },
                      "stance_score": -0.8904341473244131,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.33326631784439087
                    },
                    {
                      "id": 5840,
                      "faiss_score": 0.8876022100448608,
                      "faiss_rank": 4,
                      "doc_id": "local_math_optimization_and_convergence.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "Gradient-based methods are the most widely used optimization techniques in machine learning.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                      "primary_category": null,
                      "rerank_score": -0.7626746296882629,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.8216201663017273,
                        "neutral": 0.17583926022052765,
                        "support": 0.0025405713822692633
                      },
                      "stance_score": -0.819079594919458,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.1249275803565979
                    },
                    {
                      "id": 338,
                      "faiss_score": 0.8796346187591553,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 229,
                      "sentence": "In machine learning, genetic algorithms were used in the 1980s and 1990s.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -1.477332592010498,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.12754929065704346,
                        "neutral": 0.8693177700042725,
                        "support": 0.0031329584307968616
                      },
                      "stance_score": -0.1244163322262466,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.5976979732513428
                    }
                  ],
                  "neutral": [
                    {
                      "id": 331,
                      "faiss_score": 0.8863186836242676,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 222,
                      "sentence": "Efficient algorithms exist that perform inference and learning.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -0.2585304081439972,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.029771577566862106,
                        "neutral": 0.967642605304718,
                        "support": 0.0025857884902507067
                      },
                      "stance_score": -0.0271857890766114,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.6277882754802704
                    },
                    {
                      "id": 5831,
                      "faiss_score": 0.8993061780929565,
                      "faiss_rank": 1,
                      "doc_id": "local_math_optimization_and_convergence.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Optimization lies at the core of modern machine learning and computational systems.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                      "primary_category": null,
                      "rerank_score": -1.4539523124694824,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.07188953459262848,
                        "neutral": 0.9265451431274414,
                        "support": 0.001565277692861855
                      },
                      "stance_score": -0.07032425689976662,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.5546461343765259
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Quantum computers will replace classical computers for most workloads.",
      "expected_verdict": "INCONCLUSIVE",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Quantum computers will replace classical computers for most workloads.",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 16.151414573391225,
            "total": 16.151414573391225
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 6574,
                "faiss_score": 0.9328113794326782,
                "faiss_rank": 3,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 34,
                "sentence": "Rather than replacing classical systems, quantum computers are expected to act as accelerators for specific subroutines.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 5.315627098083496,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.9586077928543091,
                  "neutral": 0.03871520608663559,
                  "support": 0.002676980337128043
                },
                "stance_score": -0.955930812517181,
                "evidence_contribution": -5.081371730909302,
                "combined_rank_score": 6.248438477516174
              },
              {
                "id": 736,
                "faiss_score": 0.9381494522094727,
                "faiss_rank": 1,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 140,
                "sentence": "As of 2023, classical computers outperform quantum computers for all real-world applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.903462886810303,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.9913483262062073,
                  "neutral": 0.006687983404844999,
                  "support": 0.0019637111108750105
                },
                "stance_score": -0.9893846150953323,
                "evidence_contribution": -4.851410740901058,
                "combined_rank_score": 5.841612339019775
              },
              {
                "id": 6594,
                "faiss_score": 0.9081912636756897,
                "faiss_rank": 11,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 54,
                "sentence": "Ultimately, quantum computing represents a long-term research effort rather than a near-term replacement for classical computation.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 3.555327892303467,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.990568220615387,
                  "neutral": 0.007592997048050165,
                  "support": 0.001838862313888967
                },
                "stance_score": -0.988729358301498,
                "evidence_contribution": -3.515257065508624,
                "combined_rank_score": 4.4635191559791565
              },
              {
                "id": 820,
                "faiss_score": 0.9049538373947144,
                "faiss_rank": 15,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 224,
                "sentence": "In other words, quantum computers provide no additional power over classical computers in terms of computability.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 3.0737595558166504,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.5743324160575867,
                  "neutral": 0.4228309094905853,
                  "support": 0.002836654195562005
                },
                "stance_score": -0.5714957618620247,
                "evidence_contribution": -1.7566405591321153,
                "combined_rank_score": 3.9787133932113647
              },
              {
                "id": 822,
                "faiss_score": 0.902823269367218,
                "faiss_rank": 19,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 226,
                "sentence": "While quantum computers cannot solve any problems that classical computers cannot already solve, it is suspected that they can solve certain problems faster than classical computers.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 2.7105321884155273,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.35174041986465454,
                  "neutral": 0.6457991003990173,
                  "support": 0.002460495801642537
                },
                "stance_score": -0.349279924063012,
                "evidence_contribution": -0.9467344769401251,
                "combined_rank_score": 3.6133554577827454
              }
            ],
            "neutral": [
              {
                "id": 4932,
                "faiss_score": 0.9101486206054688,
                "faiss_rank": 9,
                "doc_id": "wiki_Computational_complexity",
                "file_type": ".txt",
                "position": 62,
                "sentence": "However, some problems may theoretically be solved with a much lower time complexity using a quantum computer rather than a classical computer.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Computational_complexity",
                "primary_category": "all articles containing potentially dated statements",
                "rerank_score": 2.1743216514587402,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.09721063077449799,
                  "neutral": 0.9004706144332886,
                  "support": 0.0023187061306089163
                },
                "stance_score": -0.09489192464388907,
                "evidence_contribution": -0.2063255663017992,
                "combined_rank_score": 3.084470272064209
              },
              {
                "id": 6518,
                "faiss_score": 0.9088791608810425,
                "faiss_rank": 10,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 47,
                "sentence": "Quantum computing represents a fundamentally different approach, exploiting quantum mechanical phenomena to perform certain computations more efficiently than classical machines.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": 1.8257582187652588,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.002322460524737835,
                  "neutral": 0.9969102740287781,
                  "support": 0.0007673114887438715
                },
                "stance_score": -0.0015551490359939635,
                "evidence_contribution": -0.002839326133870848,
                "combined_rank_score": 2.7346373796463013
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Quantum computers will replace classical computers for most workloads.",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 6574,
                      "faiss_score": 0.9328113794326782,
                      "faiss_rank": 3,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 34,
                      "sentence": "Rather than replacing classical systems, quantum computers are expected to act as accelerators for specific subroutines.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 5.315627098083496,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.9586077928543091,
                        "neutral": 0.03871520608663559,
                        "support": 0.002676980337128043
                      },
                      "stance_score": -0.955930812517181,
                      "evidence_contribution": -5.081371730909302,
                      "combined_rank_score": 6.248438477516174
                    },
                    {
                      "id": 736,
                      "faiss_score": 0.9381494522094727,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 140,
                      "sentence": "As of 2023, classical computers outperform quantum computers for all real-world applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.903462886810303,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.9913483262062073,
                        "neutral": 0.006687983404844999,
                        "support": 0.0019637111108750105
                      },
                      "stance_score": -0.9893846150953323,
                      "evidence_contribution": -4.851410740901058,
                      "combined_rank_score": 5.841612339019775
                    },
                    {
                      "id": 6594,
                      "faiss_score": 0.9081912636756897,
                      "faiss_rank": 11,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 54,
                      "sentence": "Ultimately, quantum computing represents a long-term research effort rather than a near-term replacement for classical computation.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 3.555327892303467,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.990568220615387,
                        "neutral": 0.007592997048050165,
                        "support": 0.001838862313888967
                      },
                      "stance_score": -0.988729358301498,
                      "evidence_contribution": -3.515257065508624,
                      "combined_rank_score": 4.4635191559791565
                    }
                  ],
                  "neutral": [
                    {
                      "id": 4932,
                      "faiss_score": 0.9101486206054688,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Computational_complexity",
                      "file_type": ".txt",
                      "position": 62,
                      "sentence": "However, some problems may theoretically be solved with a much lower time complexity using a quantum computer rather than a classical computer.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Computational_complexity",
                      "primary_category": "all articles containing potentially dated statements",
                      "rerank_score": 2.1743216514587402,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.09721063077449799,
                        "neutral": 0.9004706144332886,
                        "support": 0.0023187061306089163
                      },
                      "stance_score": -0.09489192464388907,
                      "evidence_contribution": -0.2063255663017992,
                      "combined_rank_score": 3.084470272064209
                    },
                    {
                      "id": 6518,
                      "faiss_score": 0.9088791608810425,
                      "faiss_rank": 10,
                      "doc_id": "local_physics_computation.txt",
                      "file_type": ".txt",
                      "position": 47,
                      "sentence": "Quantum computing represents a fundamentally different approach, exploiting quantum mechanical phenomena to perform certain computations more efficiently than classical machines.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                      "primary_category": null,
                      "rerank_score": 1.8257582187652588,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.002322460524737835,
                        "neutral": 0.9969102740287781,
                        "support": 0.0007673114887438715
                      },
                      "stance_score": -0.0015551490359939635,
                      "evidence_contribution": -0.002839326133870848,
                      "combined_rank_score": 2.7346373796463013
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Increasing data quality is more important than model size for all tasks.",
      "expected_verdict": "INCONCLUSIVE",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Increasing data quality is more important than model size for all tasks.",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 4.907310577250371,
            "total": 4.907310577250371
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 6137,
                "faiss_score": 0.9282467365264893,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 6.42487907409668,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.7659299969673157,
                  "neutral": 0.23193812370300293,
                  "support": 0.0021318739745765924
                },
                "stance_score": -0.7637981229927391,
                "evidence_contribution": -4.907310577250371,
                "combined_rank_score": 7.353125810623169
              },
              {
                "id": 6132,
                "faiss_score": 0.8877804279327393,
                "faiss_rank": 14,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 8,
                "sentence": "Larger models have greater representational capacity, allowing them to fit more complex functions.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -3.486907720565796,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.31466788053512573,
                  "neutral": 0.6821066737174988,
                  "support": 0.0032254562247544527
                },
                "stance_score": -0.3114424243103713,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.5991272926330566
              },
              {
                "id": 6135,
                "faiss_score": 0.912760317325592,
                "faiss_rank": 3,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 11,
                "sentence": "Data scaling plays an equally important role.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -4.107235908508301,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.5072497725486755,
                  "neutral": 0.4902275800704956,
                  "support": 0.0025226445868611336
                },
                "stance_score": -0.5047271279618144,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.1944755911827087
              },
              {
                "id": 5914,
                "faiss_score": 0.8886445760726929,
                "faiss_rank": 11,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "By identifying and removing such parameters, models can be made smaller and faster.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -4.602010726928711,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.10239699482917786,
                  "neutral": 0.8955297470092773,
                  "support": 0.002073230454698205
                },
                "stance_score": -0.10032376437447965,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.713366150856018
              }
            ],
            "neutral": [
              {
                "id": 6147,
                "faiss_score": 0.8930780291557312,
                "faiss_rank": 7,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 23,
                "sentence": "For example, increasing model size without increasing data may yield limited benefits, while increasing data without sufficient model capacity may fail to exploit the additional information.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.379410743713379,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.028468823060393333,
                  "neutral": 0.9641625881195068,
                  "support": 0.007368524093180895
                },
                "stance_score": -0.02110029896721244,
                "evidence_contribution": -0.09240687599257447,
                "combined_rank_score": 5.27248877286911
              },
              {
                "id": 5767,
                "faiss_score": 0.9157058596611023,
                "faiss_rank": 2,
                "doc_id": "local_math_information_theory_and_learning.txt",
                "file_type": ".txt",
                "position": 15,
                "sentence": "This observation emphasizes the importance of data quality and task definition.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_information_theory_and_learning.txt",
                "primary_category": null,
                "rerank_score": -0.11810709536075592,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0025519374758005142,
                  "neutral": 0.9963298439979553,
                  "support": 0.00111820874735713
                },
                "stance_score": -0.0014337287284433842,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.7975987643003464
              },
              {
                "id": 6136,
                "faiss_score": 0.8884807825088501,
                "faiss_rank": 13,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 12,
                "sentence": "Training large models on insufficient or low-quality data can lead to overfitting or wasted capacity.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -3.0387094020843506,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.012909447774291039,
                  "neutral": 0.9826837182044983,
                  "support": 0.004406800959259272
                },
                "stance_score": -0.008502646815031767,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.1502286195755005
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Increasing data quality is more important than model size for all tasks.",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 6137,
                      "faiss_score": 0.9282467365264893,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 6.42487907409668,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.7659299969673157,
                        "neutral": 0.23193812370300293,
                        "support": 0.0021318739745765924
                      },
                      "stance_score": -0.7637981229927391,
                      "evidence_contribution": -4.907310577250371,
                      "combined_rank_score": 7.353125810623169
                    },
                    {
                      "id": 6132,
                      "faiss_score": 0.8877804279327393,
                      "faiss_rank": 14,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 8,
                      "sentence": "Larger models have greater representational capacity, allowing them to fit more complex functions.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -3.486907720565796,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.31466788053512573,
                        "neutral": 0.6821066737174988,
                        "support": 0.0032254562247544527
                      },
                      "stance_score": -0.3114424243103713,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -2.5991272926330566
                    },
                    {
                      "id": 6135,
                      "faiss_score": 0.912760317325592,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 11,
                      "sentence": "Data scaling plays an equally important role.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -4.107235908508301,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.5072497725486755,
                        "neutral": 0.4902275800704956,
                        "support": 0.0025226445868611336
                      },
                      "stance_score": -0.5047271279618144,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -3.1944755911827087
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6147,
                      "faiss_score": 0.8930780291557312,
                      "faiss_rank": 7,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 23,
                      "sentence": "For example, increasing model size without increasing data may yield limited benefits, while increasing data without sufficient model capacity may fail to exploit the additional information.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.379410743713379,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.028468823060393333,
                        "neutral": 0.9641625881195068,
                        "support": 0.007368524093180895
                      },
                      "stance_score": -0.02110029896721244,
                      "evidence_contribution": -0.09240687599257447,
                      "combined_rank_score": 5.27248877286911
                    },
                    {
                      "id": 5767,
                      "faiss_score": 0.9157058596611023,
                      "faiss_rank": 2,
                      "doc_id": "local_math_information_theory_and_learning.txt",
                      "file_type": ".txt",
                      "position": 15,
                      "sentence": "This observation emphasizes the importance of data quality and task definition.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_information_theory_and_learning.txt",
                      "primary_category": null,
                      "rerank_score": -0.11810709536075592,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0025519374758005142,
                        "neutral": 0.9963298439979553,
                        "support": 0.00111820874735713
                      },
                      "stance_score": -0.0014337287284433842,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.7975987643003464
                    },
                    {
                      "id": 6136,
                      "faiss_score": 0.8884807825088501,
                      "faiss_rank": 13,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 12,
                      "sentence": "Training large models on insufficient or low-quality data can lead to overfitting or wasted capacity.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -3.0387094020843506,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.012909447774291039,
                        "neutral": 0.9826837182044983,
                        "support": 0.004406800959259272
                      },
                      "stance_score": -0.008502646815031767,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -2.1502286195755005
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Scaling large language models improves performance, enables emergent abilities, and supports zero-shot learning, but increases training cost, energy consumption, optimization instability, and diminishing returns.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Scaling large language models improves performance",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 3.180714039362715,
            "total": 3.180714039362715
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 2020,
                "faiss_score": 0.9002507925033569,
                "faiss_rank": 7,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 4.025500774383545,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.7573868036270142,
                  "neutral": 0.23684409260749817,
                  "support": 0.005769144278019667
                },
                "stance_score": -0.7516176593489945,
                "evidence_contribution": -3.025637469749725,
                "combined_rank_score": 4.925751566886902
              },
              {
                "id": 6102,
                "faiss_score": 0.8919915556907654,
                "faiss_rank": 9,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 62,
                "sentence": "Evaluation of large language models presents its own challenges.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.2447456121444702,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.12720470130443573,
                  "neutral": 0.8701755404472351,
                  "support": 0.0026197514962404966
                },
                "stance_score": -0.12458494980819523,
                "evidence_contribution": -0.15507656961299007,
                "combined_rank_score": 2.1367371678352356
              }
            ],
            "neutral": [
              {
                "id": 6124,
                "faiss_score": 0.877862274646759,
                "faiss_rank": 16,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 5.232515811920166,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0007320376462303102,
                  "neutral": 0.9974038004875183,
                  "support": 0.0018641429487615824
                },
                "stance_score": 0.0011321053025312722,
                "evidence_contribution": 0.005923758896253545,
                "combined_rank_score": 6.110378086566925
              },
              {
                "id": 6043,
                "faiss_score": 0.9281105995178223,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 5.195529460906982,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0006025525508448482,
                  "neutral": 0.9942904710769653,
                  "support": 0.005106922704726458
                },
                "stance_score": 0.0045043701538816094,
                "evidence_contribution": 0.02340258783732202,
                "combined_rank_score": 6.123640060424805
              },
              {
                "id": 6121,
                "faiss_score": 0.9091229438781738,
                "faiss_rank": 2,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 2.8631067276000977,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.01936933770775795,
                  "neutral": 0.9794360399246216,
                  "support": 0.0011946404119953513
                },
                "stance_score": -0.0181746972957626,
                "evidence_contribution": -0.0520360980995932,
                "combined_rank_score": 3.7722296714782715
              }
            ]
          }
        },
        {
          "subclaim": "Scaling large language models enables emergent abilities",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.8849807372113477,
            "total": 0.8849807372113477
          },
          "evidence": {
            "supporting": [
              {
                "id": 6109,
                "faiss_score": 0.8844212293624878,
                "faiss_rank": 9,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 69,
                "sentence": "The rapid pace of development in large language models has reshaped expectations about what machine learning systems can do.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -1.8457326889038086,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.001335775014013052,
                  "neutral": 0.8548458814620972,
                  "support": 0.14381834864616394
                },
                "stance_score": 0.1424825736321509,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.9613114595413208
              }
            ],
            "contradicting": [
              {
                "id": 2020,
                "faiss_score": 0.8869941234588623,
                "faiss_rank": 7,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 1.3232548236846924,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.6738033890724182,
                  "neutral": 0.3211841583251953,
                  "support": 0.005012524779886007
                },
                "stance_score": -0.6687908642925322,
                "evidence_contribution": -0.8849807372113477,
                "combined_rank_score": 2.2102489471435547
              }
            ],
            "neutral": [
              {
                "id": 6043,
                "faiss_score": 0.9116201400756836,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.754950761795044,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0006786141893826425,
                  "neutral": 0.9966127276420593,
                  "support": 0.002708665793761611
                },
                "stance_score": 0.0020300516043789685,
                "evidence_contribution": 0.003562640609588122,
                "combined_rank_score": 2.6665709018707275
              },
              {
                "id": 1940,
                "faiss_score": 0.8720008134841919,
                "faiss_rank": 18,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 208,
                "sentence": "argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 0.9061471223831177,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.005984219256788492,
                  "neutral": 0.9875144958496094,
                  "support": 0.0065012965351343155
                },
                "stance_score": 0.0005170772783458233,
                "evidence_contribution": 0.00046854808782276214,
                "combined_rank_score": 1.7781479358673096
              },
              {
                "id": 6047,
                "faiss_score": 0.8847442865371704,
                "faiss_rank": 8,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Despite these capabilities, the behavior of large language models remains fundamentally probabilistic.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 0.7278874516487122,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0569276362657547,
                  "neutral": 0.9358620643615723,
                  "support": 0.007210308685898781
                },
                "stance_score": -0.04971732757985592,
                "evidence_contribution": -0.03618861887488556,
                "combined_rank_score": 1.6126317381858826
              }
            ]
          }
        },
        {
          "subclaim": "Scaling large language models supports zero-shot learning",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.6956883101165249,
            "total": 0.6956883101165249
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 2020,
                "faiss_score": 0.8652899861335754,
                "faiss_rank": 5,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 0.850409209728241,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.8215959668159485,
                  "neutral": 0.17487108707427979,
                  "support": 0.003532965900376439
                },
                "stance_score": -0.818063000915572,
                "evidence_contribution": -0.6956883101165249,
                "combined_rank_score": 1.7156991958618164
              },
              {
                "id": 6121,
                "faiss_score": 0.8645410537719727,
                "faiss_rank": 6,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -0.28437477350234985,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.5763959884643555,
                  "neutral": 0.4200174808502197,
                  "support": 0.003586551873013377
                },
                "stance_score": -0.5728094365913421,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.5801662802696228
              },
              {
                "id": 6047,
                "faiss_score": 0.8596764802932739,
                "faiss_rank": 8,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Despite these capabilities, the behavior of large language models remains fundamentally probabilistic.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -1.6459540128707886,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.5432117581367493,
                  "neutral": 0.45265936851501465,
                  "support": 0.004128914326429367
                },
                "stance_score": -0.5390828438103199,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.7862775325775146
              },
              {
                "id": 6067,
                "faiss_score": 0.8749508857727051,
                "faiss_rank": 3,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 27,
                "sentence": "Large language models are also sensitive to the distribution of their training data.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -1.6497626304626465,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.22105905413627625,
                  "neutral": 0.7753354907035828,
                  "support": 0.0036054591182619333
                },
                "stance_score": -0.2174535950180143,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.7748117446899414
              },
              {
                "id": 6102,
                "faiss_score": 0.855239748954773,
                "faiss_rank": 10,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 62,
                "sentence": "Evaluation of large language models presents its own challenges.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -1.9633920192718506,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.7986255884170532,
                  "neutral": 0.19749756157398224,
                  "support": 0.003876908216625452
                },
                "stance_score": -0.7947486802004278,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.1081522703170776
              }
            ],
            "neutral": [
              {
                "id": 6043,
                "faiss_score": 0.8965744376182556,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.581221580505371,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0036855970975011587,
                  "neutral": 0.9953210949897766,
                  "support": 0.0009933628607541323
                },
                "stance_score": -0.0026922342367470264,
                "evidence_contribution": -0.004257018874919805,
                "combined_rank_score": 2.4777960181236267
              },
              {
                "id": 6040,
                "faiss_score": 0.8675651550292969,
                "faiss_rank": 4,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 0.335788369178772,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.03993375971913338,
                  "neutral": 0.9570171236991882,
                  "support": 0.003049140563234687
                },
                "stance_score": -0.03688461915589869,
                "evidence_contribution": -0.012385426114139314,
                "combined_rank_score": 1.2033535242080688
              },
              {
                "id": 6109,
                "faiss_score": 0.8546082973480225,
                "faiss_rank": 11,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 69,
                "sentence": "The rapid pace of development in large language models has reshaped expectations about what machine learning systems can do.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -0.7598472237586975,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0232249666005373,
                  "neutral": 0.9732229709625244,
                  "support": 0.0035520456731319427
                },
                "stance_score": -0.019672920927405357,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.09476107358932495
              }
            ]
          }
        },
        {
          "subclaim": "Scaling large language models increases training cost",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 1.806630821776129,
            "contradict": 0.0,
            "total": 1.806630821776129
          },
          "evidence": {
            "supporting": [
              {
                "id": 6092,
                "faiss_score": 0.9561026096343994,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 52,
                "sentence": "The computational cost of training large language models is substantial.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 4.304480075836182,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0017482315888628364,
                  "neutral": 0.5767940878868103,
                  "support": 0.4214576482772827
                },
                "stance_score": 0.4197094166884199,
                "evidence_contribution": 1.806630821776129,
                "combined_rank_score": 5.260582685470581
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 1789,
                "faiss_score": 0.8806159496307373,
                "faiss_rank": 13,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 57,
                "sentence": "Training of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 1.4576606750488281,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0032061070669442415,
                  "neutral": 0.9088557958602905,
                  "support": 0.08793813735246658
                },
                "stance_score": 0.08473203028552234,
                "evidence_contribution": 0.12351054846425225,
                "combined_rank_score": 2.3382766246795654
              },
              {
                "id": 6124,
                "faiss_score": 0.8770360350608826,
                "faiss_rank": 17,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 1.055026650428772,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.02897127903997898,
                  "neutral": 0.9692744612693787,
                  "support": 0.0017542814603075385
                },
                "stance_score": -0.027216997579671443,
                "evidence_contribution": -0.028714657791208756,
                "combined_rank_score": 1.9320626854896545
              },
              {
                "id": 2020,
                "faiss_score": 0.8799142837524414,
                "faiss_rank": 14,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 0.9285494089126587,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.001222233404405415,
                  "neutral": 0.9977548718452454,
                  "support": 0.0010229643667116761
                },
                "stance_score": -0.00019926903769373894,
                "evidence_contribution": -0.0001850311471651156,
                "combined_rank_score": 1.8084636926651
              }
            ]
          }
        },
        {
          "subclaim": "Scaling large language models increases energy consumption",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [],
            "neutral": [
              {
                "id": 6043,
                "faiss_score": 0.8894945979118347,
                "faiss_rank": 4,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.1702418327331543,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.002962291007861495,
                  "neutral": 0.9959206581115723,
                  "support": 0.0011170216603204608
                },
                "stance_score": -0.0018452693475410342,
                "evidence_contribution": -0.0021594113831527317,
                "combined_rank_score": 2.059736430644989
              },
              {
                "id": 5907,
                "faiss_score": 0.8961602449417114,
                "faiss_rank": 3,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 2,
                "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 0.8276621699333191,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.003515135031193495,
                  "neutral": 0.9713563323020935,
                  "support": 0.025128565728664398
                },
                "stance_score": 0.021613430697470903,
                "evidence_contribution": 0.017888618950772178,
                "combined_rank_score": 1.7238224148750305
              },
              {
                "id": 2020,
                "faiss_score": 0.8737888336181641,
                "faiss_rank": 13,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 0.13212397694587708,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0013766626361757517,
                  "neutral": 0.9976444840431213,
                  "support": 0.0009788443567231297
                },
                "stance_score": -0.00039781827945262194,
                "evidence_contribution": -5.2561333183046705e-05,
                "combined_rank_score": 1.0059128105640411
              }
            ]
          }
        },
        {
          "subclaim": "Scaling large language models causes optimization instability",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [
              {
                "id": 6060,
                "faiss_score": 0.8820145130157471,
                "faiss_rank": 11,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 20,
                "sentence": "Hallucination is one of the most widely discussed failure modes of large language models.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -2.862800359725952,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.002278294414281845,
                  "neutral": 0.8935396075248718,
                  "support": 0.10418205708265305
                },
                "stance_score": 0.1019037626683712,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.980785846710205
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 2020,
                "faiss_score": 0.9003187417984009,
                "faiss_rank": 3,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 0.8239505887031555,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0010097158374264836,
                  "neutral": 0.9976261258125305,
                  "support": 0.001364142750389874
                },
                "stance_score": 0.00035442691296339035,
                "evidence_contribution": 0.00029203026358842754,
                "combined_rank_score": 1.7242693305015564
              },
              {
                "id": 6043,
                "faiss_score": 0.9122200608253479,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -0.08233976364135742,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.009328887797892094,
                  "neutral": 0.988326370716095,
                  "support": 0.0023446741979569197
                },
                "stance_score": -0.006984213599935174,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.8298802971839905
              },
              {
                "id": 2037,
                "faiss_score": 0.8899509906768799,
                "faiss_rank": 8,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 305,
                "sentence": "This phenomenon undermines the reliability of large language models in multiple-choice settings.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -1.1177924871444702,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.004243152216076851,
                  "neutral": 0.9134510159492493,
                  "support": 0.08230578899383545
                },
                "stance_score": 0.0780626367777586,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.22784149646759033
              }
            ]
          }
        },
        {
          "subclaim": "Scaling large language models exhibits diminishing returns",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 1.4440078748814424,
            "contradict": 0.0,
            "total": 1.4440078748814424
          },
          "evidence": {
            "supporting": [
              {
                "id": 2020,
                "faiss_score": 0.9000077843666077,
                "faiss_rank": 6,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 3.5399813652038574,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.00867969449609518,
                  "neutral": 0.5747266411781311,
                  "support": 0.41659367084503174
                },
                "stance_score": 0.40791397634893656,
                "evidence_contribution": 1.4440078748814424,
                "combined_rank_score": 4.439989149570465
              },
              {
                "id": 6072,
                "faiss_score": 0.8800252676010132,
                "faiss_rank": 17,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 32,
                "sentence": "Another limitation of large language models is their lack of persistent memory beyond the context window.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -1.1976983547210693,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.003200442995876074,
                  "neutral": 0.876997172832489,
                  "support": 0.11980230361223221
                },
                "stance_score": 0.11660186061635613,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.31767308712005615
              },
              {
                "id": 2037,
                "faiss_score": 0.8823869824409485,
                "faiss_rank": 16,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 305,
                "sentence": "This phenomenon undermines the reliability of large language models in multiple-choice settings.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -1.770148515701294,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.006240415386855602,
                  "neutral": 0.7071327567100525,
                  "support": 0.28662681579589844
                },
                "stance_score": 0.28038640040904284,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.8877615332603455
              }
            ],
            "contradicting": [
              {
                "id": 1796,
                "faiss_score": 0.8970776200294495,
                "faiss_rank": 7,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 64,
                "sentence": "The tendency towards larger models is visible in the list of large language models.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -1.3834636211395264,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.11149914562702179,
                  "neutral": 0.8778157830238342,
                  "support": 0.010685019195079803
                },
                "stance_score": -0.10081412643194199,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.4863860011100769
              }
            ],
            "neutral": [
              {
                "id": 6043,
                "faiss_score": 0.924456000328064,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.306876540184021,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.08674194663763046,
                  "neutral": 0.9057531952857971,
                  "support": 0.007504891604185104
                },
                "stance_score": -0.07923705503344536,
                "evidence_contribution": -0.10355304833647994,
                "combined_rank_score": 2.231332540512085
              },
              {
                "id": 6121,
                "faiss_score": 0.9107099771499634,
                "faiss_rank": 3,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -0.2476663887500763,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.00297346618026495,
                  "neutral": 0.9608861207962036,
                  "support": 0.036140382289886475
                },
                "stance_score": 0.033166916109621525,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.6630435883998871
              },
              {
                "id": 6047,
                "faiss_score": 0.8917949199676514,
                "faiss_rank": 10,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Despite these capabilities, the behavior of large language models remains fundamentally probabilistic.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -1.3248001337051392,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.009706818498671055,
                  "neutral": 0.9616049528121948,
                  "support": 0.028688186779618263
                },
                "stance_score": 0.01898136828094721,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.4330052137374878
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Scaling large language models increases training cost",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6092,
                      "faiss_score": 0.9561026096343994,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 52,
                      "sentence": "The computational cost of training large language models is substantial.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 4.304480075836182,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0017482315888628364,
                        "neutral": 0.5767940878868103,
                        "support": 0.4214576482772827
                      },
                      "stance_score": 0.4197094166884199,
                      "evidence_contribution": 1.806630821776129,
                      "combined_rank_score": 5.260582685470581
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 1789,
                      "faiss_score": 0.8806159496307373,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 57,
                      "sentence": "Training of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 1.4576606750488281,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0032061070669442415,
                        "neutral": 0.9088557958602905,
                        "support": 0.08793813735246658
                      },
                      "stance_score": 0.08473203028552234,
                      "evidence_contribution": 0.12351054846425225,
                      "combined_rank_score": 2.3382766246795654
                    },
                    {
                      "id": 6124,
                      "faiss_score": 0.8770360350608826,
                      "faiss_rank": 17,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 1.055026650428772,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.02897127903997898,
                        "neutral": 0.9692744612693787,
                        "support": 0.0017542814603075385
                      },
                      "stance_score": -0.027216997579671443,
                      "evidence_contribution": -0.028714657791208756,
                      "combined_rank_score": 1.9320626854896545
                    },
                    {
                      "id": 2020,
                      "faiss_score": 0.8799142837524414,
                      "faiss_rank": 14,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 0.9285494089126587,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.001222233404405415,
                        "neutral": 0.9977548718452454,
                        "support": 0.0010229643667116761
                      },
                      "stance_score": -0.00019926903769373894,
                      "evidence_contribution": -0.0001850311471651156,
                      "combined_rank_score": 1.8084636926651
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling large language models exhibits diminishing returns",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 2020,
                      "faiss_score": 0.9000077843666077,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 3.5399813652038574,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.00867969449609518,
                        "neutral": 0.5747266411781311,
                        "support": 0.41659367084503174
                      },
                      "stance_score": 0.40791397634893656,
                      "evidence_contribution": 1.4440078748814424,
                      "combined_rank_score": 4.439989149570465
                    },
                    {
                      "id": 6072,
                      "faiss_score": 0.8800252676010132,
                      "faiss_rank": 17,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 32,
                      "sentence": "Another limitation of large language models is their lack of persistent memory beyond the context window.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -1.1976983547210693,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.003200442995876074,
                        "neutral": 0.876997172832489,
                        "support": 0.11980230361223221
                      },
                      "stance_score": 0.11660186061635613,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.31767308712005615
                    },
                    {
                      "id": 2037,
                      "faiss_score": 0.8823869824409485,
                      "faiss_rank": 16,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 305,
                      "sentence": "This phenomenon undermines the reliability of large language models in multiple-choice settings.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -1.770148515701294,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.006240415386855602,
                        "neutral": 0.7071327567100525,
                        "support": 0.28662681579589844
                      },
                      "stance_score": 0.28038640040904284,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.8877615332603455
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 1796,
                      "faiss_score": 0.8970776200294495,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 64,
                      "sentence": "The tendency towards larger models is visible in the list of large language models.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -1.3834636211395264,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.11149914562702179,
                        "neutral": 0.8778157830238342,
                        "support": 0.010685019195079803
                      },
                      "stance_score": -0.10081412643194199,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.4863860011100769
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6043,
                      "faiss_score": 0.924456000328064,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.306876540184021,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.08674194663763046,
                        "neutral": 0.9057531952857971,
                        "support": 0.007504891604185104
                      },
                      "stance_score": -0.07923705503344536,
                      "evidence_contribution": -0.10355304833647994,
                      "combined_rank_score": 2.231332540512085
                    },
                    {
                      "id": 6121,
                      "faiss_score": 0.9107099771499634,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -0.2476663887500763,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.00297346618026495,
                        "neutral": 0.9608861207962036,
                        "support": 0.036140382289886475
                      },
                      "stance_score": 0.033166916109621525,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.6630435883998871
                    },
                    {
                      "id": 6047,
                      "faiss_score": 0.8917949199676514,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "Despite these capabilities, the behavior of large language models remains fundamentally probabilistic.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -1.3248001337051392,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.009706818498671055,
                        "neutral": 0.9616049528121948,
                        "support": 0.028688186779618263
                      },
                      "stance_score": 0.01898136828094721,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.4330052137374878
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Scaling large language models improves performance",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 2020,
                      "faiss_score": 0.9002507925033569,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 4.025500774383545,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.7573868036270142,
                        "neutral": 0.23684409260749817,
                        "support": 0.005769144278019667
                      },
                      "stance_score": -0.7516176593489945,
                      "evidence_contribution": -3.025637469749725,
                      "combined_rank_score": 4.925751566886902
                    },
                    {
                      "id": 6102,
                      "faiss_score": 0.8919915556907654,
                      "faiss_rank": 9,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 62,
                      "sentence": "Evaluation of large language models presents its own challenges.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.2447456121444702,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.12720470130443573,
                        "neutral": 0.8701755404472351,
                        "support": 0.0026197514962404966
                      },
                      "stance_score": -0.12458494980819523,
                      "evidence_contribution": -0.15507656961299007,
                      "combined_rank_score": 2.1367371678352356
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6043,
                      "faiss_score": 0.9281105995178223,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 5.195529460906982,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0006025525508448482,
                        "neutral": 0.9942904710769653,
                        "support": 0.005106922704726458
                      },
                      "stance_score": 0.0045043701538816094,
                      "evidence_contribution": 0.02340258783732202,
                      "combined_rank_score": 6.123640060424805
                    },
                    {
                      "id": 6124,
                      "faiss_score": 0.877862274646759,
                      "faiss_rank": 16,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Scaling in machine learning refers to the practice of increasing model size, dataset size, or computational resources in order to improve performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 5.232515811920166,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0007320376462303102,
                        "neutral": 0.9974038004875183,
                        "support": 0.0018641429487615824
                      },
                      "stance_score": 0.0011321053025312722,
                      "evidence_contribution": 0.005923758896253545,
                      "combined_rank_score": 6.110378086566925
                    },
                    {
                      "id": 6121,
                      "faiss_score": 0.9091229438781738,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 2.8631067276000977,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.01936933770775795,
                        "neutral": 0.9794360399246216,
                        "support": 0.0011946404119953513
                      },
                      "stance_score": -0.0181746972957626,
                      "evidence_contribution": -0.0520360980995932,
                      "combined_rank_score": 3.7722296714782715
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling large language models enables emergent abilities",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6109,
                      "faiss_score": 0.8844212293624878,
                      "faiss_rank": 9,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 69,
                      "sentence": "The rapid pace of development in large language models has reshaped expectations about what machine learning systems can do.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -1.8457326889038086,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.001335775014013052,
                        "neutral": 0.8548458814620972,
                        "support": 0.14381834864616394
                      },
                      "stance_score": 0.1424825736321509,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.9613114595413208
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 2020,
                      "faiss_score": 0.8869941234588623,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 1.3232548236846924,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.6738033890724182,
                        "neutral": 0.3211841583251953,
                        "support": 0.005012524779886007
                      },
                      "stance_score": -0.6687908642925322,
                      "evidence_contribution": -0.8849807372113477,
                      "combined_rank_score": 2.2102489471435547
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6043,
                      "faiss_score": 0.9116201400756836,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.754950761795044,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0006786141893826425,
                        "neutral": 0.9966127276420593,
                        "support": 0.002708665793761611
                      },
                      "stance_score": 0.0020300516043789685,
                      "evidence_contribution": 0.003562640609588122,
                      "combined_rank_score": 2.6665709018707275
                    },
                    {
                      "id": 1940,
                      "faiss_score": 0.8720008134841919,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 208,
                      "sentence": "argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 0.9061471223831177,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.005984219256788492,
                        "neutral": 0.9875144958496094,
                        "support": 0.0065012965351343155
                      },
                      "stance_score": 0.0005170772783458233,
                      "evidence_contribution": 0.00046854808782276214,
                      "combined_rank_score": 1.7781479358673096
                    },
                    {
                      "id": 6047,
                      "faiss_score": 0.8847442865371704,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "Despite these capabilities, the behavior of large language models remains fundamentally probabilistic.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 0.7278874516487122,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.0569276362657547,
                        "neutral": 0.9358620643615723,
                        "support": 0.007210308685898781
                      },
                      "stance_score": -0.04971732757985592,
                      "evidence_contribution": -0.03618861887488556,
                      "combined_rank_score": 1.6126317381858826
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling large language models supports zero-shot learning",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 2020,
                      "faiss_score": 0.8652899861335754,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 0.850409209728241,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.8215959668159485,
                        "neutral": 0.17487108707427979,
                        "support": 0.003532965900376439
                      },
                      "stance_score": -0.818063000915572,
                      "evidence_contribution": -0.6956883101165249,
                      "combined_rank_score": 1.7156991958618164
                    },
                    {
                      "id": 6121,
                      "faiss_score": 0.8645410537719727,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -0.28437477350234985,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.5763959884643555,
                        "neutral": 0.4200174808502197,
                        "support": 0.003586551873013377
                      },
                      "stance_score": -0.5728094365913421,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.5801662802696228
                    },
                    {
                      "id": 6067,
                      "faiss_score": 0.8749508857727051,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 27,
                      "sentence": "Large language models are also sensitive to the distribution of their training data.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -1.6497626304626465,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.22105905413627625,
                        "neutral": 0.7753354907035828,
                        "support": 0.0036054591182619333
                      },
                      "stance_score": -0.2174535950180143,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.7748117446899414
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6043,
                      "faiss_score": 0.8965744376182556,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.581221580505371,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0036855970975011587,
                        "neutral": 0.9953210949897766,
                        "support": 0.0009933628607541323
                      },
                      "stance_score": -0.0026922342367470264,
                      "evidence_contribution": -0.004257018874919805,
                      "combined_rank_score": 2.4777960181236267
                    },
                    {
                      "id": 6040,
                      "faiss_score": 0.8675651550292969,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 0.335788369178772,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.03993375971913338,
                        "neutral": 0.9570171236991882,
                        "support": 0.003049140563234687
                      },
                      "stance_score": -0.03688461915589869,
                      "evidence_contribution": -0.012385426114139314,
                      "combined_rank_score": 1.2033535242080688
                    },
                    {
                      "id": 6109,
                      "faiss_score": 0.8546082973480225,
                      "faiss_rank": 11,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 69,
                      "sentence": "The rapid pace of development in large language models has reshaped expectations about what machine learning systems can do.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -0.7598472237586975,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0232249666005373,
                        "neutral": 0.9732229709625244,
                        "support": 0.0035520456731319427
                      },
                      "stance_score": -0.019672920927405357,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.09476107358932495
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Scaling large language models increases energy consumption",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6043,
                      "faiss_score": 0.8894945979118347,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.1702418327331543,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.002962291007861495,
                        "neutral": 0.9959206581115723,
                        "support": 0.0011170216603204608
                      },
                      "stance_score": -0.0018452693475410342,
                      "evidence_contribution": -0.0021594113831527317,
                      "combined_rank_score": 2.059736430644989
                    },
                    {
                      "id": 5907,
                      "faiss_score": 0.8961602449417114,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 0.8276621699333191,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.003515135031193495,
                        "neutral": 0.9713563323020935,
                        "support": 0.025128565728664398
                      },
                      "stance_score": 0.021613430697470903,
                      "evidence_contribution": 0.017888618950772178,
                      "combined_rank_score": 1.7238224148750305
                    },
                    {
                      "id": 2020,
                      "faiss_score": 0.8737888336181641,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 0.13212397694587708,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0013766626361757517,
                        "neutral": 0.9976444840431213,
                        "support": 0.0009788443567231297
                      },
                      "stance_score": -0.00039781827945262194,
                      "evidence_contribution": -5.2561333183046705e-05,
                      "combined_rank_score": 1.0059128105640411
                    }
                  ]
                }
              },
              {
                "subclaim": "Scaling large language models causes optimization instability",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6060,
                      "faiss_score": 0.8820145130157471,
                      "faiss_rank": 11,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 20,
                      "sentence": "Hallucination is one of the most widely discussed failure modes of large language models.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -2.862800359725952,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.002278294414281845,
                        "neutral": 0.8935396075248718,
                        "support": 0.10418205708265305
                      },
                      "stance_score": 0.1019037626683712,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.980785846710205
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 2020,
                      "faiss_score": 0.9003187417984009,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 288,
                      "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 0.8239505887031555,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0010097158374264836,
                        "neutral": 0.9976261258125305,
                        "support": 0.001364142750389874
                      },
                      "stance_score": 0.00035442691296339035,
                      "evidence_contribution": 0.00029203026358842754,
                      "combined_rank_score": 1.7242693305015564
                    },
                    {
                      "id": 6043,
                      "faiss_score": 0.9122200608253479,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -0.08233976364135742,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.009328887797892094,
                        "neutral": 0.988326370716095,
                        "support": 0.0023446741979569197
                      },
                      "stance_score": -0.006984213599935174,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.8298802971839905
                    },
                    {
                      "id": 2037,
                      "faiss_score": 0.8899509906768799,
                      "faiss_rank": 8,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 305,
                      "sentence": "This phenomenon undermines the reliability of large language models in multiple-choice settings.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -1.1177924871444702,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.004243152216076851,
                        "neutral": 0.9134510159492493,
                        "support": 0.08230578899383545
                      },
                      "stance_score": 0.0780626367777586,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.22784149646759033
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Transformer architectures improve parallelization, capture long-range dependencies, and scale efficiently, but require large compute budgets, are sensitive to hyperparameters, and struggle with very long contexts.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Transformer architectures improve parallelization",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.03443377739166427,
            "contradict": 0.13494455876626898,
            "total": 0.16937833615793324
          },
          "evidence": {
            "supporting": [
              {
                "id": 6284,
                "faiss_score": 0.8840556144714355,
                "faiss_rank": 18,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 23,
                "sentence": "Transformers typically employ multiple attention heads in parallel.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 0.2052372246980667,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.001994922524318099,
                  "neutral": 0.8282346129417419,
                  "support": 0.16977041959762573
                },
                "stance_score": 0.16777549707330763,
                "evidence_contribution": 0.03443377739166427,
                "combined_rank_score": 1.0892928391695023
              },
              {
                "id": 6275,
                "faiss_score": 0.9147120714187622,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Instead of processing tokens sequentially, transformers process entire sequences in parallel, enabling efficient training on modern hardware.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -1.3350728750228882,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.0016989217838272452,
                  "neutral": 0.43683573603630066,
                  "support": 0.561465322971344
                },
                "stance_score": 0.5597664011875167,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.420360803604126
              }
            ],
            "contradicting": [
              {
                "id": 6426,
                "faiss_score": 0.8849560618400574,
                "faiss_rank": 16,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 165,
                "sentence": "This interdependence complicates efforts to simplify or interpret transformer architectures.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 0.5542351007461548,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.25036686658859253,
                  "neutral": 0.7427451610565186,
                  "support": 0.006887955591082573
                },
                "stance_score": -0.24347891099750996,
                "evidence_contribution": -0.13494455876626898,
                "combined_rank_score": 1.4391911625862122
              }
            ],
            "neutral": [
              {
                "id": 6273,
                "faiss_score": 0.8840447664260864,
                "faiss_rank": 19,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 12,
                "sentence": "The transformer architecture generalized this concept by eliminating recurrence entirely and relying solely on attention mechanisms to model relationships within a sequence.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 1.5003280639648438,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0010420186445116997,
                  "neutral": 0.9928264617919922,
                  "support": 0.0061315009370446205
                },
                "stance_score": 0.005089482292532921,
                "evidence_contribution": 0.007635893114539272,
                "combined_rank_score": 2.38437283039093
              },
              {
                "id": 6261,
                "faiss_score": 0.8891482353210449,
                "faiss_rank": 10,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Transformer-based neural network architectures emerged from a broader effort to overcome structural limitations present in earlier approaches to sequence modeling.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 1.3882910013198853,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0013937019975855947,
                  "neutral": 0.9938596487045288,
                  "support": 0.004746603313833475
                },
                "stance_score": 0.0033529013162478805,
                "evidence_contribution": 0.004654802725660531,
                "combined_rank_score": 2.27743923664093
              },
              {
                "id": 1807,
                "faiss_score": 0.8870072960853577,
                "faiss_rank": 14,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 75,
                "sentence": "LLMs are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 1.3345541954040527,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0011263169581070542,
                  "neutral": 0.94564288854599,
                  "support": 0.053230807185173035
                },
                "stance_score": 0.05210449022706598,
                "evidence_contribution": 0.06953626603192037,
                "combined_rank_score": 2.2215614914894104
              }
            ]
          }
        },
        {
          "subclaim": "Transformer architectures capture long-range dependencies",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [
              {
                "id": 1807,
                "faiss_score": 0.9011234641075134,
                "faiss_rank": 2,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 75,
                "sentence": "LLMs are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -1.2148985862731934,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0071091135032474995,
                  "neutral": 0.7715122699737549,
                  "support": 0.22137866914272308
                },
                "stance_score": 0.21426955563947558,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.31377512216567993
              },
              {
                "id": 6333,
                "faiss_score": 0.8927822113037109,
                "faiss_rank": 6,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 72,
                "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -2.4214539527893066,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.00578355323523283,
                  "neutral": 0.8242341876029968,
                  "support": 0.16998225450515747
                },
                "stance_score": 0.16419870126992464,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.5286717414855957
              },
              {
                "id": 6334,
                "faiss_score": 0.886968731880188,
                "faiss_rank": 18,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 73,
                "sentence": "Rather than training models from scratch for each individual task, practitioners increasingly rely on pretrained transformer backbones that capture broad linguistic or sequential knowledge.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -3.5181727409362793,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.008221310563385487,
                  "neutral": 0.6556732058525085,
                  "support": 0.3361055552959442
                },
                "stance_score": 0.3278842447325587,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.6312040090560913
              }
            ],
            "contradicting": [
              {
                "id": 6426,
                "faiss_score": 0.9104562997817993,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 165,
                "sentence": "This interdependence complicates efforts to simplify or interpret transformer architectures.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -3.2392001152038574,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.6054443120956421,
                  "neutral": 0.32951802015304565,
                  "support": 0.06503769010305405
                },
                "stance_score": -0.540406621992588,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.328743815422058
              }
            ],
            "neutral": [
              {
                "id": 6296,
                "faiss_score": 0.8920329809188843,
                "faiss_rank": 8,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 35,
                "sentence": "The transformer architecture proved highly effective across a wide range of natural language processing tasks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -0.8406956791877747,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0042916773818433285,
                  "neutral": 0.9397426247596741,
                  "support": 0.055965688079595566
                },
                "stance_score": 0.05167401069775224,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.05133730173110962
              },
              {
                "id": 3083,
                "faiss_score": 0.8960912823677063,
                "faiss_rank": 4,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 138,
                "sentence": "The transformer architecture is constructed to calculate output tokens iteratively.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": -1.5020763874053955,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.017301296815276146,
                  "neutral": 0.9663417935371399,
                  "support": 0.016356920823454857
                },
                "stance_score": -0.0009443759918212891,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.6059851050376892
              },
              {
                "id": 2987,
                "faiss_score": 0.8904885053634644,
                "faiss_rank": 9,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": -1.9293653964996338,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.003014353569597006,
                  "neutral": 0.9932332634925842,
                  "support": 0.003752337768673897
                },
                "stance_score": 0.0007379841990768909,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.0388768911361694
              }
            ]
          }
        },
        {
          "subclaim": "Transformer architectures scale efficiently",
          "verdict": "CONTRADICT",
          "controversial": true,
          "strengths": {
            "support": 1.8327672995520672,
            "contradict": 3.6161226726748623,
            "total": 5.4488899722269295
          },
          "evidence": {
            "supporting": [
              {
                "id": 6300,
                "faiss_score": 0.9007487297058105,
                "faiss_rank": 3,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "As transformer models grew larger, their performance continued to improve, reinforcing the idea that scaling was a key driver of success.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 2.286881446838379,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0058963969349861145,
                  "neutral": 0.31589367985725403,
                  "support": 0.6782099008560181
                },
                "stance_score": 0.672313503921032,
                "evidence_contribution": 1.5375012785759097,
                "combined_rank_score": 3.1876301765441895
              },
              {
                "id": 6296,
                "faiss_score": 0.8871057629585266,
                "faiss_rank": 12,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 35,
                "sentence": "The transformer architecture proved highly effective across a wide range of natural language processing tasks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 2.623847007751465,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0037631988525390625,
                  "neutral": 0.8799418807029724,
                  "support": 0.11629492044448853
                },
                "stance_score": 0.11253172159194946,
                "evidence_contribution": 0.2952660209761575,
                "combined_rank_score": 3.5109527707099915
              },
              {
                "id": 1759,
                "faiss_score": 0.8951883316040039,
                "faiss_rank": 6,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 27,
                "sentence": "As of 2024, the largest and most capable models are all based on the transformer architecture.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -0.7142032384872437,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.0027968829963356256,
                  "neutral": 0.8943379521369934,
                  "support": 0.10286521166563034
                },
                "stance_score": 0.10006832866929471,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.18098509311676025
              }
            ],
            "contradicting": [
              {
                "id": 6351,
                "faiss_score": 0.9340454339981079,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 90,
                "sentence": "The architecture of transformers lends itself well to these approaches, but achieving efficient scaling still requires careful engineering.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 5.214434623718262,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.7765777707099915,
                  "neutral": 0.14032764732837677,
                  "support": 0.08309459686279297
                },
                "stance_score": -0.6934831738471985,
                "evidence_contribution": -3.6161226726748623,
                "combined_rank_score": 6.14848005771637
              },
              {
                "id": 6426,
                "faiss_score": 0.8797202110290527,
                "faiss_rank": 19,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 165,
                "sentence": "This interdependence complicates efforts to simplify or interpret transformer architectures.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -0.4166874289512634,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.8952831625938416,
                  "neutral": 0.09827237576246262,
                  "support": 0.006444440223276615
                },
                "stance_score": -0.8888387223705649,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.4630327820777893
              }
            ],
            "neutral": [
              {
                "id": 6333,
                "faiss_score": 0.8859295845031738,
                "faiss_rank": 16,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 72,
                "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 2.3689064979553223,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.004411540925502777,
                  "neutral": 0.9787090420722961,
                  "support": 0.01687942072749138
                },
                "stance_score": 0.012467879801988602,
                "evidence_contribution": 0.029535241478656715,
                "combined_rank_score": 3.254836082458496
              },
              {
                "id": 3083,
                "faiss_score": 0.905130922794342,
                "faiss_rank": 2,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 138,
                "sentence": "The transformer architecture is constructed to calculate output tokens iteratively.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": 1.5265390872955322,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0027802942786365747,
                  "neutral": 0.9887701272964478,
                  "support": 0.008449570275843143
                },
                "stance_score": 0.005669275997206569,
                "evidence_contribution": 0.008654371406402184,
                "combined_rank_score": 2.4316700100898743
              },
              {
                "id": 3070,
                "faiss_score": 0.894027590751648,
                "faiss_rank": 8,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 125,
                "sentence": "The attention mechanism used in the transformer architecture are scaled dot-product attention units.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": 1.3396185636520386,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.002749847248196602,
                  "neutral": 0.9895386695861816,
                  "support": 0.007711493410170078
                },
                "stance_score": 0.004961646161973476,
                "evidence_contribution": 0.006646713304852558,
                "combined_rank_score": 2.2336461544036865
              }
            ]
          }
        },
        {
          "subclaim": "Transformer architectures require large compute budgets",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 6435,
                "faiss_score": 0.9053575992584229,
                "faiss_rank": 3,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 174,
                "sentence": "From an engineering perspective, deploying transformer-based systems requires careful consideration of resource constraints.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -4.20745325088501,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.11402056366205215,
                  "neutral": 0.8815873861312866,
                  "support": 0.004392093978822231
                },
                "stance_score": -0.10962846968322992,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.302095651626587
              }
            ],
            "neutral": [
              {
                "id": 3083,
                "faiss_score": 0.9009952545166016,
                "faiss_rank": 4,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 138,
                "sentence": "The transformer architecture is constructed to calculate output tokens iteratively.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": -1.3453997373580933,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.001218362944200635,
                  "neutral": 0.9977374076843262,
                  "support": 0.0010442466009408236
                },
                "stance_score": -0.0001741163432598114,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.4444044828414917
              },
              {
                "id": 6354,
                "faiss_score": 0.9239134192466736,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 93,
                "sentence": "Transformer-based models can be computationally expensive at inference time, particularly when generating long outputs or processing large batches.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -1.5679820775985718,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0014876507921144366,
                  "neutral": 0.9060214757919312,
                  "support": 0.09249088913202286
                },
                "stance_score": 0.09100323833990842,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.6440686583518982
              },
              {
                "id": 1759,
                "faiss_score": 0.8875057697296143,
                "faiss_rank": 16,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 27,
                "sentence": "As of 2024, the largest and most capable models are all based on the transformer architecture.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -2.110743999481201,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.001405173446983099,
                  "neutral": 0.9974345564842224,
                  "support": 0.001160280779004097
                },
                "stance_score": -0.000244892667979002,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.223238229751587
              }
            ]
          }
        },
        {
          "subclaim": "Transformer architectures are sensitive to hyperparameters",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 0.6742413128261404,
            "contradict": 0.0,
            "total": 0.6742413128261404
          },
          "evidence": {
            "supporting": [
              {
                "id": 6375,
                "faiss_score": 0.9090120196342468,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 114,
                "sentence": "Transformer-based models can be sensitive to adversarial inputs, distribution shifts, and subtle perturbations.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 2.766561508178711,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.06621425598859787,
                  "neutral": 0.6238605380058289,
                  "support": 0.3099251985549927
                },
                "stance_score": 0.2437109425663948,
                "evidence_contribution": 0.6742413128261404,
                "combined_rank_score": 3.6755735278129578
              },
              {
                "id": 6427,
                "faiss_score": 0.8893992304801941,
                "faiss_rank": 17,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 166,
                "sentence": "Transformers are also sensitive to input formatting and prompting strategies.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -4.198785781860352,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.14572855830192566,
                  "neutral": 0.5844072103500366,
                  "support": 0.2698642611503601
                },
                "stance_score": 0.12413570284843445,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.3093865513801575
              }
            ],
            "contradicting": [
              {
                "id": 6341,
                "faiss_score": 0.9085462093353271,
                "faiss_rank": 2,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 80,
                "sentence": "One notable characteristic of transformers trained in this way is their sensitivity to data distribution.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -6.512417793273926,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.3421061038970947,
                  "neutral": 0.48605436086654663,
                  "support": 0.17183957993984222
                },
                "stance_score": -0.1702665239572525,
                "evidence_contribution": -0.0,
                "combined_rank_score": -5.603871583938599
              }
            ],
            "neutral": [
              {
                "id": 6333,
                "faiss_score": 0.8922982215881348,
                "faiss_rank": 10,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 72,
                "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -2.543890953063965,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.03739587962627411,
                  "neutral": 0.9368780255317688,
                  "support": 0.02572610229253769
                },
                "stance_score": -0.01166977733373642,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.65159273147583
              },
              {
                "id": 6261,
                "faiss_score": 0.8909812569618225,
                "faiss_rank": 14,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Transformer-based neural network architectures emerged from a broader effort to overcome structural limitations present in earlier approaches to sequence modeling.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -2.7282087802886963,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.04260823130607605,
                  "neutral": 0.9484440088272095,
                  "support": 0.00894780084490776
                },
                "stance_score": -0.03366043046116829,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.8372275233268738
              },
              {
                "id": 6426,
                "faiss_score": 0.888773500919342,
                "faiss_rank": 20,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 165,
                "sentence": "This interdependence complicates efforts to simplify or interpret transformer architectures.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -3.9169061183929443,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.019332002848386765,
                  "neutral": 0.9136319756507874,
                  "support": 0.06703604757785797
                },
                "stance_score": 0.04770404472947121,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.0281326174736023
              }
            ]
          }
        },
        {
          "subclaim": "Transformer architectures struggle with very long contexts",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [
              {
                "id": 6354,
                "faiss_score": 0.9011918902397156,
                "faiss_rank": 3,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 93,
                "sentence": "Transformer-based models can be computationally expensive at inference time, particularly when generating long outputs or processing large batches.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -3.5607409477233887,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.004106350243091583,
                  "neutral": 0.4961184859275818,
                  "support": 0.4997752010822296
                },
                "stance_score": 0.49566885083913803,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.659549057483673
              }
            ],
            "contradicting": [
              {
                "id": 6333,
                "faiss_score": 0.8825433850288391,
                "faiss_rank": 20,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 72,
                "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -1.6507289409637451,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.3231250047683716,
                  "neutral": 0.6701911091804504,
                  "support": 0.006683885585516691
                },
                "stance_score": -0.3164411191828549,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.768185555934906
              },
              {
                "id": 2954,
                "faiss_score": 0.8992305994033813,
                "faiss_rank": 4,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 9,
                "sentence": "Modern transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": -4.5092573165893555,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.21358922123908997,
                  "neutral": 0.7063183784484863,
                  "support": 0.08009237051010132
                },
                "stance_score": -0.13349685072898865,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.610026717185974
              }
            ],
            "neutral": [
              {
                "id": 2997,
                "faiss_score": 0.8859200477600098,
                "faiss_rank": 17,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 52,
                "sentence": "The plain transformer architecture had difficulty in converging.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": -0.9928778409957886,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.00383265339769423,
                  "neutral": 0.9727511405944824,
                  "support": 0.023416148498654366
                },
                "stance_score": 0.019583495100960135,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.10695779323577881
              },
              {
                "id": 6426,
                "faiss_score": 0.9080196022987366,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 165,
                "sentence": "This interdependence complicates efforts to simplify or interpret transformer architectures.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -2.7841544151306152,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.005680007394403219,
                  "neutral": 0.9186755418777466,
                  "support": 0.07564450800418854
                },
                "stance_score": 0.06996450060978532,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.8761348128318787
              },
              {
                "id": 6379,
                "faiss_score": 0.8833831548690796,
                "faiss_rank": 19,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 118,
                "sentence": "New practitioners often learn transformer architectures early in their training, sometimes at the expense of understanding alternative models.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -2.899383544921875,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.020065484568476677,
                  "neutral": 0.9761005640029907,
                  "support": 0.0038339702878147364
                },
                "stance_score": -0.01623151428066194,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.0160003900527954
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Transformer architectures are sensitive to hyperparameters",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6375,
                      "faiss_score": 0.9090120196342468,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 114,
                      "sentence": "Transformer-based models can be sensitive to adversarial inputs, distribution shifts, and subtle perturbations.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 2.766561508178711,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.06621425598859787,
                        "neutral": 0.6238605380058289,
                        "support": 0.3099251985549927
                      },
                      "stance_score": 0.2437109425663948,
                      "evidence_contribution": 0.6742413128261404,
                      "combined_rank_score": 3.6755735278129578
                    },
                    {
                      "id": 6427,
                      "faiss_score": 0.8893992304801941,
                      "faiss_rank": 17,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 166,
                      "sentence": "Transformers are also sensitive to input formatting and prompting strategies.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -4.198785781860352,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.14572855830192566,
                        "neutral": 0.5844072103500366,
                        "support": 0.2698642611503601
                      },
                      "stance_score": 0.12413570284843445,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -3.3093865513801575
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6341,
                      "faiss_score": 0.9085462093353271,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 80,
                      "sentence": "One notable characteristic of transformers trained in this way is their sensitivity to data distribution.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -6.512417793273926,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.3421061038970947,
                        "neutral": 0.48605436086654663,
                        "support": 0.17183957993984222
                      },
                      "stance_score": -0.1702665239572525,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -5.603871583938599
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6333,
                      "faiss_score": 0.8922982215881348,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 72,
                      "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -2.543890953063965,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.03739587962627411,
                        "neutral": 0.9368780255317688,
                        "support": 0.02572610229253769
                      },
                      "stance_score": -0.01166977733373642,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.65159273147583
                    },
                    {
                      "id": 6261,
                      "faiss_score": 0.8909812569618225,
                      "faiss_rank": 14,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Transformer-based neural network architectures emerged from a broader effort to overcome structural limitations present in earlier approaches to sequence modeling.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -2.7282087802886963,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.04260823130607605,
                        "neutral": 0.9484440088272095,
                        "support": 0.00894780084490776
                      },
                      "stance_score": -0.03366043046116829,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.8372275233268738
                    },
                    {
                      "id": 6426,
                      "faiss_score": 0.888773500919342,
                      "faiss_rank": 20,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 165,
                      "sentence": "This interdependence complicates efforts to simplify or interpret transformer architectures.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -3.9169061183929443,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.019332002848386765,
                        "neutral": 0.9136319756507874,
                        "support": 0.06703604757785797
                      },
                      "stance_score": 0.04770404472947121,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -3.0281326174736023
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Transformer architectures scale efficiently",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6296,
                      "faiss_score": 0.8871057629585266,
                      "faiss_rank": 12,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 35,
                      "sentence": "The transformer architecture proved highly effective across a wide range of natural language processing tasks.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 2.623847007751465,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0037631988525390625,
                        "neutral": 0.8799418807029724,
                        "support": 0.11629492044448853
                      },
                      "stance_score": 0.11253172159194946,
                      "evidence_contribution": 0.2952660209761575,
                      "combined_rank_score": 3.5109527707099915
                    },
                    {
                      "id": 6300,
                      "faiss_score": 0.9007487297058105,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 39,
                      "sentence": "As transformer models grew larger, their performance continued to improve, reinforcing the idea that scaling was a key driver of success.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 2.286881446838379,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.0058963969349861145,
                        "neutral": 0.31589367985725403,
                        "support": 0.6782099008560181
                      },
                      "stance_score": 0.672313503921032,
                      "evidence_contribution": 1.5375012785759097,
                      "combined_rank_score": 3.1876301765441895
                    },
                    {
                      "id": 1759,
                      "faiss_score": 0.8951883316040039,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 27,
                      "sentence": "As of 2024, the largest and most capable models are all based on the transformer architecture.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -0.7142032384872437,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.0027968829963356256,
                        "neutral": 0.8943379521369934,
                        "support": 0.10286521166563034
                      },
                      "stance_score": 0.10006832866929471,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.18098509311676025
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6351,
                      "faiss_score": 0.9340454339981079,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 90,
                      "sentence": "The architecture of transformers lends itself well to these approaches, but achieving efficient scaling still requires careful engineering.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 5.214434623718262,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.7765777707099915,
                        "neutral": 0.14032764732837677,
                        "support": 0.08309459686279297
                      },
                      "stance_score": -0.6934831738471985,
                      "evidence_contribution": -3.6161226726748623,
                      "combined_rank_score": 6.14848005771637
                    },
                    {
                      "id": 6426,
                      "faiss_score": 0.8797202110290527,
                      "faiss_rank": 19,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 165,
                      "sentence": "This interdependence complicates efforts to simplify or interpret transformer architectures.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -0.4166874289512634,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.8952831625938416,
                        "neutral": 0.09827237576246262,
                        "support": 0.006444440223276615
                      },
                      "stance_score": -0.8888387223705649,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.4630327820777893
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6333,
                      "faiss_score": 0.8859295845031738,
                      "faiss_rank": 16,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 72,
                      "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 2.3689064979553223,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.004411540925502777,
                        "neutral": 0.9787090420722961,
                        "support": 0.01687942072749138
                      },
                      "stance_score": 0.012467879801988602,
                      "evidence_contribution": 0.029535241478656715,
                      "combined_rank_score": 3.254836082458496
                    },
                    {
                      "id": 3083,
                      "faiss_score": 0.905130922794342,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 138,
                      "sentence": "The transformer architecture is constructed to calculate output tokens iteratively.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": 1.5265390872955322,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0027802942786365747,
                        "neutral": 0.9887701272964478,
                        "support": 0.008449570275843143
                      },
                      "stance_score": 0.005669275997206569,
                      "evidence_contribution": 0.008654371406402184,
                      "combined_rank_score": 2.4316700100898743
                    },
                    {
                      "id": 3070,
                      "faiss_score": 0.894027590751648,
                      "faiss_rank": 8,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 125,
                      "sentence": "The attention mechanism used in the transformer architecture are scaled dot-product attention units.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": 1.3396185636520386,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.002749847248196602,
                        "neutral": 0.9895386695861816,
                        "support": 0.007711493410170078
                      },
                      "stance_score": 0.004961646161973476,
                      "evidence_contribution": 0.006646713304852558,
                      "combined_rank_score": 2.2336461544036865
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Transformer architectures improve parallelization",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6284,
                      "faiss_score": 0.8840556144714355,
                      "faiss_rank": 18,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 23,
                      "sentence": "Transformers typically employ multiple attention heads in parallel.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 0.2052372246980667,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.001994922524318099,
                        "neutral": 0.8282346129417419,
                        "support": 0.16977041959762573
                      },
                      "stance_score": 0.16777549707330763,
                      "evidence_contribution": 0.03443377739166427,
                      "combined_rank_score": 1.0892928391695023
                    },
                    {
                      "id": 6275,
                      "faiss_score": 0.9147120714187622,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "Instead of processing tokens sequentially, transformers process entire sequences in parallel, enabling efficient training on modern hardware.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -1.3350728750228882,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.0016989217838272452,
                        "neutral": 0.43683573603630066,
                        "support": 0.561465322971344
                      },
                      "stance_score": 0.5597664011875167,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.420360803604126
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6426,
                      "faiss_score": 0.8849560618400574,
                      "faiss_rank": 16,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 165,
                      "sentence": "This interdependence complicates efforts to simplify or interpret transformer architectures.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 0.5542351007461548,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.25036686658859253,
                        "neutral": 0.7427451610565186,
                        "support": 0.006887955591082573
                      },
                      "stance_score": -0.24347891099750996,
                      "evidence_contribution": -0.13494455876626898,
                      "combined_rank_score": 1.4391911625862122
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6273,
                      "faiss_score": 0.8840447664260864,
                      "faiss_rank": 19,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 12,
                      "sentence": "The transformer architecture generalized this concept by eliminating recurrence entirely and relying solely on attention mechanisms to model relationships within a sequence.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 1.5003280639648438,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0010420186445116997,
                        "neutral": 0.9928264617919922,
                        "support": 0.0061315009370446205
                      },
                      "stance_score": 0.005089482292532921,
                      "evidence_contribution": 0.007635893114539272,
                      "combined_rank_score": 2.38437283039093
                    },
                    {
                      "id": 6261,
                      "faiss_score": 0.8891482353210449,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Transformer-based neural network architectures emerged from a broader effort to overcome structural limitations present in earlier approaches to sequence modeling.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 1.3882910013198853,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0013937019975855947,
                        "neutral": 0.9938596487045288,
                        "support": 0.004746603313833475
                      },
                      "stance_score": 0.0033529013162478805,
                      "evidence_contribution": 0.004654802725660531,
                      "combined_rank_score": 2.27743923664093
                    },
                    {
                      "id": 1807,
                      "faiss_score": 0.8870072960853577,
                      "faiss_rank": 14,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "LLMs are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 1.3345541954040527,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0011263169581070542,
                        "neutral": 0.94564288854599,
                        "support": 0.053230807185173035
                      },
                      "stance_score": 0.05210449022706598,
                      "evidence_contribution": 0.06953626603192037,
                      "combined_rank_score": 2.2215614914894104
                    }
                  ]
                }
              },
              {
                "subclaim": "Transformer architectures capture long-range dependencies",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 1807,
                      "faiss_score": 0.9011234641075134,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "LLMs are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -1.2148985862731934,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0071091135032474995,
                        "neutral": 0.7715122699737549,
                        "support": 0.22137866914272308
                      },
                      "stance_score": 0.21426955563947558,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.31377512216567993
                    },
                    {
                      "id": 6333,
                      "faiss_score": 0.8927822113037109,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 72,
                      "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -2.4214539527893066,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.00578355323523283,
                        "neutral": 0.8242341876029968,
                        "support": 0.16998225450515747
                      },
                      "stance_score": 0.16419870126992464,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.5286717414855957
                    },
                    {
                      "id": 6334,
                      "faiss_score": 0.886968731880188,
                      "faiss_rank": 18,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 73,
                      "sentence": "Rather than training models from scratch for each individual task, practitioners increasingly rely on pretrained transformer backbones that capture broad linguistic or sequential knowledge.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -3.5181727409362793,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.008221310563385487,
                        "neutral": 0.6556732058525085,
                        "support": 0.3361055552959442
                      },
                      "stance_score": 0.3278842447325587,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.6312040090560913
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6426,
                      "faiss_score": 0.9104562997817993,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 165,
                      "sentence": "This interdependence complicates efforts to simplify or interpret transformer architectures.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -3.2392001152038574,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.6054443120956421,
                        "neutral": 0.32951802015304565,
                        "support": 0.06503769010305405
                      },
                      "stance_score": -0.540406621992588,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -2.328743815422058
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6296,
                      "faiss_score": 0.8920329809188843,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 35,
                      "sentence": "The transformer architecture proved highly effective across a wide range of natural language processing tasks.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -0.8406956791877747,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0042916773818433285,
                        "neutral": 0.9397426247596741,
                        "support": 0.055965688079595566
                      },
                      "stance_score": 0.05167401069775224,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.05133730173110962
                    },
                    {
                      "id": 3083,
                      "faiss_score": 0.8960912823677063,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 138,
                      "sentence": "The transformer architecture is constructed to calculate output tokens iteratively.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": -1.5020763874053955,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.017301296815276146,
                        "neutral": 0.9663417935371399,
                        "support": 0.016356920823454857
                      },
                      "stance_score": -0.0009443759918212891,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.6059851050376892
                    },
                    {
                      "id": 2987,
                      "faiss_score": 0.8904885053634644,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": -1.9293653964996338,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.003014353569597006,
                        "neutral": 0.9932332634925842,
                        "support": 0.003752337768673897
                      },
                      "stance_score": 0.0007379841990768909,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.0388768911361694
                    }
                  ]
                }
              },
              {
                "subclaim": "Transformer architectures require large compute budgets",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 6435,
                      "faiss_score": 0.9053575992584229,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 174,
                      "sentence": "From an engineering perspective, deploying transformer-based systems requires careful consideration of resource constraints.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -4.20745325088501,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.11402056366205215,
                        "neutral": 0.8815873861312866,
                        "support": 0.004392093978822231
                      },
                      "stance_score": -0.10962846968322992,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -3.302095651626587
                    }
                  ],
                  "neutral": [
                    {
                      "id": 3083,
                      "faiss_score": 0.9009952545166016,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 138,
                      "sentence": "The transformer architecture is constructed to calculate output tokens iteratively.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": -1.3453997373580933,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.001218362944200635,
                        "neutral": 0.9977374076843262,
                        "support": 0.0010442466009408236
                      },
                      "stance_score": -0.0001741163432598114,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.4444044828414917
                    },
                    {
                      "id": 6354,
                      "faiss_score": 0.9239134192466736,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 93,
                      "sentence": "Transformer-based models can be computationally expensive at inference time, particularly when generating long outputs or processing large batches.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -1.5679820775985718,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0014876507921144366,
                        "neutral": 0.9060214757919312,
                        "support": 0.09249088913202286
                      },
                      "stance_score": 0.09100323833990842,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.6440686583518982
                    },
                    {
                      "id": 1759,
                      "faiss_score": 0.8875057697296143,
                      "faiss_rank": 16,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 27,
                      "sentence": "As of 2024, the largest and most capable models are all based on the transformer architecture.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -2.110743999481201,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.001405173446983099,
                        "neutral": 0.9974345564842224,
                        "support": 0.001160280779004097
                      },
                      "stance_score": -0.000244892667979002,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.223238229751587
                    }
                  ]
                }
              },
              {
                "subclaim": "Transformer architectures struggle with very long contexts",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6354,
                      "faiss_score": 0.9011918902397156,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 93,
                      "sentence": "Transformer-based models can be computationally expensive at inference time, particularly when generating long outputs or processing large batches.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -3.5607409477233887,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.004106350243091583,
                        "neutral": 0.4961184859275818,
                        "support": 0.4997752010822296
                      },
                      "stance_score": 0.49566885083913803,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.659549057483673
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6333,
                      "faiss_score": 0.8825433850288391,
                      "faiss_rank": 20,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 72,
                      "sentence": "The influence of transformer architectures is closely tied to their role in large-scale pretraining paradigms.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -1.6507289409637451,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.3231250047683716,
                        "neutral": 0.6701911091804504,
                        "support": 0.006683885585516691
                      },
                      "stance_score": -0.3164411191828549,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.768185555934906
                    },
                    {
                      "id": 2954,
                      "faiss_score": 0.8992305994033813,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "Modern transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": -4.5092573165893555,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.21358922123908997,
                        "neutral": 0.7063183784484863,
                        "support": 0.08009237051010132
                      },
                      "stance_score": -0.13349685072898865,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -3.610026717185974
                    }
                  ],
                  "neutral": [
                    {
                      "id": 2997,
                      "faiss_score": 0.8859200477600098,
                      "faiss_rank": 17,
                      "doc_id": "wiki_Transformer_(machine_learning_model)",
                      "file_type": ".txt",
                      "position": 52,
                      "sentence": "The plain transformer architecture had difficulty in converging.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                      "primary_category": "artificial intelligence",
                      "rerank_score": -0.9928778409957886,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.00383265339769423,
                        "neutral": 0.9727511405944824,
                        "support": 0.023416148498654366
                      },
                      "stance_score": 0.019583495100960135,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.10695779323577881
                    },
                    {
                      "id": 6426,
                      "faiss_score": 0.9080196022987366,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 165,
                      "sentence": "This interdependence complicates efforts to simplify or interpret transformer architectures.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -2.7841544151306152,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.005680007394403219,
                        "neutral": 0.9186755418777466,
                        "support": 0.07564450800418854
                      },
                      "stance_score": 0.06996450060978532,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.8761348128318787
                    },
                    {
                      "id": 6379,
                      "faiss_score": 0.8833831548690796,
                      "faiss_rank": 19,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 118,
                      "sentence": "New practitioners often learn transformer architectures early in their training, sometimes at the expense of understanding alternative models.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -2.899383544921875,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.020065484568476677,
                        "neutral": 0.9761005640029907,
                        "support": 0.0038339702878147364
                      },
                      "stance_score": -0.01623151428066194,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -2.0160003900527954
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Distributed systems improve scalability, availability, and fault tolerance, but introduce coordination overhead, consistency challenges, debugging difficulty, and increased system complexity.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Distributed systems improve scalability",
          "verdict": "CONTRADICT",
          "controversial": true,
          "strengths": {
            "support": 1.0321325065354696,
            "contradict": 2.1855973309983234,
            "total": 3.217729837533793
          },
          "evidence": {
            "supporting": [
              {
                "id": 499,
                "faiss_score": 0.9017831683158875,
                "faiss_rank": 10,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 75,
                "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.7927249670028687,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0015425255987793207,
                  "neutral": 0.4211810231208801,
                  "support": 0.5772764086723328
                },
                "stance_score": 0.5757338830735534,
                "evidence_contribution": 1.0321325065354696,
                "combined_rank_score": 2.694508135318756
              }
            ],
            "contradicting": [
              {
                "id": 5658,
                "faiss_score": 0.9028811454772949,
                "faiss_rank": 9,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 61,
                "sentence": "Eventually consistent systems illustrate how relaxing guarantees can improve scalability.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 2.7626805305480957,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.8435043692588806,
                  "neutral": 0.10410597920417786,
                  "support": 0.052389614284038544
                },
                "stance_score": -0.7911147549748421,
                "evidence_contribution": -2.1855973309983234,
                "combined_rank_score": 3.6655616760253906
              }
            ],
            "neutral": [
              {
                "id": 445,
                "faiss_score": 0.9056973457336426,
                "faiss_rank": 4,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.2563084363937378,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.000767619232647121,
                  "neutral": 0.9977124929428101,
                  "support": 0.001519894110970199
                },
                "stance_score": 0.0007522748783230782,
                "evidence_contribution": 0.0009450892761243557,
                "combined_rank_score": 2.1620057821273804
              },
              {
                "id": 5597,
                "faiss_score": 0.9030466675758362,
                "faiss_rank": 8,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 1.2206794023513794,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0007695626700296998,
                  "neutral": 0.9976724982261658,
                  "support": 0.0015579789178445935
                },
                "stance_score": 0.0007884162478148937,
                "evidence_contribution": 0.0009624034741868015,
                "combined_rank_score": 2.1237260699272156
              },
              {
                "id": 5646,
                "faiss_score": 0.9050657749176025,
                "faiss_rank": 7,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 49,
                "sentence": "The evolution of distributed systems has been driven by practical needs.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": -0.1592615842819214,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.0006361278356052935,
                  "neutral": 0.9965994954109192,
                  "support": 0.002764445496723056
                },
                "stance_score": 0.0021283176611177623,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.7458041906356812
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems improve availability",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 0.9617573299358448,
            "contradict": 0.0,
            "total": 0.9617573299358448
          },
          "evidence": {
            "supporting": [
              {
                "id": 3817,
                "faiss_score": 0.8974882364273071,
                "faiss_rank": 11,
                "doc_id": "wiki_CAP_theorem",
                "file_type": ".txt",
                "position": 8,
                "sentence": "Most modern distributed databases offer configuration options for both consistency and availability.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/CAP_theorem",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 2.578249454498291,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.00218738685362041,
                  "neutral": 0.6701467633247375,
                  "support": 0.3276658058166504
                },
                "stance_score": 0.32547841896303,
                "evidence_contribution": 0.8391645561423983,
                "combined_rank_score": 3.475737690925598
              },
              {
                "id": 5668,
                "faiss_score": 0.8971213102340698,
                "faiss_rank": 12,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 71,
                "sentence": "Modern distributed systems increasingly rely on automation for deployment, scaling, and recovery.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 1.209920048713684,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.0025057359598577023,
                  "neutral": 0.8936654329299927,
                  "support": 0.10382877290248871
                },
                "stance_score": 0.101323036942631,
                "evidence_contribution": 0.12259277379344652,
                "combined_rank_score": 2.107041358947754
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 445,
                "faiss_score": 0.8953639268875122,
                "faiss_rank": 14,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.510157585144043,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0007595759234391153,
                  "neutral": 0.9976643323898315,
                  "support": 0.0015760851092636585
                },
                "stance_score": 0.0008165091858245432,
                "evidence_contribution": 0.0012330575403127209,
                "combined_rank_score": 2.405521512031555
              },
              {
                "id": 499,
                "faiss_score": 0.8946641087532043,
                "faiss_rank": 18,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 75,
                "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.4583913087844849,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0019902971107512712,
                  "neutral": 0.9616448879241943,
                  "support": 0.03636479750275612
                },
                "stance_score": 0.03437450039200485,
                "evidence_contribution": 0.05013147261550874,
                "combined_rank_score": 2.353055417537689
              },
              {
                "id": 5646,
                "faiss_score": 0.8958644270896912,
                "faiss_rank": 13,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 49,
                "sentence": "The evolution of distributed systems has been driven by practical needs.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 1.289405345916748,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0006776307127438486,
                  "neutral": 0.9960979223251343,
                  "support": 0.003224448300898075
                },
                "stance_score": 0.0025468175881542265,
                "evidence_contribution": 0.0032838802132408584,
                "combined_rank_score": 2.185269773006439
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems improve fault tolerance",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.6231427099437152,
            "contradict": 1.885650826345333,
            "total": 2.5087935362890486
          },
          "evidence": {
            "supporting": [
              {
                "id": 5614,
                "faiss_score": 0.9063659906387329,
                "faiss_rank": 8,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 17,
                "sentence": "Replication is commonly used to improve fault tolerance and availability.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 1.4775861501693726,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.0022014533169567585,
                  "neutral": 0.5738668441772461,
                  "support": 0.42393165826797485
                },
                "stance_score": 0.4217302049510181,
                "evidence_contribution": 0.6231427099437152,
                "combined_rank_score": 2.3839521408081055
              }
            ],
            "contradicting": [
              {
                "id": 568,
                "faiss_score": 0.9054936170578003,
                "faiss_rank": 9,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 144,
                "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 4.992817401885986,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.38152015209198,
                  "neutral": 0.6146324276924133,
                  "support": 0.0038474525790661573
                },
                "stance_score": -0.3776726995129138,
                "evidence_contribution": -1.885650826345333,
                "combined_rank_score": 5.898311018943787
              }
            ],
            "neutral": [
              {
                "id": 430,
                "faiss_score": 0.9204719066619873,
                "faiss_rank": 1,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 6,
                "sentence": "Other typical properties of distributed systems are: The system must tolerate failures in individual computers.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 2.3290367126464844,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.00475419033318758,
                  "neutral": 0.9388123750686646,
                  "support": 0.056433361023664474
                },
                "stance_score": 0.051679170690476894,
                "evidence_contribution": 0.12036268581724485,
                "combined_rank_score": 3.2495086193084717
              },
              {
                "id": 493,
                "faiss_score": 0.8894362449645996,
                "faiss_rank": 19,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 69,
                "sentence": "Cell-based architecture has been adopted in some large-scale distributed systems, particularly in cloud-native and high-availability environments, where fault isolation and redundancy are key design considerations.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 2.1658031940460205,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.0025158931966871023,
                  "neutral": 0.9371961951255798,
                  "support": 0.06028788164258003
                },
                "stance_score": 0.05777198844589293,
                "evidence_contribution": 0.1251227571025047,
                "combined_rank_score": 3.05523943901062
              },
              {
                "id": 3521,
                "faiss_score": 0.89457106590271,
                "faiss_rank": 16,
                "doc_id": "wiki_Fault_tolerance",
                "file_type": ".txt",
                "position": 92,
                "sentence": "Fault-tolerant systems are typically based on the concept of redundancy.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 1.3589634895324707,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.004133067559450865,
                  "neutral": 0.9230316877365112,
                  "support": 0.07283523678779602
                },
                "stance_score": 0.06870216922834516,
                "evidence_contribution": 0.09336373963300226,
                "combined_rank_score": 2.2535345554351807
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems introduce coordination overhead",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 6.237892706271433,
            "contradict": 0.0,
            "total": 6.237892706271433
          },
          "evidence": {
            "supporting": [
              {
                "id": 6685,
                "faiss_score": 0.9233837127685547,
                "faiss_rank": 1,
                "doc_id": "local_systems_latency_throughput_tradeoffs.txt",
                "file_type": ".txt",
                "position": 17,
                "sentence": "However, parallelism introduces coordination overhead, synchronization costs, and contention for shared resources.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_latency_throughput_tradeoffs.txt",
                "primary_category": null,
                "rerank_score": 4.450502395629883,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0018082939786836505,
                  "neutral": 0.06981738656759262,
                  "support": 0.9283743500709534
                },
                "stance_score": 0.9265660560922697,
                "evidence_contribution": 4.1236844523479785,
                "combined_rank_score": 5.3738861083984375
              },
              {
                "id": 5633,
                "faiss_score": 0.9139895439147949,
                "faiss_rank": 5,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 36,
                "sentence": "However, scaling introduces coordination overhead that can limit achievable gains.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 2.998728036880493,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.006788631435483694,
                  "neutral": 0.2813877463340759,
                  "support": 0.7118236422538757
                },
                "stance_score": 0.705035010818392,
                "evidence_contribution": 2.114208253923454,
                "combined_rank_score": 3.912717580795288
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 585,
                "faiss_score": 0.9145564436912537,
                "faiss_rank": 2,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 161,
                "sentence": "In order to perform coordination, distributed systems employ the concept of coordinators.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 3.5947370529174805,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.21808390319347382,
                  "neutral": 0.6171427369117737,
                  "support": 0.1647733598947525
                },
                "stance_score": -0.053310543298721313,
                "evidence_contribution": -0.1916373853070752,
                "combined_rank_score": 4.509293496608734
              },
              {
                "id": 542,
                "faiss_score": 0.9130808115005493,
                "faiss_rank": 6,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 118,
                "sentence": "The main focus is on coordinating the operation of an arbitrary distributed system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.3913816213607788,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.007331767585128546,
                  "neutral": 0.9015898108482361,
                  "support": 0.09107837826013565
                },
                "stance_score": 0.0837466106750071,
                "evidence_contribution": 0.11652349494446129,
                "combined_rank_score": 2.304462432861328
              },
              {
                "id": 5597,
                "faiss_score": 0.8991247415542603,
                "faiss_rank": 11,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 1.1333130598068237,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.07815498858690262,
                  "neutral": 0.9142075777053833,
                  "support": 0.007637408562004566
                },
                "stance_score": -0.07051758002489805,
                "evidence_contribution": -0.07991849438818976,
                "combined_rank_score": 2.032437801361084
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems introduce consistency challenges",
          "verdict": "SUPPORT",
          "controversial": true,
          "strengths": {
            "support": 4.032798836433388,
            "contradict": 1.3893306015126505,
            "total": 5.4221294379460385
          },
          "evidence": {
            "supporting": [
              {
                "id": 5600,
                "faiss_score": 0.8907561302185059,
                "faiss_rank": 16,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "Achieving this illusion of coherence in the presence of failures, delays, and partial information is the central challenge of distributed systems design.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 4.005821228027344,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.005488081835210323,
                  "neutral": 0.5665578842163086,
                  "support": 0.4279540479183197
                },
                "stance_score": 0.4224659660831094,
                "evidence_contribution": 1.6923231350547994,
                "combined_rank_score": 4.89657735824585
              },
              {
                "id": 5612,
                "faiss_score": 0.9273715019226074,
                "faiss_rank": 2,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 15,
                "sentence": "Strong consistency models aim to make distributed systems behave as if there were a single shared state, but enforcing such behavior requires coordination and synchronization, which can be expensive or impossible under certain failure conditions.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 4.537818908691406,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0031162735540419817,
                  "neutral": 0.731314480304718,
                  "support": 0.2655692398548126
                },
                "stance_score": 0.26245296630077064,
                "evidence_contribution": 1.1909640331217854,
                "combined_rank_score": 5.465190410614014
              },
              {
                "id": 5634,
                "faiss_score": 0.907062292098999,
                "faiss_rank": 3,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 37,
                "sentence": "State management is particularly challenging in distributed systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 2.5993077754974365,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0030246321111917496,
                  "neutral": 0.7135766744613647,
                  "support": 0.28339874744415283
                },
                "stance_score": 0.2803741153329611,
                "evidence_contribution": 0.7287786180331808,
                "combined_rank_score": 3.5063700675964355
              },
              {
                "id": 5618,
                "faiss_score": 0.8958674669265747,
                "faiss_rank": 11,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Consensus is a fundamental problem in distributed systems that captures the difficulty of agreement in the presence of failures.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 0.6288250088691711,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.0027271497529000044,
                  "neutral": 0.3254675567150116,
                  "support": 0.6718052625656128
                },
                "stance_score": 0.6690781128127128,
                "evidence_contribution": 0.4207330502236224,
                "combined_rank_score": 1.5246924757957458
              }
            ],
            "contradicting": [
              {
                "id": 5610,
                "faiss_score": 0.9464955925941467,
                "faiss_rank": 1,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Consistency is a central concept in distributed systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 4.305271148681641,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.2551124393939972,
                  "neutral": 0.7050133943557739,
                  "support": 0.03987419977784157
                },
                "stance_score": -0.21523823961615562,
                "evidence_contribution": -0.9266589831124605,
                "combined_rank_score": 5.251766741275787
              },
              {
                "id": 3817,
                "faiss_score": 0.8984368443489075,
                "faiss_rank": 10,
                "doc_id": "wiki_CAP_theorem",
                "file_type": ".txt",
                "position": 8,
                "sentence": "Most modern distributed databases offer configuration options for both consistency and availability.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/CAP_theorem",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 1.4618778228759766,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.3348703682422638,
                  "neutral": 0.6467505693435669,
                  "support": 0.01837906427681446
                },
                "stance_score": -0.31649130396544933,
                "evidence_contribution": -0.46267161840019,
                "combined_rank_score": 2.360314667224884
              },
              {
                "id": 5658,
                "faiss_score": 0.8944575190544128,
                "faiss_rank": 12,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 61,
                "sentence": "Eventually consistent systems illustrate how relaxing guarantees can improve scalability.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": -0.7680312395095825,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.4701039791107178,
                  "neutral": 0.5211150050163269,
                  "support": 0.00878104753792286
                },
                "stance_score": -0.4613229315727949,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.12642627954483032
              }
            ],
            "neutral": [
              {
                "id": 568,
                "faiss_score": 0.9033069610595703,
                "faiss_rank": 4,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 144,
                "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": 2.3717024326324463,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.006213893182575703,
                  "neutral": 0.8884377479553223,
                  "support": 0.10534839332103729
                },
                "stance_score": 0.09913450013846159,
                "evidence_contribution": 0.23511753513619094,
                "combined_rank_score": 3.2750093936920166
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems introduce debugging difficulty",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 6.467301621731835,
            "contradict": 0.0,
            "total": 6.467301621731835
          },
          "evidence": {
            "supporting": [
              {
                "id": 5644,
                "faiss_score": 0.9589457511901855,
                "faiss_rank": 1,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 47,
                "sentence": "Because failures and performance issues may arise from interactions between components, debugging distributed systems is notoriously difficult.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 6.9515380859375,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0014179548015818,
                  "neutral": 0.06682299822568893,
                  "support": 0.9317590594291687
                },
                "stance_score": 0.9303411046275869,
                "evidence_contribution": 6.467301621731835,
                "combined_rank_score": 7.9104838371276855
              },
              {
                "id": 6619,
                "faiss_score": 0.9016984105110168,
                "faiss_rank": 3,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 22,
                "sentence": "In distributed systems, it is often impossible to distinguish between a failed component and a slow or unreachable one.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": -3.407491683959961,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.0018577711889520288,
                  "neutral": 0.8090157508850098,
                  "support": 0.18912647664546967
                },
                "stance_score": 0.18726870545651764,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.505793273448944
              }
            ],
            "contradicting": [
              {
                "id": 5662,
                "faiss_score": 0.9012587070465088,
                "faiss_rank": 4,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 65,
                "sentence": "Distributed systems research emphasizes the importance of understanding failure modes.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": -2.6714560985565186,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.10966315865516663,
                  "neutral": 0.8834505081176758,
                  "support": 0.006886407732963562
                },
                "stance_score": -0.10277675092220306,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.7701973915100098
              }
            ],
            "neutral": [
              {
                "id": 568,
                "faiss_score": 0.8847370743751526,
                "faiss_rank": 15,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 144,
                "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": -0.7992189526557922,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.005395065527409315,
                  "neutral": 0.9231296181678772,
                  "support": 0.07147528231143951
                },
                "stance_score": 0.0660802167840302,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.08551812171936035
              },
              {
                "id": 5634,
                "faiss_score": 0.9050875306129456,
                "faiss_rank": 2,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 37,
                "sentence": "State management is particularly challenging in distributed systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": -1.3250395059585571,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.012132744304835796,
                  "neutral": 0.9328728318214417,
                  "support": 0.05499438941478729
                },
                "stance_score": 0.042861645109951496,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.4199519753456116
              },
              {
                "id": 5600,
                "faiss_score": 0.8822277784347534,
                "faiss_rank": 19,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "Achieving this illusion of coherence in the presence of failures, delays, and partial information is the central challenge of distributed systems design.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": -1.884582281112671,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.00968132633715868,
                  "neutral": 0.9559323787689209,
                  "support": 0.03438630327582359
                },
                "stance_score": 0.024704976938664913,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.0023545026779175
              }
            ]
          }
        },
        {
          "subclaim": "Distributed systems introduce increased system complexity",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 4.237919804408783,
            "contradict": 0.0,
            "total": 4.237919804408783
          },
          "evidence": {
            "supporting": [
              {
                "id": 6234,
                "faiss_score": 0.8932847380638123,
                "faiss_rank": 12,
                "doc_id": "local_ml_training_dynamics.txt",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Distributed training introduces additional complexity into training dynamics.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                "primary_category": null,
                "rerank_score": 3.379138708114624,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.001261566998437047,
                  "neutral": 0.04580743610858917,
                  "support": 0.9529309272766113
                },
                "stance_score": 0.9516693602781743,
                "evidence_contribution": 3.2158227726426607,
                "combined_rank_score": 4.272423446178436
              },
              {
                "id": 5674,
                "faiss_score": 0.9300456047058105,
                "faiss_rank": 1,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 77,
                "sentence": "Distributed systems exemplify the broader theme that complexity emerges from interaction.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": 4.361238479614258,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.07227763533592224,
                  "neutral": 0.6210853457450867,
                  "support": 0.3066369891166687
                },
                "stance_score": 0.23435935378074646,
                "evidence_contribution": 1.0220970317661227,
                "combined_rank_score": 5.291284084320068
              }
            ],
            "contradicting": [
              {
                "id": 499,
                "faiss_score": 0.8980528116226196,
                "faiss_rank": 5,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 75,
                "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": -1.3816155195236206,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.8274977803230286,
                  "neutral": 0.16937381029129028,
                  "support": 0.0031284403521567583
                },
                "stance_score": -0.8243693399708718,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.483562707901001
              }
            ],
            "neutral": [
              {
                "id": 5597,
                "faiss_score": 0.8919234275817871,
                "faiss_rank": 13,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": -0.5184941291809082,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.025026975199580193,
                  "neutral": 0.9722483158111572,
                  "support": 0.002724685473367572
                },
                "stance_score": -0.02230228972621262,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.3734292984008789
              },
              {
                "id": 430,
                "faiss_score": 0.8880521059036255,
                "faiss_rank": 18,
                "doc_id": "wiki_Distributed_computing",
                "file_type": ".txt",
                "position": 6,
                "sentence": "Other typical properties of distributed systems are: The system must tolerate failures in individual computers.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                "primary_category": "all articles with unsourced statements",
                "rerank_score": -1.6830490827560425,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.017227118834853172,
                  "neutral": 0.972519040107727,
                  "support": 0.010253790766000748
                },
                "stance_score": -0.006973328068852425,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.794996976852417
              },
              {
                "id": 5650,
                "faiss_score": 0.9097122550010681,
                "faiss_rank": 2,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 53,
                "sentence": "Distributed systems also intersect with security concerns.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": -1.7851831912994385,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.005453850608319044,
                  "neutral": 0.9848649501800537,
                  "support": 0.009681235998868942
                },
                "stance_score": 0.004227385390549898,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.8754709362983704
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Distributed systems improve availability",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 3817,
                      "faiss_score": 0.8974882364273071,
                      "faiss_rank": 11,
                      "doc_id": "wiki_CAP_theorem",
                      "file_type": ".txt",
                      "position": 8,
                      "sentence": "Most modern distributed databases offer configuration options for both consistency and availability.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/CAP_theorem",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 2.578249454498291,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.00218738685362041,
                        "neutral": 0.6701467633247375,
                        "support": 0.3276658058166504
                      },
                      "stance_score": 0.32547841896303,
                      "evidence_contribution": 0.8391645561423983,
                      "combined_rank_score": 3.475737690925598
                    },
                    {
                      "id": 5668,
                      "faiss_score": 0.8971213102340698,
                      "faiss_rank": 12,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 71,
                      "sentence": "Modern distributed systems increasingly rely on automation for deployment, scaling, and recovery.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 1.209920048713684,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.0025057359598577023,
                        "neutral": 0.8936654329299927,
                        "support": 0.10382877290248871
                      },
                      "stance_score": 0.101323036942631,
                      "evidence_contribution": 0.12259277379344652,
                      "combined_rank_score": 2.107041358947754
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 445,
                      "faiss_score": 0.8953639268875122,
                      "faiss_rank": 14,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.510157585144043,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0007595759234391153,
                        "neutral": 0.9976643323898315,
                        "support": 0.0015760851092636585
                      },
                      "stance_score": 0.0008165091858245432,
                      "evidence_contribution": 0.0012330575403127209,
                      "combined_rank_score": 2.405521512031555
                    },
                    {
                      "id": 499,
                      "faiss_score": 0.8946641087532043,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.4583913087844849,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0019902971107512712,
                        "neutral": 0.9616448879241943,
                        "support": 0.03636479750275612
                      },
                      "stance_score": 0.03437450039200485,
                      "evidence_contribution": 0.05013147261550874,
                      "combined_rank_score": 2.353055417537689
                    },
                    {
                      "id": 5646,
                      "faiss_score": 0.8958644270896912,
                      "faiss_rank": 13,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 49,
                      "sentence": "The evolution of distributed systems has been driven by practical needs.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 1.289405345916748,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.0006776307127438486,
                        "neutral": 0.9960979223251343,
                        "support": 0.003224448300898075
                      },
                      "stance_score": 0.0025468175881542265,
                      "evidence_contribution": 0.0032838802132408584,
                      "combined_rank_score": 2.185269773006439
                    }
                  ]
                }
              },
              {
                "subclaim": "Distributed systems introduce coordination overhead",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6685,
                      "faiss_score": 0.9233837127685547,
                      "faiss_rank": 1,
                      "doc_id": "local_systems_latency_throughput_tradeoffs.txt",
                      "file_type": ".txt",
                      "position": 17,
                      "sentence": "However, parallelism introduces coordination overhead, synchronization costs, and contention for shared resources.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_latency_throughput_tradeoffs.txt",
                      "primary_category": null,
                      "rerank_score": 4.450502395629883,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0018082939786836505,
                        "neutral": 0.06981738656759262,
                        "support": 0.9283743500709534
                      },
                      "stance_score": 0.9265660560922697,
                      "evidence_contribution": 4.1236844523479785,
                      "combined_rank_score": 5.3738861083984375
                    },
                    {
                      "id": 5633,
                      "faiss_score": 0.9139895439147949,
                      "faiss_rank": 5,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 36,
                      "sentence": "However, scaling introduces coordination overhead that can limit achievable gains.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 2.998728036880493,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.006788631435483694,
                        "neutral": 0.2813877463340759,
                        "support": 0.7118236422538757
                      },
                      "stance_score": 0.705035010818392,
                      "evidence_contribution": 2.114208253923454,
                      "combined_rank_score": 3.912717580795288
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 585,
                      "faiss_score": 0.9145564436912537,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 161,
                      "sentence": "In order to perform coordination, distributed systems employ the concept of coordinators.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 3.5947370529174805,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.21808390319347382,
                        "neutral": 0.6171427369117737,
                        "support": 0.1647733598947525
                      },
                      "stance_score": -0.053310543298721313,
                      "evidence_contribution": -0.1916373853070752,
                      "combined_rank_score": 4.509293496608734
                    },
                    {
                      "id": 542,
                      "faiss_score": 0.9130808115005493,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 118,
                      "sentence": "The main focus is on coordinating the operation of an arbitrary distributed system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.3913816213607788,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.007331767585128546,
                        "neutral": 0.9015898108482361,
                        "support": 0.09107837826013565
                      },
                      "stance_score": 0.0837466106750071,
                      "evidence_contribution": 0.11652349494446129,
                      "combined_rank_score": 2.304462432861328
                    },
                    {
                      "id": 5597,
                      "faiss_score": 0.8991247415542603,
                      "faiss_rank": 11,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 1.1333130598068237,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.07815498858690262,
                        "neutral": 0.9142075777053833,
                        "support": 0.007637408562004566
                      },
                      "stance_score": -0.07051758002489805,
                      "evidence_contribution": -0.07991849438818976,
                      "combined_rank_score": 2.032437801361084
                    }
                  ]
                }
              },
              {
                "subclaim": "Distributed systems introduce consistency challenges",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 5612,
                      "faiss_score": 0.9273715019226074,
                      "faiss_rank": 2,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 15,
                      "sentence": "Strong consistency models aim to make distributed systems behave as if there were a single shared state, but enforcing such behavior requires coordination and synchronization, which can be expensive or impossible under certain failure conditions.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 4.537818908691406,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0031162735540419817,
                        "neutral": 0.731314480304718,
                        "support": 0.2655692398548126
                      },
                      "stance_score": 0.26245296630077064,
                      "evidence_contribution": 1.1909640331217854,
                      "combined_rank_score": 5.465190410614014
                    },
                    {
                      "id": 5600,
                      "faiss_score": 0.8907561302185059,
                      "faiss_rank": 16,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "Achieving this illusion of coherence in the presence of failures, delays, and partial information is the central challenge of distributed systems design.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 4.005821228027344,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.005488081835210323,
                        "neutral": 0.5665578842163086,
                        "support": 0.4279540479183197
                      },
                      "stance_score": 0.4224659660831094,
                      "evidence_contribution": 1.6923231350547994,
                      "combined_rank_score": 4.89657735824585
                    },
                    {
                      "id": 5634,
                      "faiss_score": 0.907062292098999,
                      "faiss_rank": 3,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 37,
                      "sentence": "State management is particularly challenging in distributed systems.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 2.5993077754974365,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.0030246321111917496,
                        "neutral": 0.7135766744613647,
                        "support": 0.28339874744415283
                      },
                      "stance_score": 0.2803741153329611,
                      "evidence_contribution": 0.7287786180331808,
                      "combined_rank_score": 3.5063700675964355
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 5610,
                      "faiss_score": 0.9464955925941467,
                      "faiss_rank": 1,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Consistency is a central concept in distributed systems.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 4.305271148681641,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.2551124393939972,
                        "neutral": 0.7050133943557739,
                        "support": 0.03987419977784157
                      },
                      "stance_score": -0.21523823961615562,
                      "evidence_contribution": -0.9266589831124605,
                      "combined_rank_score": 5.251766741275787
                    },
                    {
                      "id": 3817,
                      "faiss_score": 0.8984368443489075,
                      "faiss_rank": 10,
                      "doc_id": "wiki_CAP_theorem",
                      "file_type": ".txt",
                      "position": 8,
                      "sentence": "Most modern distributed databases offer configuration options for both consistency and availability.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/CAP_theorem",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.4618778228759766,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.3348703682422638,
                        "neutral": 0.6467505693435669,
                        "support": 0.01837906427681446
                      },
                      "stance_score": -0.31649130396544933,
                      "evidence_contribution": -0.46267161840019,
                      "combined_rank_score": 2.360314667224884
                    },
                    {
                      "id": 5658,
                      "faiss_score": 0.8944575190544128,
                      "faiss_rank": 12,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 61,
                      "sentence": "Eventually consistent systems illustrate how relaxing guarantees can improve scalability.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": -0.7680312395095825,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.4701039791107178,
                        "neutral": 0.5211150050163269,
                        "support": 0.00878104753792286
                      },
                      "stance_score": -0.4613229315727949,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.12642627954483032
                    }
                  ],
                  "neutral": [
                    {
                      "id": 568,
                      "faiss_score": 0.9033069610595703,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 144,
                      "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 2.3717024326324463,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.006213893182575703,
                        "neutral": 0.8884377479553223,
                        "support": 0.10534839332103729
                      },
                      "stance_score": 0.09913450013846159,
                      "evidence_contribution": 0.23511753513619094,
                      "combined_rank_score": 3.2750093936920166
                    }
                  ]
                }
              },
              {
                "subclaim": "Distributed systems introduce debugging difficulty",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 5644,
                      "faiss_score": 0.9589457511901855,
                      "faiss_rank": 1,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 47,
                      "sentence": "Because failures and performance issues may arise from interactions between components, debugging distributed systems is notoriously difficult.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 6.9515380859375,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0014179548015818,
                        "neutral": 0.06682299822568893,
                        "support": 0.9317590594291687
                      },
                      "stance_score": 0.9303411046275869,
                      "evidence_contribution": 6.467301621731835,
                      "combined_rank_score": 7.9104838371276855
                    },
                    {
                      "id": 6619,
                      "faiss_score": 0.9016984105110168,
                      "faiss_rank": 3,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 22,
                      "sentence": "In distributed systems, it is often impossible to distinguish between a failed component and a slow or unreachable one.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": -3.407491683959961,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.0018577711889520288,
                        "neutral": 0.8090157508850098,
                        "support": 0.18912647664546967
                      },
                      "stance_score": 0.18726870545651764,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.505793273448944
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 5662,
                      "faiss_score": 0.9012587070465088,
                      "faiss_rank": 4,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 65,
                      "sentence": "Distributed systems research emphasizes the importance of understanding failure modes.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": -2.6714560985565186,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.10966315865516663,
                        "neutral": 0.8834505081176758,
                        "support": 0.006886407732963562
                      },
                      "stance_score": -0.10277675092220306,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.7701973915100098
                    }
                  ],
                  "neutral": [
                    {
                      "id": 568,
                      "faiss_score": 0.8847370743751526,
                      "faiss_rank": 15,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 144,
                      "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": -0.7992189526557922,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.005395065527409315,
                        "neutral": 0.9231296181678772,
                        "support": 0.07147528231143951
                      },
                      "stance_score": 0.0660802167840302,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.08551812171936035
                    },
                    {
                      "id": 5634,
                      "faiss_score": 0.9050875306129456,
                      "faiss_rank": 2,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 37,
                      "sentence": "State management is particularly challenging in distributed systems.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": -1.3250395059585571,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.012132744304835796,
                        "neutral": 0.9328728318214417,
                        "support": 0.05499438941478729
                      },
                      "stance_score": 0.042861645109951496,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.4199519753456116
                    },
                    {
                      "id": 5600,
                      "faiss_score": 0.8822277784347534,
                      "faiss_rank": 19,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "Achieving this illusion of coherence in the presence of failures, delays, and partial information is the central challenge of distributed systems design.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": -1.884582281112671,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.00968132633715868,
                        "neutral": 0.9559323787689209,
                        "support": 0.03438630327582359
                      },
                      "stance_score": 0.024704976938664913,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.0023545026779175
                    }
                  ]
                }
              },
              {
                "subclaim": "Distributed systems introduce increased system complexity",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 5674,
                      "faiss_score": 0.9300456047058105,
                      "faiss_rank": 1,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 77,
                      "sentence": "Distributed systems exemplify the broader theme that complexity emerges from interaction.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 4.361238479614258,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.07227763533592224,
                        "neutral": 0.6210853457450867,
                        "support": 0.3066369891166687
                      },
                      "stance_score": 0.23435935378074646,
                      "evidence_contribution": 1.0220970317661227,
                      "combined_rank_score": 5.291284084320068
                    },
                    {
                      "id": 6234,
                      "faiss_score": 0.8932847380638123,
                      "faiss_rank": 12,
                      "doc_id": "local_ml_training_dynamics.txt",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Distributed training introduces additional complexity into training dynamics.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                      "primary_category": null,
                      "rerank_score": 3.379138708114624,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.001261566998437047,
                        "neutral": 0.04580743610858917,
                        "support": 0.9529309272766113
                      },
                      "stance_score": 0.9516693602781743,
                      "evidence_contribution": 3.2158227726426607,
                      "combined_rank_score": 4.272423446178436
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 499,
                      "faiss_score": 0.8980528116226196,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": -1.3816155195236206,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.8274977803230286,
                        "neutral": 0.16937381029129028,
                        "support": 0.0031284403521567583
                      },
                      "stance_score": -0.8243693399708718,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.483562707901001
                    }
                  ],
                  "neutral": [
                    {
                      "id": 5597,
                      "faiss_score": 0.8919234275817871,
                      "faiss_rank": 13,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": -0.5184941291809082,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.025026975199580193,
                        "neutral": 0.9722483158111572,
                        "support": 0.002724685473367572
                      },
                      "stance_score": -0.02230228972621262,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.3734292984008789
                    },
                    {
                      "id": 430,
                      "faiss_score": 0.8880521059036255,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 6,
                      "sentence": "Other typical properties of distributed systems are: The system must tolerate failures in individual computers.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": -1.6830490827560425,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.017227118834853172,
                        "neutral": 0.972519040107727,
                        "support": 0.010253790766000748
                      },
                      "stance_score": -0.006973328068852425,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.794996976852417
                    },
                    {
                      "id": 5650,
                      "faiss_score": 0.9097122550010681,
                      "faiss_rank": 2,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 53,
                      "sentence": "Distributed systems also intersect with security concerns.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": -1.7851831912994385,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.005453850608319044,
                        "neutral": 0.9848649501800537,
                        "support": 0.009681235998868942
                      },
                      "stance_score": 0.004227385390549898,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.8754709362983704
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Distributed systems improve scalability",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 499,
                      "faiss_score": 0.9017831683158875,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 75,
                      "sentence": "Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.7927249670028687,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0015425255987793207,
                        "neutral": 0.4211810231208801,
                        "support": 0.5772764086723328
                      },
                      "stance_score": 0.5757338830735534,
                      "evidence_contribution": 1.0321325065354696,
                      "combined_rank_score": 2.694508135318756
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 5658,
                      "faiss_score": 0.9028811454772949,
                      "faiss_rank": 9,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 61,
                      "sentence": "Eventually consistent systems illustrate how relaxing guarantees can improve scalability.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 2.7626805305480957,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.8435043692588806,
                        "neutral": 0.10410597920417786,
                        "support": 0.052389614284038544
                      },
                      "stance_score": -0.7911147549748421,
                      "evidence_contribution": -2.1855973309983234,
                      "combined_rank_score": 3.6655616760253906
                    }
                  ],
                  "neutral": [
                    {
                      "id": 445,
                      "faiss_score": 0.9056973457336426,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Distributed systems are groups of networked computers which share a common goal for their work.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 1.2563084363937378,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.000767619232647121,
                        "neutral": 0.9977124929428101,
                        "support": 0.001519894110970199
                      },
                      "stance_score": 0.0007522748783230782,
                      "evidence_contribution": 0.0009450892761243557,
                      "combined_rank_score": 2.1620057821273804
                    },
                    {
                      "id": 5597,
                      "faiss_score": 0.9030466675758362,
                      "faiss_rank": 8,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Distributed systems are collections of independent computing components that coordinate their actions through communication in order to achieve a common goal.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 1.2206794023513794,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.0007695626700296998,
                        "neutral": 0.9976724982261658,
                        "support": 0.0015579789178445935
                      },
                      "stance_score": 0.0007884162478148937,
                      "evidence_contribution": 0.0009624034741868015,
                      "combined_rank_score": 2.1237260699272156
                    },
                    {
                      "id": 5646,
                      "faiss_score": 0.9050657749176025,
                      "faiss_rank": 7,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 49,
                      "sentence": "The evolution of distributed systems has been driven by practical needs.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": -0.1592615842819214,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.0006361278356052935,
                        "neutral": 0.9965994954109192,
                        "support": 0.002764445496723056
                      },
                      "stance_score": 0.0021283176611177623,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.7458041906356812
                    }
                  ]
                }
              },
              {
                "subclaim": "Distributed systems improve fault tolerance",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 5614,
                      "faiss_score": 0.9063659906387329,
                      "faiss_rank": 8,
                      "doc_id": "local_distributed_systems_fundamentals.txt",
                      "file_type": ".txt",
                      "position": 17,
                      "sentence": "Replication is commonly used to improve fault tolerance and availability.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                      "primary_category": null,
                      "rerank_score": 1.4775861501693726,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.0022014533169567585,
                        "neutral": 0.5738668441772461,
                        "support": 0.42393165826797485
                      },
                      "stance_score": 0.4217302049510181,
                      "evidence_contribution": 0.6231427099437152,
                      "combined_rank_score": 2.3839521408081055
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 568,
                      "faiss_score": 0.9054936170578003,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 144,
                      "sentence": "There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 4.992817401885986,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.38152015209198,
                        "neutral": 0.6146324276924133,
                        "support": 0.0038474525790661573
                      },
                      "stance_score": -0.3776726995129138,
                      "evidence_contribution": -1.885650826345333,
                      "combined_rank_score": 5.898311018943787
                    }
                  ],
                  "neutral": [
                    {
                      "id": 430,
                      "faiss_score": 0.9204719066619873,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 6,
                      "sentence": "Other typical properties of distributed systems are: The system must tolerate failures in individual computers.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 2.3290367126464844,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.00475419033318758,
                        "neutral": 0.9388123750686646,
                        "support": 0.056433361023664474
                      },
                      "stance_score": 0.051679170690476894,
                      "evidence_contribution": 0.12036268581724485,
                      "combined_rank_score": 3.2495086193084717
                    },
                    {
                      "id": 493,
                      "faiss_score": 0.8894362449645996,
                      "faiss_rank": 19,
                      "doc_id": "wiki_Distributed_computing",
                      "file_type": ".txt",
                      "position": 69,
                      "sentence": "Cell-based architecture has been adopted in some large-scale distributed systems, particularly in cloud-native and high-availability environments, where fault isolation and redundancy are key design considerations.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Distributed_computing",
                      "primary_category": "all articles with unsourced statements",
                      "rerank_score": 2.1658031940460205,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.0025158931966871023,
                        "neutral": 0.9371961951255798,
                        "support": 0.06028788164258003
                      },
                      "stance_score": 0.05777198844589293,
                      "evidence_contribution": 0.1251227571025047,
                      "combined_rank_score": 3.05523943901062
                    },
                    {
                      "id": 3521,
                      "faiss_score": 0.89457106590271,
                      "faiss_rank": 16,
                      "doc_id": "wiki_Fault_tolerance",
                      "file_type": ".txt",
                      "position": 92,
                      "sentence": "Fault-tolerant systems are typically based on the concept of redundancy.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Fault_tolerance",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 1.3589634895324707,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.004133067559450865,
                        "neutral": 0.9230316877365112,
                        "support": 0.07283523678779602
                      },
                      "stance_score": 0.06870216922834516,
                      "evidence_contribution": 0.09336373963300226,
                      "combined_rank_score": 2.2535345554351807
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Increasing dataset size improves model generalization, training stability, and robustness, but data collection is expensive, labeling is costly, noisy data degrades performance, and returns diminish beyond scale.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Increasing dataset size improves model generalization",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 1.0499862145448748,
            "total": 1.0499862145448748
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 1453,
                "faiss_score": 0.911958634853363,
                "faiss_rank": 3,
                "doc_id": "wiki_Regularization_(mathematics)",
                "file_type": ".txt",
                "position": 25,
                "sentence": "By regularizing for time, model complexity can be controlled, improving generalization.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
                "primary_category": "articles with short description",
                "rerank_score": 2.2767534255981445,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.48357275128364563,
                  "neutral": 0.49403151869773865,
                  "support": 0.022395795211195946
                },
                "stance_score": -0.4611769560724497,
                "evidence_contribution": -1.0499862145448748,
                "combined_rank_score": 3.1887120604515076
              },
              {
                "id": 5941,
                "faiss_score": 0.905327558517456,
                "faiss_rank": 5,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 36,
                "sentence": "Smaller or compressed models may generalize better due to implicit regularization, but excessive compression can harm performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -1.098554253578186,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.8726226687431335,
                  "neutral": 0.12322617322206497,
                  "support": 0.00415118969976902
                },
                "stance_score": -0.8684714790433645,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.19322669506072998
              },
              {
                "id": 6213,
                "faiss_score": 0.8910093903541565,
                "faiss_rank": 12,
                "doc_id": "local_ml_training_dynamics.txt",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Large batches provide more accurate gradient estimates and better hardware utilization but can lead to sharp minima or reduced generalization.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                "primary_category": null,
                "rerank_score": -2.1232433319091797,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.9904155731201172,
                  "neutral": 0.00799955241382122,
                  "support": 0.0015847948379814625
                },
                "stance_score": -0.9888307782821357,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.2322339415550232
              }
            ],
            "neutral": [
              {
                "id": 6137,
                "faiss_score": 0.9258404970169067,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 5.599934101104736,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.001429139287211001,
                  "neutral": 0.9253905415534973,
                  "support": 0.07318033277988434
                },
                "stance_score": 0.07175119349267334,
                "evidence_contribution": 0.4018019552345857,
                "combined_rank_score": 6.525774598121643
              },
              {
                "id": 5906,
                "faiss_score": 0.8898252248764038,
                "faiss_rank": 15,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 2.2275302410125732,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.051618073135614395,
                  "neutral": 0.9082584977149963,
                  "support": 0.04012349620461464
                },
                "stance_score": -0.011494576930999756,
                "evidence_contribution": -0.02560451772144745,
                "combined_rank_score": 3.117355465888977
              },
              {
                "id": 1446,
                "faiss_score": 0.8880124092102051,
                "faiss_rank": 17,
                "doc_id": "wiki_Regularization_(mathematics)",
                "file_type": ".txt",
                "position": 18,
                "sentence": "Regularization introduces a penalty for exploring certain regions of the function space used to build the model, which can improve generalization.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
                "primary_category": "articles with short description",
                "rerank_score": 0.6240619421005249,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.028009310364723206,
                  "neutral": 0.9518670439720154,
                  "support": 0.020123686641454697
                },
                "stance_score": -0.007885623723268509,
                "evidence_contribution": -0.004921117655416918,
                "combined_rank_score": 1.51207435131073
              }
            ]
          }
        },
        {
          "subclaim": "Increasing dataset size improves training stability",
          "verdict": "CONTRADICT",
          "controversial": true,
          "strengths": {
            "support": 2.1623834417627696,
            "contradict": 4.41853652691198,
            "total": 6.58091996867475
          },
          "evidence": {
            "supporting": [
              {
                "id": 2613,
                "faiss_score": 0.8831011056900024,
                "faiss_rank": 10,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 208,
                "sentence": "Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 3.4986326694488525,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.004449686035513878,
                  "neutral": 0.5582360029220581,
                  "support": 0.43731430172920227
                },
                "stance_score": 0.4328646156936884,
                "evidence_contribution": 1.5144342859143607,
                "combined_rank_score": 4.381733775138855
              },
              {
                "id": 1416,
                "faiss_score": 0.8797472715377808,
                "faiss_rank": 18,
                "doc_id": "wiki_Overfitting",
                "file_type": ".txt",
                "position": 66,
                "sentence": "Increase the amount of training data: If the model is underfitting due to a lack of data, increasing the amount of training data may help.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                "primary_category": "machine learning",
                "rerank_score": 2.418670892715454,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0035214696545153856,
                  "neutral": 0.7250623106956482,
                  "support": 0.27141618728637695
                },
                "stance_score": 0.26789471763186157,
                "evidence_contribution": 0.6479491558484092,
                "combined_rank_score": 3.298418164253235
              }
            ],
            "contradicting": [
              {
                "id": 6133,
                "faiss_score": 0.8990232944488525,
                "faiss_rank": 3,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 2.8726301193237305,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.7590318322181702,
                  "neutral": 0.217951238155365,
                  "support": 0.023016992956399918
                },
                "stance_score": -0.7360148392617702,
                "evidence_contribution": -2.1142983955325754,
                "combined_rank_score": 3.771653413772583
              },
              {
                "id": 1785,
                "faiss_score": 0.8818594813346863,
                "faiss_rank": 12,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 53,
                "sentence": "Cleaned datasets can increase training efficiency and lead to improved downstream performance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 3.4402031898498535,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.4319572150707245,
                  "neutral": 0.5465472936630249,
                  "support": 0.021495435386896133
                },
                "stance_score": -0.41046177968382835,
                "evidence_contribution": -1.412071923779754,
                "combined_rank_score": 4.32206267118454
              },
              {
                "id": 5906,
                "faiss_score": 0.8880614638328552,
                "faiss_rank": 8,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 2.7191481590270996,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.3397914171218872,
                  "neutral": 0.648522138595581,
                  "support": 0.01168638002127409
                },
                "stance_score": -0.3281050371006131,
                "evidence_contribution": -0.8921662075996504,
                "combined_rank_score": 3.607209622859955
              },
              {
                "id": 6308,
                "faiss_score": 0.8807340860366821,
                "faiss_rank": 14,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 47,
                "sentence": "Another challenge associated with transformers is their reliance on large datasets for effective training.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -1.253514051437378,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.7971049547195435,
                  "neutral": 0.1963687241077423,
                  "support": 0.0065263123251497746
                },
                "stance_score": -0.7905786423943937,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.3727799654006958
              },
              {
                "id": 6352,
                "faiss_score": 0.901928722858429,
                "faiss_rank": 2,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 91,
                "sentence": "Communication overhead, memory constraints, and numerical stability all play important roles in large-scale training.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -4.075257301330566,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.14073993265628815,
                  "neutral": 0.8553996682167053,
                  "support": 0.003860404249280691
                },
                "stance_score": -0.13687952840700746,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.1733285784721375
              },
              {
                "id": 6136,
                "faiss_score": 0.8894740343093872,
                "faiss_rank": 7,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 12,
                "sentence": "Training large models on insufficient or low-quality data can lead to overfitting or wasted capacity.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -4.333821773529053,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.9451566338539124,
                  "neutral": 0.05112721771001816,
                  "support": 0.003716180333867669
                },
                "stance_score": -0.9414404535200447,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.4443477392196655
              }
            ],
            "neutral": [
              {
                "id": 6044,
                "faiss_score": 0.8903433084487915,
                "faiss_rank": 6,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 4,
                "sentence": "Increasing the number of parameters, the amount of training data, and the total compute used during training has been shown to produce consistent improvements in performance across many benchmarks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 2.333214282989502,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.0027996383141726255,
                  "neutral": 0.9180957674980164,
                  "support": 0.07910455763339996
                },
                "stance_score": 0.07630491931922734,
                "evidence_contribution": 0.1780357276179828,
                "combined_rank_score": 3.2235575914382935
              },
              {
                "id": 6137,
                "faiss_score": 0.9186047315597534,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 1.8626278638839722,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.0020000473596155643,
                  "neutral": 0.9066423773765564,
                  "support": 0.09135754406452179
                },
                "stance_score": 0.08935749670490623,
                "evidence_contribution": 0.16643976320947856,
                "combined_rank_score": 2.7812325954437256
              }
            ]
          }
        },
        {
          "subclaim": "Increasing dataset size improves robustness",
          "verdict": "CONTRADICT",
          "controversial": true,
          "strengths": {
            "support": 0.48852589122443124,
            "contradict": 0.9534001303478744,
            "total": 1.4419260215723058
          },
          "evidence": {
            "supporting": [
              {
                "id": 6137,
                "faiss_score": 0.9264270067214966,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 3.0166993141174316,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.004063395783305168,
                  "neutral": 0.8299326300621033,
                  "support": 0.16600392758846283
                },
                "stance_score": 0.16194053180515766,
                "evidence_contribution": 0.48852589122443124,
                "combined_rank_score": 3.9431263208389282
              },
              {
                "id": 2613,
                "faiss_score": 0.8810292482376099,
                "faiss_rank": 10,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 208,
                "sentence": "Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": -3.591813564300537,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.005783679895102978,
                  "neutral": 0.6264408826828003,
                  "support": 0.36777544021606445
                },
                "stance_score": 0.3619917603209615,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.7107843160629272
              }
            ],
            "contradicting": [
              {
                "id": 6133,
                "faiss_score": 0.8859886527061462,
                "faiss_rank": 5,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 1.7032674551010132,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.423473984003067,
                  "neutral": 0.5593301057815552,
                  "support": 0.01719595119357109
                },
                "stance_score": -0.4062780328094959,
                "evidence_contribution": -0.6920001510068761,
                "combined_rank_score": 2.5892561078071594
              },
              {
                "id": 6147,
                "faiss_score": 0.8822609782218933,
                "faiss_rank": 7,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 23,
                "sentence": "For example, increasing model size without increasing data may yield limited benefits, while increasing data without sufficient model capacity may fail to exploit the additional information.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 0.8042117357254028,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.33419978618621826,
                  "neutral": 0.6566392183303833,
                  "support": 0.009161033667623997
                },
                "stance_score": -0.32503875251859426,
                "evidence_contribution": -0.26139997934099835,
                "combined_rank_score": 1.6864727139472961
              },
              {
                "id": 6161,
                "faiss_score": 0.8941830396652222,
                "faiss_rank": 2,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 37,
                "sentence": "Scaling affects robustness and generalization in nontrivial ways.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -3.672755241394043,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.7847391366958618,
                  "neutral": 0.2066025733947754,
                  "support": 0.008658240549266338
                },
                "stance_score": -0.7760808961465955,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.778572201728821
              },
              {
                "id": 5906,
                "faiss_score": 0.8823684453964233,
                "faiss_rank": 6,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -3.7619071006774902,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.11878746002912521,
                  "neutral": 0.869875431060791,
                  "support": 0.011337077245116234
                },
                "stance_score": -0.10745038278400898,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.879538655281067
              },
              {
                "id": 6374,
                "faiss_score": 0.8873706459999084,
                "faiss_rank": 3,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 113,
                "sentence": "Robustness is another area of concern.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -4.382403373718262,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.2938911020755768,
                  "neutral": 0.7046731114387512,
                  "support": 0.0014357101172208786
                },
                "stance_score": -0.2924553919583559,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.4950327277183533
              }
            ],
            "neutral": [
              {
                "id": 6007,
                "faiss_score": 0.8798772096633911,
                "faiss_rank": 13,
                "doc_id": "local_ml_evaluation_and_generalization.txt",
                "file_type": ".txt",
                "position": 33,
                "sentence": "Robust evaluation requires understanding the provenance and limitations of datasets.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                "primary_category": null,
                "rerank_score": -2.3191099166870117,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.009657689370214939,
                  "neutral": 0.9895644187927246,
                  "support": 0.000777834327891469
                },
                "stance_score": -0.00887985504232347,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.4392327070236206
              },
              {
                "id": 1785,
                "faiss_score": 0.8803674578666687,
                "faiss_rank": 12,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 53,
                "sentence": "Cleaned datasets can increase training efficiency and lead to improved downstream performance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -2.49263072013855,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.05982460454106331,
                  "neutral": 0.8901634216308594,
                  "support": 0.05001198872923851
                },
                "stance_score": -0.009812615811824799,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.612263262271881
              },
              {
                "id": 1394,
                "faiss_score": 0.8871293067932129,
                "faiss_rank": 4,
                "doc_id": "wiki_Overfitting",
                "file_type": ".txt",
                "position": 44,
                "sentence": "The optimal function usually needs verification on bigger or completely new datasets.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                "primary_category": "machine learning",
                "rerank_score": -6.338343143463135,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.010466142557561398,
                  "neutral": 0.9807940721511841,
                  "support": 0.008739816956222057
                },
                "stance_score": -0.0017263256013393402,
                "evidence_contribution": -0.0,
                "combined_rank_score": -5.451213836669922
              }
            ]
          }
        },
        {
          "subclaim": "Data collection is expensive",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 1.8207431373586895,
            "contradict": 0.0,
            "total": 1.8207431373586895
          },
          "evidence": {
            "supporting": [
              {
                "id": 6500,
                "faiss_score": 0.8686016201972961,
                "faiss_rank": 1,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 29,
                "sentence": "Accessing data from memory is often more expensive in terms of time and energy than performing arithmetic operations.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": 3.566348075866699,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0024147434160113335,
                  "neutral": 0.5592390894889832,
                  "support": 0.4383462071418762
                },
                "stance_score": 0.4359314637258649,
                "evidence_contribution": 1.554683336868492,
                "combined_rank_score": 4.434949696063995
              },
              {
                "id": 1387,
                "faiss_score": 0.8628299832344055,
                "faiss_rank": 2,
                "doc_id": "wiki_Overfitting",
                "file_type": ".txt",
                "position": 37,
                "sentence": "Other negative consequences include: A function that is overfitted is likely to request more information about each item in the validation dataset than does the optimal function; gathering this additional unneeded data can be expensive or error-prone, especially if each individual piece of information must be gathered by human observation and manual data entry.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                "primary_category": "machine learning",
                "rerank_score": 0.6261616945266724,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0022822844330221415,
                  "neutral": 0.5705295205116272,
                  "support": 0.4271881878376007
                },
                "stance_score": 0.42490590340457857,
                "evidence_contribution": 0.2660598004901975,
                "combined_rank_score": 1.4889916777610779
              },
              {
                "id": 6354,
                "faiss_score": 0.8548440337181091,
                "faiss_rank": 10,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 93,
                "sentence": "Transformer-based models can be computationally expensive at inference time, particularly when generating long outputs or processing large batches.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -5.124258518218994,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.007638900075107813,
                  "neutral": 0.8466179370880127,
                  "support": 0.14574311673641205
                },
                "stance_score": 0.13810421666130424,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.269414484500885
              },
              {
                "id": 6104,
                "faiss_score": 0.8590776920318604,
                "faiss_rank": 7,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 64,
                "sentence": "Human evaluation is often necessary but is expensive and subjective.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -5.9997148513793945,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.015658235177397728,
                  "neutral": 0.3812158405780792,
                  "support": 0.6031259298324585
                },
                "stance_score": 0.5874676946550608,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.140637159347534
              },
              {
                "id": 5950,
                "faiss_score": 0.8499007821083069,
                "faiss_rank": 19,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 45,
                "sentence": "Data preprocessing, communication overhead, and orchestration costs can dominate overall performance.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -8.503372192382812,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.003731388133019209,
                  "neutral": 0.6884207725524902,
                  "support": 0.30784788727760315
                },
                "stance_score": 0.30411649914458394,
                "evidence_contribution": 0.0,
                "combined_rank_score": -7.653471410274506
              },
              {
                "id": 6096,
                "faiss_score": 0.8513523936271667,
                "faiss_rank": 17,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 56,
                "sentence": "Inference costs are also significant, particularly for interactive applications.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -9.917675018310547,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.021079104393720627,
                  "neutral": 0.7834919095039368,
                  "support": 0.1954289972782135
                },
                "stance_score": 0.17434989288449287,
                "evidence_contribution": 0.0,
                "combined_rank_score": -9.06632262468338
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 6138,
                "faiss_score": 0.8567302227020264,
                "faiss_rank": 8,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "This observation has motivated large-scale data collection and curation efforts, as well as synthetic data generation in some settings.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -6.279105186462402,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0021278501953929663,
                  "neutral": 0.9968488812446594,
                  "support": 0.0010233029024675488
                },
                "stance_score": -0.0011045472929254174,
                "evidence_contribution": -0.0,
                "combined_rank_score": -5.422374963760376
              },
              {
                "id": 5482,
                "faiss_score": 0.8522249460220337,
                "faiss_rank": 16,
                "doc_id": "local_bio_ethics_biotech.txt",
                "file_type": ".txt",
                "position": 28,
                "sentence": "Advanced biotechnological interventions are often expensive and resource-intensive.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_ethics_biotech.txt",
                "primary_category": null,
                "rerank_score": -6.943849563598633,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.02766532637178898,
                  "neutral": 0.8897552490234375,
                  "support": 0.08257947117090225
                },
                "stance_score": 0.054914144799113274,
                "evidence_contribution": 0.0,
                "combined_rank_score": -6.091624617576599
              },
              {
                "id": 6170,
                "faiss_score": 0.8536031246185303,
                "faiss_rank": 13,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 46,
                "sentence": "Experiments become more expensive and slower to iterate, reducing the ability to explore many alternatives.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -7.887742042541504,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.002640694845467806,
                  "neutral": 0.9025747179985046,
                  "support": 0.09478458762168884
                },
                "stance_score": 0.09214389277622104,
                "evidence_contribution": 0.0,
                "combined_rank_score": -7.034138917922974
              }
            ]
          }
        },
        {
          "subclaim": "Labeling is costly",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 1.8008435411998125,
            "contradict": 0.0,
            "total": 1.8008435411998125
          },
          "evidence": {
            "supporting": [
              {
                "id": 2318,
                "faiss_score": 0.8671115636825562,
                "faiss_rank": 1,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 249,
                "sentence": "This approach directly quantifies predictive performance but may be impractical when labels are delayed or costly to obtain.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": 4.585840702056885,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.005023222416639328,
                  "neutral": 0.5972570776939392,
                  "support": 0.39771971106529236
                },
                "stance_score": 0.39269648864865303,
                "evidence_contribution": 1.8008435411998125,
                "combined_rank_score": 5.452952265739441
              },
              {
                "id": 6104,
                "faiss_score": 0.8473619818687439,
                "faiss_rank": 4,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 64,
                "sentence": "Human evaluation is often necessary but is expensive and subjective.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -9.037809371948242,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.004817895591259003,
                  "neutral": 0.11614442616701126,
                  "support": 0.8790376782417297
                },
                "stance_score": 0.8742197826504707,
                "evidence_contribution": 0.0,
                "combined_rank_score": -8.190447390079498
              },
              {
                "id": 5482,
                "faiss_score": 0.8428661227226257,
                "faiss_rank": 5,
                "doc_id": "local_bio_ethics_biotech.txt",
                "file_type": ".txt",
                "position": 28,
                "sentence": "Advanced biotechnological interventions are often expensive and resource-intensive.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_ethics_biotech.txt",
                "primary_category": null,
                "rerank_score": -9.244827270507812,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.04173171892762184,
                  "neutral": 0.8021056652069092,
                  "support": 0.1561625897884369
                },
                "stance_score": 0.11443087086081505,
                "evidence_contribution": 0.0,
                "combined_rank_score": -8.401961147785187
              },
              {
                "id": 6354,
                "faiss_score": 0.8329081535339355,
                "faiss_rank": 11,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 93,
                "sentence": "Transformer-based models can be computationally expensive at inference time, particularly when generating long outputs or processing large batches.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -10.136804580688477,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.007945132441818714,
                  "neutral": 0.6579692363739014,
                  "support": 0.3340855538845062
                },
                "stance_score": 0.3261404214426875,
                "evidence_contribution": 0.0,
                "combined_rank_score": -9.303896427154541
              }
            ],
            "contradicting": [
              {
                "id": 202,
                "faiss_score": 0.8283368945121765,
                "faiss_rank": 14,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 93,
                "sentence": "In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -0.7225422859191895,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.7162351608276367,
                  "neutral": 0.24769654870033264,
                  "support": 0.03606829047203064
                },
                "stance_score": -0.6801668703556061,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.10579460859298706
              }
            ],
            "neutral": [
              {
                "id": 2426,
                "faiss_score": 0.8386695384979248,
                "faiss_rank": 7,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 21,
                "sentence": "This is an important benefit because unlabeled data is more abundant than the labeled data.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": -3.514857530593872,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.032294243574142456,
                  "neutral": 0.9599062204360962,
                  "support": 0.00779948104172945
                },
                "stance_score": -0.024494762532413006,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.6761879920959473
              },
              {
                "id": 2900,
                "faiss_score": 0.8363354206085205,
                "faiss_rank": 9,
                "doc_id": "wiki_Self-supervised_learning",
                "file_type": ".txt",
                "position": 9,
                "sentence": "This strategy reduces reliance on manual labeling while helping maintain long-term model performance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Self-supervised_learning",
                "primary_category": "machine learning",
                "rerank_score": -5.754866600036621,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0032300609163939953,
                  "neutral": 0.9925988912582397,
                  "support": 0.00417111162096262
                },
                "stance_score": 0.0009410507045686245,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.918531179428101
              },
              {
                "id": 2891,
                "faiss_score": 0.8278757333755493,
                "faiss_rank": 18,
                "doc_id": "wiki_Self-supervised_learning",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Pseudo-labels are automatically generated labels that a model assigns to unlabeled data based on its own predictions.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Self-supervised_learning",
                "primary_category": "machine learning",
                "rerank_score": -8.949430465698242,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.00467250170186162,
                  "neutral": 0.9935052394866943,
                  "support": 0.001822242047637701
                },
                "stance_score": -0.002850259654223919,
                "evidence_contribution": -0.0,
                "combined_rank_score": -8.121554732322693
              }
            ]
          }
        },
        {
          "subclaim": "Noisy data degrades performance",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [
              {
                "id": 4087,
                "faiss_score": 0.8815248608589172,
                "faiss_rank": 7,
                "doc_id": "wiki_Information_theory",
                "file_type": ".txt",
                "position": 81,
                "sentence": "However, channels often fail to produce exact reconstruction of a signal; noise, periods of silence, and other forms of signal corruption often degrade quality.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Information_theory",
                "primary_category": "all articles needing additional references",
                "rerank_score": -0.8287366032600403,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.003933201543986797,
                  "neutral": 0.49653515219688416,
                  "support": 0.49953165650367737
                },
                "stance_score": 0.49559845495969057,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.05278825759887695
              },
              {
                "id": 5808,
                "faiss_score": 0.8804681897163391,
                "faiss_rank": 8,
                "doc_id": "local_math_information_theory_and_learning.txt",
                "file_type": ".txt",
                "position": 56,
                "sentence": "Even with infinite computation and perfect optimization, performance is bounded by noise and ambiguity.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_information_theory_and_learning.txt",
                "primary_category": null,
                "rerank_score": -3.4648706912994385,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.00144195684697479,
                  "neutral": 0.7477361559867859,
                  "support": 0.25082188844680786
                },
                "stance_score": 0.24937993159983307,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.5844025015830994
              },
              {
                "id": 6005,
                "faiss_score": 0.8975870013237,
                "faiss_rank": 2,
                "doc_id": "local_ml_evaluation_and_generalization.txt",
                "file_type": ".txt",
                "position": 31,
                "sentence": "Noisy labels, missing values, and biased sampling affect both training and evaluation.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                "primary_category": null,
                "rerank_score": -4.529215335845947,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.001983974128961563,
                  "neutral": 0.5871595144271851,
                  "support": 0.41085657477378845
                },
                "stance_score": 0.4088726006448269,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.6316283345222473
              },
              {
                "id": 5784,
                "faiss_score": 0.8904547095298767,
                "faiss_rank": 4,
                "doc_id": "local_math_information_theory_and_learning.txt",
                "file_type": ".txt",
                "position": 32,
                "sentence": "On one hand, noise increases entropy and makes prediction harder.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_information_theory_and_learning.txt",
                "primary_category": null,
                "rerank_score": -6.483351707458496,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.004294100683182478,
                  "neutral": 0.8367171287536621,
                  "support": 0.15898878872394562
                },
                "stance_score": 0.15469468804076314,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.592896997928619
              }
            ],
            "contradicting": [
              {
                "id": 5854,
                "faiss_score": 0.884116530418396,
                "faiss_rank": 6,
                "doc_id": "local_math_optimization_and_convergence.txt",
                "file_type": ".txt",
                "position": 23,
                "sentence": "Noise in optimization can be both beneficial and harmful.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                "primary_category": null,
                "rerank_score": -5.094203948974609,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.6732237339019775,
                  "neutral": 0.28176796436309814,
                  "support": 0.04500836506485939
                },
                "stance_score": -0.6282153688371181,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.210087418556213
              },
              {
                "id": 1785,
                "faiss_score": 0.8795800805091858,
                "faiss_rank": 9,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 53,
                "sentence": "Cleaned datasets can increase training efficiency and lead to improved downstream performance.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -6.785529136657715,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.18365582823753357,
                  "neutral": 0.8093551397323608,
                  "support": 0.006989020388573408
                },
                "stance_score": -0.17666680784896016,
                "evidence_contribution": -0.0,
                "combined_rank_score": -5.905949056148529
              }
            ],
            "neutral": [
              {
                "id": 5998,
                "faiss_score": 0.9058985114097595,
                "faiss_rank": 1,
                "doc_id": "local_ml_evaluation_and_generalization.txt",
                "file_type": ".txt",
                "position": 24,
                "sentence": "When deployment data differs from training data, performance may degrade unpredictably.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                "primary_category": null,
                "rerank_score": 0.8202033042907715,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.008945327252149582,
                  "neutral": 0.9763994812965393,
                  "support": 0.014655262231826782
                },
                "stance_score": 0.0057099349796772,
                "evidence_contribution": 0.004683307537616699,
                "combined_rank_score": 1.726101815700531
              },
              {
                "id": 6215,
                "faiss_score": 0.8717062473297119,
                "faiss_rank": 15,
                "doc_id": "local_ml_training_dynamics.txt",
                "file_type": ".txt",
                "position": 23,
                "sentence": "Noise in training dynamics arises from multiple sources, including stochastic gradient estimation, data variability, and numerical precision.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                "primary_category": null,
                "rerank_score": -3.703622341156006,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0024312855675816536,
                  "neutral": 0.9882625937461853,
                  "support": 0.009306076914072037
                },
                "stance_score": 0.006874791346490383,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.831916093826294
              },
              {
                "id": 1445,
                "faiss_score": 0.8750660419464111,
                "faiss_rank": 14,
                "doc_id": "wiki_Regularization_(mathematics)",
                "file_type": ".txt",
                "position": 17,
                "sentence": "Typically in learning problems, only a subset of input data and labels are available, measured with some noise.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
                "primary_category": "articles with short description",
                "rerank_score": -6.994556427001953,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.006035220809280872,
                  "neutral": 0.9910310506820679,
                  "support": 0.0029336868319660425
                },
                "stance_score": -0.00310153397731483,
                "evidence_contribution": -0.0,
                "combined_rank_score": -6.119490385055542
              }
            ]
          }
        },
        {
          "subclaim": "Returns diminish beyond scale",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 1.531920996780304,
            "contradict": 0.0,
            "total": 1.531920996780304
          },
          "evidence": {
            "supporting": [
              {
                "id": 6178,
                "faiss_score": 0.8914539217948914,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 54,
                "sentence": "While scaling has delivered consistent gains, it may encounter diminishing returns or external constraints that necessitate new approaches.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 4.174654960632324,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.04899430647492409,
                  "neutral": 0.5350538492202759,
                  "support": 0.41595181822776794
                },
                "stance_score": 0.36695751175284386,
                "evidence_contribution": 1.531920996780304,
                "combined_rank_score": 5.066108882427216
              },
              {
                "id": 6133,
                "faiss_score": 0.8767734169960022,
                "faiss_rank": 2,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 9,
                "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -8.22350025177002,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.002733273431658745,
                  "neutral": 0.3507341742515564,
                  "support": 0.6465325355529785
                },
                "stance_score": 0.6437992621213198,
                "evidence_contribution": 0.0,
                "combined_rank_score": -7.346726834774017
              },
              {
                "id": 6414,
                "faiss_score": 0.8572297096252441,
                "faiss_rank": 5,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 153,
                "sentence": "Although larger models often support longer contexts, this approach scales poorly due to the quadratic cost of attention.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -10.639811515808105,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.006133015733212233,
                  "neutral": 0.837619423866272,
                  "support": 0.15624749660491943
                },
                "stance_score": 0.1501144808717072,
                "evidence_contribution": 0.0,
                "combined_rank_score": -9.782581806182861
              },
              {
                "id": 5633,
                "faiss_score": 0.8463342189788818,
                "faiss_rank": 20,
                "doc_id": "local_distributed_systems_fundamentals.txt",
                "file_type": ".txt",
                "position": 36,
                "sentence": "However, scaling introduces coordination overhead that can limit achievable gains.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\distributed_systems_fundamentals.txt",
                "primary_category": null,
                "rerank_score": -10.806743621826172,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.0371006615459919,
                  "neutral": 0.7580291032791138,
                  "support": 0.20487017929553986
                },
                "stance_score": 0.16776951774954796,
                "evidence_contribution": 0.0,
                "combined_rank_score": -9.96040940284729
              }
            ],
            "contradicting": [
              {
                "id": 6157,
                "faiss_score": 0.8606178760528564,
                "faiss_rank": 4,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 33,
                "sentence": "Inference efficiency is another scaling concern.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -9.952547073364258,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.13407321274280548,
                  "neutral": 0.8408279418945312,
                  "support": 0.025098849087953568
                },
                "stance_score": -0.10897436365485191,
                "evidence_contribution": -0.0,
                "combined_rank_score": -9.091929197311401
              },
              {
                "id": 6161,
                "faiss_score": 0.848158061504364,
                "faiss_rank": 17,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 37,
                "sentence": "Scaling affects robustness and generalization in nontrivial ways.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -10.488603591918945,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.1861957162618637,
                  "neutral": 0.7812544107437134,
                  "support": 0.03254988044500351
                },
                "stance_score": -0.1536458358168602,
                "evidence_contribution": -0.0,
                "combined_rank_score": -9.640445530414581
              }
            ],
            "neutral": [
              {
                "id": 6186,
                "faiss_score": 0.8617356419563293,
                "faiss_rank": 3,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 62,
                "sentence": "Ultimately, scaling is a powerful but blunt tool.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -9.478944778442383,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.025735752657055855,
                  "neutral": 0.8864394426345825,
                  "support": 0.0878247618675232
                },
                "stance_score": 0.06208900921046734,
                "evidence_contribution": 0.0,
                "combined_rank_score": -8.617209136486053
              },
              {
                "id": 6146,
                "faiss_score": 0.8530365228652954,
                "faiss_rank": 11,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 22,
                "sentence": "One important implication of scaling laws is that suboptimal allocation of resources leads to inefficiency.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -10.16385269165039,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.06512042880058289,
                  "neutral": 0.8837094306945801,
                  "support": 0.051170170307159424
                },
                "stance_score": -0.013950258493423462,
                "evidence_contribution": -0.0,
                "combined_rank_score": -9.310816168785095
              },
              {
                "id": 6129,
                "faiss_score": 0.8517745137214661,
                "faiss_rank": 13,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 5,
                "sentence": "However, scaling is not a single-dimensional concept.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -10.469619750976562,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.044353026896715164,
                  "neutral": 0.9474198818206787,
                  "support": 0.008227115496993065
                },
                "stance_score": -0.0361259113997221,
                "evidence_contribution": -0.0,
                "combined_rank_score": -9.617845237255096
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Data collection is expensive",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6500,
                      "faiss_score": 0.8686016201972961,
                      "faiss_rank": 1,
                      "doc_id": "local_physics_computation.txt",
                      "file_type": ".txt",
                      "position": 29,
                      "sentence": "Accessing data from memory is often more expensive in terms of time and energy than performing arithmetic operations.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                      "primary_category": null,
                      "rerank_score": 3.566348075866699,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0024147434160113335,
                        "neutral": 0.5592390894889832,
                        "support": 0.4383462071418762
                      },
                      "stance_score": 0.4359314637258649,
                      "evidence_contribution": 1.554683336868492,
                      "combined_rank_score": 4.434949696063995
                    },
                    {
                      "id": 1387,
                      "faiss_score": 0.8628299832344055,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Overfitting",
                      "file_type": ".txt",
                      "position": 37,
                      "sentence": "Other negative consequences include: A function that is overfitted is likely to request more information about each item in the validation dataset than does the optimal function; gathering this additional unneeded data can be expensive or error-prone, especially if each individual piece of information must be gathered by human observation and manual data entry.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                      "primary_category": "machine learning",
                      "rerank_score": 0.6261616945266724,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0022822844330221415,
                        "neutral": 0.5705295205116272,
                        "support": 0.4271881878376007
                      },
                      "stance_score": 0.42490590340457857,
                      "evidence_contribution": 0.2660598004901975,
                      "combined_rank_score": 1.4889916777610779
                    },
                    {
                      "id": 6354,
                      "faiss_score": 0.8548440337181091,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 93,
                      "sentence": "Transformer-based models can be computationally expensive at inference time, particularly when generating long outputs or processing large batches.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -5.124258518218994,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.007638900075107813,
                        "neutral": 0.8466179370880127,
                        "support": 0.14574311673641205
                      },
                      "stance_score": 0.13810421666130424,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -4.269414484500885
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 6138,
                      "faiss_score": 0.8567302227020264,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "This observation has motivated large-scale data collection and curation efforts, as well as synthetic data generation in some settings.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -6.279105186462402,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0021278501953929663,
                        "neutral": 0.9968488812446594,
                        "support": 0.0010233029024675488
                      },
                      "stance_score": -0.0011045472929254174,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -5.422374963760376
                    },
                    {
                      "id": 5482,
                      "faiss_score": 0.8522249460220337,
                      "faiss_rank": 16,
                      "doc_id": "local_bio_ethics_biotech.txt",
                      "file_type": ".txt",
                      "position": 28,
                      "sentence": "Advanced biotechnological interventions are often expensive and resource-intensive.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_ethics_biotech.txt",
                      "primary_category": null,
                      "rerank_score": -6.943849563598633,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.02766532637178898,
                        "neutral": 0.8897552490234375,
                        "support": 0.08257947117090225
                      },
                      "stance_score": 0.054914144799113274,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -6.091624617576599
                    },
                    {
                      "id": 6170,
                      "faiss_score": 0.8536031246185303,
                      "faiss_rank": 13,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 46,
                      "sentence": "Experiments become more expensive and slower to iterate, reducing the ability to explore many alternatives.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -7.887742042541504,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.002640694845467806,
                        "neutral": 0.9025747179985046,
                        "support": 0.09478458762168884
                      },
                      "stance_score": 0.09214389277622104,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -7.034138917922974
                    }
                  ]
                }
              },
              {
                "subclaim": "Labeling is costly",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 2318,
                      "faiss_score": 0.8671115636825562,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Artificial_neural_network",
                      "file_type": ".txt",
                      "position": 249,
                      "sentence": "This approach directly quantifies predictive performance but may be impractical when labels are delayed or costly to obtain.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                      "primary_category": "neural networks",
                      "rerank_score": 4.585840702056885,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.005023222416639328,
                        "neutral": 0.5972570776939392,
                        "support": 0.39771971106529236
                      },
                      "stance_score": 0.39269648864865303,
                      "evidence_contribution": 1.8008435411998125,
                      "combined_rank_score": 5.452952265739441
                    },
                    {
                      "id": 6104,
                      "faiss_score": 0.8473619818687439,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 64,
                      "sentence": "Human evaluation is often necessary but is expensive and subjective.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -9.037809371948242,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.004817895591259003,
                        "neutral": 0.11614442616701126,
                        "support": 0.8790376782417297
                      },
                      "stance_score": 0.8742197826504707,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -8.190447390079498
                    },
                    {
                      "id": 5482,
                      "faiss_score": 0.8428661227226257,
                      "faiss_rank": 5,
                      "doc_id": "local_bio_ethics_biotech.txt",
                      "file_type": ".txt",
                      "position": 28,
                      "sentence": "Advanced biotechnological interventions are often expensive and resource-intensive.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_ethics_biotech.txt",
                      "primary_category": null,
                      "rerank_score": -9.244827270507812,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.04173171892762184,
                        "neutral": 0.8021056652069092,
                        "support": 0.1561625897884369
                      },
                      "stance_score": 0.11443087086081505,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -8.401961147785187
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 202,
                      "faiss_score": 0.8283368945121765,
                      "faiss_rank": 14,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 93,
                      "sentence": "In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -0.7225422859191895,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.7162351608276367,
                        "neutral": 0.24769654870033264,
                        "support": 0.03606829047203064
                      },
                      "stance_score": -0.6801668703556061,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.10579460859298706
                    }
                  ],
                  "neutral": [
                    {
                      "id": 2426,
                      "faiss_score": 0.8386695384979248,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "This is an important benefit because unlabeled data is more abundant than the labeled data.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": -3.514857530593872,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.032294243574142456,
                        "neutral": 0.9599062204360962,
                        "support": 0.00779948104172945
                      },
                      "stance_score": -0.024494762532413006,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -2.6761879920959473
                    },
                    {
                      "id": 2900,
                      "faiss_score": 0.8363354206085205,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Self-supervised_learning",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "This strategy reduces reliance on manual labeling while helping maintain long-term model performance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Self-supervised_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -5.754866600036621,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0032300609163939953,
                        "neutral": 0.9925988912582397,
                        "support": 0.00417111162096262
                      },
                      "stance_score": 0.0009410507045686245,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -4.918531179428101
                    },
                    {
                      "id": 2891,
                      "faiss_score": 0.8278757333755493,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Self-supervised_learning",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Pseudo-labels are automatically generated labels that a model assigns to unlabeled data based on its own predictions.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Self-supervised_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -8.949430465698242,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.00467250170186162,
                        "neutral": 0.9935052394866943,
                        "support": 0.001822242047637701
                      },
                      "stance_score": -0.002850259654223919,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -8.121554732322693
                    }
                  ]
                }
              },
              {
                "subclaim": "Returns diminish beyond scale",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6178,
                      "faiss_score": 0.8914539217948914,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 54,
                      "sentence": "While scaling has delivered consistent gains, it may encounter diminishing returns or external constraints that necessitate new approaches.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 4.174654960632324,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.04899430647492409,
                        "neutral": 0.5350538492202759,
                        "support": 0.41595181822776794
                      },
                      "stance_score": 0.36695751175284386,
                      "evidence_contribution": 1.531920996780304,
                      "combined_rank_score": 5.066108882427216
                    },
                    {
                      "id": 6133,
                      "faiss_score": 0.8767734169960022,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -8.22350025177002,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.002733273431658745,
                        "neutral": 0.3507341742515564,
                        "support": 0.6465325355529785
                      },
                      "stance_score": 0.6437992621213198,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -7.346726834774017
                    },
                    {
                      "id": 6414,
                      "faiss_score": 0.8572297096252441,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 153,
                      "sentence": "Although larger models often support longer contexts, this approach scales poorly due to the quadratic cost of attention.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -10.639811515808105,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.006133015733212233,
                        "neutral": 0.837619423866272,
                        "support": 0.15624749660491943
                      },
                      "stance_score": 0.1501144808717072,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -9.782581806182861
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6157,
                      "faiss_score": 0.8606178760528564,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 33,
                      "sentence": "Inference efficiency is another scaling concern.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -9.952547073364258,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.13407321274280548,
                        "neutral": 0.8408279418945312,
                        "support": 0.025098849087953568
                      },
                      "stance_score": -0.10897436365485191,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -9.091929197311401
                    },
                    {
                      "id": 6161,
                      "faiss_score": 0.848158061504364,
                      "faiss_rank": 17,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 37,
                      "sentence": "Scaling affects robustness and generalization in nontrivial ways.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -10.488603591918945,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.1861957162618637,
                        "neutral": 0.7812544107437134,
                        "support": 0.03254988044500351
                      },
                      "stance_score": -0.1536458358168602,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -9.640445530414581
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6186,
                      "faiss_score": 0.8617356419563293,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 62,
                      "sentence": "Ultimately, scaling is a powerful but blunt tool.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -9.478944778442383,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.025735752657055855,
                        "neutral": 0.8864394426345825,
                        "support": 0.0878247618675232
                      },
                      "stance_score": 0.06208900921046734,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -8.617209136486053
                    },
                    {
                      "id": 6146,
                      "faiss_score": 0.8530365228652954,
                      "faiss_rank": 11,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 22,
                      "sentence": "One important implication of scaling laws is that suboptimal allocation of resources leads to inefficiency.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -10.16385269165039,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.06512042880058289,
                        "neutral": 0.8837094306945801,
                        "support": 0.051170170307159424
                      },
                      "stance_score": -0.013950258493423462,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -9.310816168785095
                    },
                    {
                      "id": 6129,
                      "faiss_score": 0.8517745137214661,
                      "faiss_rank": 13,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 5,
                      "sentence": "However, scaling is not a single-dimensional concept.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -10.469619750976562,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.044353026896715164,
                        "neutral": 0.9474198818206787,
                        "support": 0.008227115496993065
                      },
                      "stance_score": -0.0361259113997221,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -9.617845237255096
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Increasing dataset size improves model generalization",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 1453,
                      "faiss_score": 0.911958634853363,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Regularization_(mathematics)",
                      "file_type": ".txt",
                      "position": 25,
                      "sentence": "By regularizing for time, model complexity can be controlled, improving generalization.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
                      "primary_category": "articles with short description",
                      "rerank_score": 2.2767534255981445,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.48357275128364563,
                        "neutral": 0.49403151869773865,
                        "support": 0.022395795211195946
                      },
                      "stance_score": -0.4611769560724497,
                      "evidence_contribution": -1.0499862145448748,
                      "combined_rank_score": 3.1887120604515076
                    },
                    {
                      "id": 5941,
                      "faiss_score": 0.905327558517456,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 36,
                      "sentence": "Smaller or compressed models may generalize better due to implicit regularization, but excessive compression can harm performance.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -1.098554253578186,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.8726226687431335,
                        "neutral": 0.12322617322206497,
                        "support": 0.00415118969976902
                      },
                      "stance_score": -0.8684714790433645,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.19322669506072998
                    },
                    {
                      "id": 6213,
                      "faiss_score": 0.8910093903541565,
                      "faiss_rank": 12,
                      "doc_id": "local_ml_training_dynamics.txt",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Large batches provide more accurate gradient estimates and better hardware utilization but can lead to sharp minima or reduced generalization.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                      "primary_category": null,
                      "rerank_score": -2.1232433319091797,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.9904155731201172,
                        "neutral": 0.00799955241382122,
                        "support": 0.0015847948379814625
                      },
                      "stance_score": -0.9888307782821357,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.2322339415550232
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6137,
                      "faiss_score": 0.9258404970169067,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 5.599934101104736,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.001429139287211001,
                        "neutral": 0.9253905415534973,
                        "support": 0.07318033277988434
                      },
                      "stance_score": 0.07175119349267334,
                      "evidence_contribution": 0.4018019552345857,
                      "combined_rank_score": 6.525774598121643
                    },
                    {
                      "id": 5906,
                      "faiss_score": 0.8898252248764038,
                      "faiss_rank": 15,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 1,
                      "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 2.2275302410125732,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.051618073135614395,
                        "neutral": 0.9082584977149963,
                        "support": 0.04012349620461464
                      },
                      "stance_score": -0.011494576930999756,
                      "evidence_contribution": -0.02560451772144745,
                      "combined_rank_score": 3.117355465888977
                    },
                    {
                      "id": 1446,
                      "faiss_score": 0.8880124092102051,
                      "faiss_rank": 17,
                      "doc_id": "wiki_Regularization_(mathematics)",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "Regularization introduces a penalty for exploring certain regions of the function space used to build the model, which can improve generalization.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
                      "primary_category": "articles with short description",
                      "rerank_score": 0.6240619421005249,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.028009310364723206,
                        "neutral": 0.9518670439720154,
                        "support": 0.020123686641454697
                      },
                      "stance_score": -0.007885623723268509,
                      "evidence_contribution": -0.004921117655416918,
                      "combined_rank_score": 1.51207435131073
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing dataset size improves training stability",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 2613,
                      "faiss_score": 0.8831011056900024,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 208,
                      "sentence": "Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": 3.4986326694488525,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.004449686035513878,
                        "neutral": 0.5582360029220581,
                        "support": 0.43731430172920227
                      },
                      "stance_score": 0.4328646156936884,
                      "evidence_contribution": 1.5144342859143607,
                      "combined_rank_score": 4.381733775138855
                    },
                    {
                      "id": 1416,
                      "faiss_score": 0.8797472715377808,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Overfitting",
                      "file_type": ".txt",
                      "position": 66,
                      "sentence": "Increase the amount of training data: If the model is underfitting due to a lack of data, increasing the amount of training data may help.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                      "primary_category": "machine learning",
                      "rerank_score": 2.418670892715454,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0035214696545153856,
                        "neutral": 0.7250623106956482,
                        "support": 0.27141618728637695
                      },
                      "stance_score": 0.26789471763186157,
                      "evidence_contribution": 0.6479491558484092,
                      "combined_rank_score": 3.298418164253235
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 1785,
                      "faiss_score": 0.8818594813346863,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 53,
                      "sentence": "Cleaned datasets can increase training efficiency and lead to improved downstream performance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 3.4402031898498535,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.4319572150707245,
                        "neutral": 0.5465472936630249,
                        "support": 0.021495435386896133
                      },
                      "stance_score": -0.41046177968382835,
                      "evidence_contribution": -1.412071923779754,
                      "combined_rank_score": 4.32206267118454
                    },
                    {
                      "id": 6133,
                      "faiss_score": 0.8990232944488525,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 2.8726301193237305,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.7590318322181702,
                        "neutral": 0.217951238155365,
                        "support": 0.023016992956399918
                      },
                      "stance_score": -0.7360148392617702,
                      "evidence_contribution": -2.1142983955325754,
                      "combined_rank_score": 3.771653413772583
                    },
                    {
                      "id": 5906,
                      "faiss_score": 0.8880614638328552,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 1,
                      "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 2.7191481590270996,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.3397914171218872,
                        "neutral": 0.648522138595581,
                        "support": 0.01168638002127409
                      },
                      "stance_score": -0.3281050371006131,
                      "evidence_contribution": -0.8921662075996504,
                      "combined_rank_score": 3.607209622859955
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6044,
                      "faiss_score": 0.8903433084487915,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 4,
                      "sentence": "Increasing the number of parameters, the amount of training data, and the total compute used during training has been shown to produce consistent improvements in performance across many benchmarks.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 2.333214282989502,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.0027996383141726255,
                        "neutral": 0.9180957674980164,
                        "support": 0.07910455763339996
                      },
                      "stance_score": 0.07630491931922734,
                      "evidence_contribution": 0.1780357276179828,
                      "combined_rank_score": 3.2235575914382935
                    },
                    {
                      "id": 6137,
                      "faiss_score": 0.9186047315597534,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 1.8626278638839722,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.0020000473596155643,
                        "neutral": 0.9066423773765564,
                        "support": 0.09135754406452179
                      },
                      "stance_score": 0.08935749670490623,
                      "evidence_contribution": 0.16643976320947856,
                      "combined_rank_score": 2.7812325954437256
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing dataset size improves robustness",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "weak",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6137,
                      "faiss_score": 0.9264270067214966,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Empirical evidence suggests that for many tasks, performance improves more reliably when both model size and dataset size are increased together.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 3.0166993141174316,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.004063395783305168,
                        "neutral": 0.8299326300621033,
                        "support": 0.16600392758846283
                      },
                      "stance_score": 0.16194053180515766,
                      "evidence_contribution": 0.48852589122443124,
                      "combined_rank_score": 3.9431263208389282
                    },
                    {
                      "id": 2613,
                      "faiss_score": 0.8810292482376099,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 208,
                      "sentence": "Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": -3.591813564300537,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.005783679895102978,
                        "neutral": 0.6264408826828003,
                        "support": 0.36777544021606445
                      },
                      "stance_score": 0.3619917603209615,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.7107843160629272
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6133,
                      "faiss_score": 0.8859886527061462,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 9,
                      "sentence": "In practice, increasing model size often improves performance up to a point, after which gains diminish or become unstable if other factors are not adjusted.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 1.7032674551010132,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.423473984003067,
                        "neutral": 0.5593301057815552,
                        "support": 0.01719595119357109
                      },
                      "stance_score": -0.4062780328094959,
                      "evidence_contribution": -0.6920001510068761,
                      "combined_rank_score": 2.5892561078071594
                    },
                    {
                      "id": 6147,
                      "faiss_score": 0.8822609782218933,
                      "faiss_rank": 7,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 23,
                      "sentence": "For example, increasing model size without increasing data may yield limited benefits, while increasing data without sufficient model capacity may fail to exploit the additional information.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 0.8042117357254028,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.33419978618621826,
                        "neutral": 0.6566392183303833,
                        "support": 0.009161033667623997
                      },
                      "stance_score": -0.32503875251859426,
                      "evidence_contribution": -0.26139997934099835,
                      "combined_rank_score": 1.6864727139472961
                    },
                    {
                      "id": 6161,
                      "faiss_score": 0.8941830396652222,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 37,
                      "sentence": "Scaling affects robustness and generalization in nontrivial ways.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -3.672755241394043,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.7847391366958618,
                        "neutral": 0.2066025733947754,
                        "support": 0.008658240549266338
                      },
                      "stance_score": -0.7760808961465955,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -2.778572201728821
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6007,
                      "faiss_score": 0.8798772096633911,
                      "faiss_rank": 13,
                      "doc_id": "local_ml_evaluation_and_generalization.txt",
                      "file_type": ".txt",
                      "position": 33,
                      "sentence": "Robust evaluation requires understanding the provenance and limitations of datasets.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                      "primary_category": null,
                      "rerank_score": -2.3191099166870117,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.009657689370214939,
                        "neutral": 0.9895644187927246,
                        "support": 0.000777834327891469
                      },
                      "stance_score": -0.00887985504232347,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.4392327070236206
                    },
                    {
                      "id": 1785,
                      "faiss_score": 0.8803674578666687,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 53,
                      "sentence": "Cleaned datasets can increase training efficiency and lead to improved downstream performance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -2.49263072013855,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.05982460454106331,
                        "neutral": 0.8901634216308594,
                        "support": 0.05001198872923851
                      },
                      "stance_score": -0.009812615811824799,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.612263262271881
                    },
                    {
                      "id": 1394,
                      "faiss_score": 0.8871293067932129,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Overfitting",
                      "file_type": ".txt",
                      "position": 44,
                      "sentence": "The optimal function usually needs verification on bigger or completely new datasets.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                      "primary_category": "machine learning",
                      "rerank_score": -6.338343143463135,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.010466142557561398,
                        "neutral": 0.9807940721511841,
                        "support": 0.008739816956222057
                      },
                      "stance_score": -0.0017263256013393402,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -5.451213836669922
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Noisy data degrades performance",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 4087,
                      "faiss_score": 0.8815248608589172,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Information_theory",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "However, channels often fail to produce exact reconstruction of a signal; noise, periods of silence, and other forms of signal corruption often degrade quality.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Information_theory",
                      "primary_category": "all articles needing additional references",
                      "rerank_score": -0.8287366032600403,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.003933201543986797,
                        "neutral": 0.49653515219688416,
                        "support": 0.49953165650367737
                      },
                      "stance_score": 0.49559845495969057,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.05278825759887695
                    },
                    {
                      "id": 5808,
                      "faiss_score": 0.8804681897163391,
                      "faiss_rank": 8,
                      "doc_id": "local_math_information_theory_and_learning.txt",
                      "file_type": ".txt",
                      "position": 56,
                      "sentence": "Even with infinite computation and perfect optimization, performance is bounded by noise and ambiguity.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_information_theory_and_learning.txt",
                      "primary_category": null,
                      "rerank_score": -3.4648706912994385,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.00144195684697479,
                        "neutral": 0.7477361559867859,
                        "support": 0.25082188844680786
                      },
                      "stance_score": 0.24937993159983307,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.5844025015830994
                    },
                    {
                      "id": 6005,
                      "faiss_score": 0.8975870013237,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_evaluation_and_generalization.txt",
                      "file_type": ".txt",
                      "position": 31,
                      "sentence": "Noisy labels, missing values, and biased sampling affect both training and evaluation.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                      "primary_category": null,
                      "rerank_score": -4.529215335845947,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.001983974128961563,
                        "neutral": 0.5871595144271851,
                        "support": 0.41085657477378845
                      },
                      "stance_score": 0.4088726006448269,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -3.6316283345222473
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 5854,
                      "faiss_score": 0.884116530418396,
                      "faiss_rank": 6,
                      "doc_id": "local_math_optimization_and_convergence.txt",
                      "file_type": ".txt",
                      "position": 23,
                      "sentence": "Noise in optimization can be both beneficial and harmful.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                      "primary_category": null,
                      "rerank_score": -5.094203948974609,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.6732237339019775,
                        "neutral": 0.28176796436309814,
                        "support": 0.04500836506485939
                      },
                      "stance_score": -0.6282153688371181,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -4.210087418556213
                    },
                    {
                      "id": 1785,
                      "faiss_score": 0.8795800805091858,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 53,
                      "sentence": "Cleaned datasets can increase training efficiency and lead to improved downstream performance.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -6.785529136657715,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.18365582823753357,
                        "neutral": 0.8093551397323608,
                        "support": 0.006989020388573408
                      },
                      "stance_score": -0.17666680784896016,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -5.905949056148529
                    }
                  ],
                  "neutral": [
                    {
                      "id": 5998,
                      "faiss_score": 0.9058985114097595,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_evaluation_and_generalization.txt",
                      "file_type": ".txt",
                      "position": 24,
                      "sentence": "When deployment data differs from training data, performance may degrade unpredictably.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                      "primary_category": null,
                      "rerank_score": 0.8202033042907715,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.008945327252149582,
                        "neutral": 0.9763994812965393,
                        "support": 0.014655262231826782
                      },
                      "stance_score": 0.0057099349796772,
                      "evidence_contribution": 0.004683307537616699,
                      "combined_rank_score": 1.726101815700531
                    },
                    {
                      "id": 6215,
                      "faiss_score": 0.8717062473297119,
                      "faiss_rank": 15,
                      "doc_id": "local_ml_training_dynamics.txt",
                      "file_type": ".txt",
                      "position": 23,
                      "sentence": "Noise in training dynamics arises from multiple sources, including stochastic gradient estimation, data variability, and numerical precision.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_training_dynamics.txt",
                      "primary_category": null,
                      "rerank_score": -3.703622341156006,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.0024312855675816536,
                        "neutral": 0.9882625937461853,
                        "support": 0.009306076914072037
                      },
                      "stance_score": 0.006874791346490383,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.831916093826294
                    },
                    {
                      "id": 1445,
                      "faiss_score": 0.8750660419464111,
                      "faiss_rank": 14,
                      "doc_id": "wiki_Regularization_(mathematics)",
                      "file_type": ".txt",
                      "position": 17,
                      "sentence": "Typically in learning problems, only a subset of input data and labels are available, measured with some noise.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
                      "primary_category": "articles with short description",
                      "rerank_score": -6.994556427001953,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.006035220809280872,
                        "neutral": 0.9910310506820679,
                        "support": 0.0029336868319660425
                      },
                      "stance_score": -0.00310153397731483,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -6.119490385055542
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Quantum error correction enables reliable quantum computation and scalability, but requires many physical qubits, introduces large overhead, limits near-term feasibility, and increases system complexity.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "MIXED",
      "subclaims": [
        {
          "subclaim": "Quantum error correction enables reliable quantum computation",
          "verdict": "CONTRADICT",
          "controversial": true,
          "strengths": {
            "support": 1.622760508875249,
            "contradict": 3.78928737277848,
            "total": 5.412047881653729
          },
          "evidence": {
            "supporting": [
              {
                "id": 4795,
                "faiss_score": 0.8979429006576538,
                "faiss_rank": 15,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 5.168917655944824,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.018803942948579788,
                  "neutral": 0.6484461426734924,
                  "support": 0.3327498435974121
                },
                "stance_score": 0.3139459006488323,
                "evidence_contribution": 1.622760508875249,
                "combined_rank_score": 6.066860556602478
              }
            ],
            "contradicting": [
              {
                "id": 4719,
                "faiss_score": 0.9038070440292358,
                "faiss_rank": 9,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.851476669311523,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.7934035658836365,
                  "neutral": 0.1942514181137085,
                  "support": 0.012345007620751858
                },
                "stance_score": -0.7810585582628846,
                "evidence_contribution": -3.78928737277848,
                "combined_rank_score": 5.755283713340759
              }
            ],
            "neutral": [
              {
                "id": 4862,
                "faiss_score": 0.920432448387146,
                "faiss_rank": 2,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 109,
                "sentence": "Quantum error correction can be applied to quantum metrology.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 4.4757890701293945,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.007816636003553867,
                  "neutral": 0.962035059928894,
                  "support": 0.03014826402068138
                },
                "stance_score": 0.022331628017127514,
                "evidence_contribution": 0.09995165659725469,
                "combined_rank_score": 5.3962215185165405
              },
              {
                "id": 756,
                "faiss_score": 0.8932042121887207,
                "faiss_rank": 19,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 160,
                "sentence": "As described by the threshold theorem, if the error rate is small enough, it is thought to be possible to use quantum error correction to suppress errors and decoherence.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.274754047393799,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0026605098973959684,
                  "neutral": 0.9881643056869507,
                  "support": 0.009175206534564495
                },
                "stance_score": 0.006514696637168527,
                "evidence_contribution": 0.02784872581727893,
                "combined_rank_score": 5.1679582595825195
              },
              {
                "id": 4824,
                "faiss_score": 0.8977358341217041,
                "faiss_rank": 16,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 71,
                "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 4.153146743774414,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.0023480842355638742,
                  "neutral": 0.971402108669281,
                  "support": 0.026249852031469345
                },
                "stance_score": 0.02390176779590547,
                "evidence_contribution": 0.09926754909201696,
                "combined_rank_score": 5.050882577896118
              }
            ]
          }
        },
        {
          "subclaim": "Quantum error correction requires many physical qubits",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 23.16308875487529,
            "contradict": 0.5789238203380016,
            "total": 23.74201257521329
          },
          "evidence": {
            "supporting": [
              {
                "id": 6554,
                "faiss_score": 0.9731938242912292,
                "faiss_rank": 1,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 9.974997520446777,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0009988424135372043,
                  "neutral": 0.005816575605422258,
                  "support": 0.9931846261024475
                },
                "stance_score": 0.9921857836889103,
                "evidence_contribution": 9.897050732119423,
                "combined_rank_score": 10.948191344738007
              },
              {
                "id": 4792,
                "faiss_score": 0.9309069514274597,
                "faiss_rank": 2,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 39,
                "sentence": "According to the quantum Hamming bound, encoding a single logical qubit with the ability to correct any single-qubit error requires at least five physical qubits.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 8.700674057006836,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.003476664423942566,
                  "neutral": 0.07948478311300278,
                  "support": 0.9170385003089905
                },
                "stance_score": 0.9135618358850479,
                "evidence_contribution": 7.948603764956573,
                "combined_rank_score": 9.631581008434296
              },
              {
                "id": 765,
                "faiss_score": 0.9033190608024597,
                "faiss_rank": 13,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 169,
                "sentence": "Careful estimates show that at least 3 million physical qubits would factor 2,048-bit integer in 5 months on a fully error-corrected trapped-ion quantum computer.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 6.840010643005371,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0012221212964504957,
                  "neutral": 0.220154270529747,
                  "support": 0.7786235809326172
                },
                "stance_score": 0.7774014596361667,
                "evidence_contribution": 5.317434257799291,
                "combined_rank_score": 7.743329703807831
              }
            ],
            "contradicting": [
              {
                "id": 4795,
                "faiss_score": 0.8958277702331543,
                "faiss_rank": 19,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 3.683861255645752,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.2773669958114624,
                  "neutral": 0.6024173498153687,
                  "support": 0.12021563202142715
                },
                "stance_score": -0.15715136379003525,
                "evidence_contribution": -0.5789238203380016,
                "combined_rank_score": 4.579689025878906
              },
              {
                "id": 4774,
                "faiss_score": 0.8971900939941406,
                "faiss_rank": 18,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 21,
                "sentence": "Consequently, errors on an n-qubit system can be described by a binary string of length 2n, allowing classical error-correction techniques to be applied under suitable constraints.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": -0.24668924510478973,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.44234442710876465,
                  "neutral": 0.5252451300621033,
                  "support": 0.032410457730293274
                },
                "stance_score": -0.4099339693784714,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.6505008488893509
              }
            ],
            "neutral": [
              {
                "id": 760,
                "faiss_score": 0.9304232597351074,
                "faiss_rank": 3,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 164,
                "sentence": "However, the use of error correction brings with it the cost of a greatly increased number of required qubits.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 1.8865216970443726,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.0037514108698815107,
                  "neutral": 0.9413119554519653,
                  "support": 0.05493664741516113
                },
                "stance_score": 0.05118523654527962,
                "evidence_contribution": 0.09656205931101855,
                "combined_rank_score": 2.81694495677948
              },
              {
                "id": 4719,
                "faiss_score": 0.9088607430458069,
                "faiss_rank": 6,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": -0.015107106417417526,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.003241618163883686,
                  "neutral": 0.990568220615387,
                  "support": 0.006190212909132242
                },
                "stance_score": 0.002948594745248556,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.8937536366283894
              }
            ]
          }
        },
        {
          "subclaim": "Quantum error correction introduces large overhead",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 8.401991656593532,
            "contradict": 0.0,
            "total": 8.401991656593532
          },
          "evidence": {
            "supporting": [
              {
                "id": 764,
                "faiss_score": 0.9220078587532043,
                "faiss_rank": 6,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 168,
                "sentence": "However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 5.192421913146973,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0018393112113699317,
                  "neutral": 0.101406030356884,
                  "support": 0.8967546820640564
                },
                "stance_score": 0.8949153708526865,
                "evidence_contribution": 4.646778182027539,
                "combined_rank_score": 6.114429771900177
              },
              {
                "id": 793,
                "faiss_score": 0.9589597582817078,
                "faiss_rank": 1,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.363793849945068,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0015836000675335526,
                  "neutral": 0.13629405200481415,
                  "support": 0.8621222972869873
                },
                "stance_score": 0.8605386972194538,
                "evidence_contribution": 3.7552134745659935,
                "combined_rank_score": 5.322753608226776
              },
              {
                "id": 6553,
                "faiss_score": 0.9579821825027466,
                "faiss_rank": 3,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": -1.1027113199234009,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.0016034289728850126,
                  "neutral": 0.17472197115421295,
                  "support": 0.8236745595932007
                },
                "stance_score": 0.8220711306203157,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.1447291374206543
              }
            ],
            "contradicting": [
              {
                "id": 4795,
                "faiss_score": 0.8939375281333923,
                "faiss_rank": 12,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": -0.6144595146179199,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.1697005182504654,
                  "neutral": 0.8215643763542175,
                  "support": 0.008735112845897675
                },
                "stance_score": -0.16096540540456772,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.2794780135154724
              },
              {
                "id": 4824,
                "faiss_score": 0.8887735605239868,
                "faiss_rank": 18,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 71,
                "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": -1.4877570867538452,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.2573750913143158,
                  "neutral": 0.7392532229423523,
                  "support": 0.00337165012024343
                },
                "stance_score": -0.25400344119407237,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.5989835262298584
              }
            ],
            "neutral": [
              {
                "id": 4862,
                "faiss_score": 0.891497790813446,
                "faiss_rank": 15,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 109,
                "sentence": "Quantum error correction can be applied to quantum metrology.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": -0.8552758693695068,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.008183952420949936,
                  "neutral": 0.9910325407981873,
                  "support": 0.0007834827993065119
                },
                "stance_score": -0.007400469621643424,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.03622192144393921
              },
              {
                "id": 6554,
                "faiss_score": 0.9196069836616516,
                "faiss_rank": 8,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": -1.5475598573684692,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.002062471816316247,
                  "neutral": 0.9819877743721008,
                  "support": 0.015949703752994537
                },
                "stance_score": 0.01388723193667829,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.6279528737068176
              },
              {
                "id": 6555,
                "faiss_score": 0.9095394611358643,
                "faiss_rank": 9,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 15,
                "sentence": "This overhead means that a useful, fault-tolerant quantum computer would need orders of magnitude more qubits than are currently available.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": -1.880692720413208,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.005107593722641468,
                  "neutral": 0.9913631677627563,
                  "support": 0.003529252950102091
                },
                "stance_score": -0.0015783407725393772,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.9711532592773438
              }
            ]
          }
        },
        {
          "subclaim": "Quantum error correction limits near-term feasibility",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 1.9361405172395258,
            "contradict": 0.3860712608028134,
            "total": 2.3222117780423392
          },
          "evidence": {
            "supporting": [
              {
                "id": 6562,
                "faiss_score": 0.9053820371627808,
                "faiss_rank": 5,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 22,
                "sentence": "This constraint has motivated interest in near-term quantum devices that operate without full error correction, often referred to as noisy intermediate-scale quantum systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 3.418632984161377,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.03480833023786545,
                  "neutral": 0.36403393745422363,
                  "support": 0.6011576652526855
                },
                "stance_score": 0.5663493350148201,
                "evidence_contribution": 1.9361405172395258,
                "combined_rank_score": 4.324015021324158
              },
              {
                "id": 793,
                "faiss_score": 0.9205553531646729,
                "faiss_rank": 2,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": -2.2823166847229004,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.006018023006618023,
                  "neutral": 0.7036983370780945,
                  "support": 0.29028359055519104
                },
                "stance_score": 0.284265567548573,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.3617613315582275
              },
              {
                "id": 6553,
                "faiss_score": 0.9212385416030884,
                "faiss_rank": 1,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": -2.804901123046875,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.06996207684278488,
                  "neutral": 0.5819587111473083,
                  "support": 0.34807923436164856
                },
                "stance_score": 0.2781171575188637,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.8836625814437866
              }
            ],
            "contradicting": [
              {
                "id": 4795,
                "faiss_score": 0.8939117193222046,
                "faiss_rank": 10,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 42,
                "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 1.228224277496338,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.3718124032020569,
                  "neutral": 0.5707080364227295,
                  "support": 0.057479534298181534
                },
                "stance_score": -0.31433286890387535,
                "evidence_contribution": -0.3860712608028134,
                "combined_rank_score": 2.1221359968185425
              },
              {
                "id": 4824,
                "faiss_score": 0.8983690738677979,
                "faiss_rank": 8,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 71,
                "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": -0.5382038950920105,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.1801389753818512,
                  "neutral": 0.813462495803833,
                  "support": 0.006398575846105814
                },
                "stance_score": -0.17374039953574538,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.36016517877578735
              }
            ],
            "neutral": [
              {
                "id": 756,
                "faiss_score": 0.8903536200523376,
                "faiss_rank": 12,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 160,
                "sentence": "As described by the threshold theorem, if the error rate is small enough, it is thought to be possible to use quantum error correction to suppress errors and decoherence.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": -1.1330981254577637,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.040424078702926636,
                  "neutral": 0.9570391774177551,
                  "support": 0.0025368176866322756
                },
                "stance_score": -0.03788726101629436,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.24274450540542603
              },
              {
                "id": 4862,
                "faiss_score": 0.8929986357688904,
                "faiss_rank": 11,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 109,
                "sentence": "Quantum error correction can be applied to quantum metrology.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": -1.851318120956421,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.0033798974473029375,
                  "neutral": 0.9954323768615723,
                  "support": 0.0011877332581207156
                },
                "stance_score": -0.002192164189182222,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.9583194851875305
              },
              {
                "id": 6554,
                "faiss_score": 0.8998539447784424,
                "faiss_rank": 7,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": -2.0523762702941895,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.0015734165208414197,
                  "neutral": 0.9968135952949524,
                  "support": 0.0016129479045048356
                },
                "stance_score": 3.953138366341591e-05,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.152522325515747
              }
            ]
          }
        },
        {
          "subclaim": "Quantum error correction increases system complexity",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 3.7791287603029637,
            "contradict": 0.07190165519970329,
            "total": 3.851030415502667
          },
          "evidence": {
            "supporting": [
              {
                "id": 764,
                "faiss_score": 0.9119178056716919,
                "faiss_rank": 7,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 168,
                "sentence": "However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 4.0019683837890625,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.007853573188185692,
                  "neutral": 0.3639184236526489,
                  "support": 0.6282280087471008
                },
                "stance_score": 0.6203744355589151,
                "evidence_contribution": 2.4827188772177635,
                "combined_rank_score": 4.913886189460754
              },
              {
                "id": 793,
                "faiss_score": 0.9260589480400085,
                "faiss_rank": 4,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 197,
                "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": 2.7478976249694824,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.025753580033779144,
                  "neutral": 0.47671034932136536,
                  "support": 0.4975360333919525
                },
                "stance_score": 0.47178245335817337,
                "evidence_contribution": 1.2964098830852002,
                "combined_rank_score": 3.673956573009491
              },
              {
                "id": 760,
                "faiss_score": 0.9287818670272827,
                "faiss_rank": 1,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 164,
                "sentence": "However, the use of error correction brings with it the cost of a greatly increased number of required qubits.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": -1.370161771774292,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.008020526729524136,
                  "neutral": 0.6548252701759338,
                  "support": 0.33715423941612244
                },
                "stance_score": 0.3291337126865983,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.4413799047470093
              }
            ],
            "contradicting": [
              {
                "id": 4862,
                "faiss_score": 0.8882696628570557,
                "faiss_rank": 18,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 109,
                "sentence": "Quantum error correction can be applied to quantum metrology.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": 0.5913575291633606,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.12550652027130127,
                  "neutral": 0.870574414730072,
                  "support": 0.00391906825825572
                },
                "stance_score": -0.12158745201304555,
                "evidence_contribution": -0.07190165519970329,
                "combined_rank_score": 1.4796271920204163
              },
              {
                "id": 4786,
                "faiss_score": 0.8890231847763062,
                "faiss_rank": 15,
                "doc_id": "wiki_Quantum_error_correction",
                "file_type": ".txt",
                "position": 33,
                "sentence": "It states that errors can be corrected by recursively concatenating quantum codes\u2014such as CSS codes\u2014across logarithmically many levels, provided the error rate of individual quantum gates remains below a certain threshold.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                "primary_category": "articles with short description",
                "rerank_score": -0.16000661253929138,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.11752313375473022,
                  "neutral": 0.8781783580780029,
                  "support": 0.00429850909858942
                },
                "stance_score": -0.1132246246561408,
                "evidence_contribution": -0.0,
                "combined_rank_score": 0.7290165722370148
              }
            ],
            "neutral": [
              {
                "id": 6553,
                "faiss_score": 0.9282435178756714,
                "faiss_rank": 3,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 2.169891357421875,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.12731100618839264,
                  "neutral": 0.6538717150688171,
                  "support": 0.21881726384162903
                },
                "stance_score": 0.09150625765323639,
                "evidence_contribution": 0.19855863763177695,
                "combined_rank_score": 3.0981348752975464
              },
              {
                "id": 6519,
                "faiss_score": 0.9061214923858643,
                "faiss_rank": 9,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 48,
                "sentence": "However, practical quantum systems face substantial obstacles, including noise, error correction, and scalability.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": 1.6414793729782104,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.004262776114046574,
                  "neutral": 0.9861917495727539,
                  "support": 0.009545507840812206
                },
                "stance_score": 0.005282731726765633,
                "evidence_contribution": 0.00867149516246335,
                "combined_rank_score": 2.5476008653640747
              },
              {
                "id": 6554,
                "faiss_score": 0.9244345426559448,
                "faiss_rank": 6,
                "doc_id": "local_physics_quantum_overview.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                "primary_category": null,
                "rerank_score": 1.0123276710510254,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.007600029930472374,
                  "neutral": 0.9813321828842163,
                  "support": 0.0110678281635046
                },
                "stance_score": 0.0034677982330322266,
                "evidence_contribution": 0.003510548108920375,
                "combined_rank_score": 1.9367622137069702
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The evidence presents both benefits and limitations.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Quantum error correction requires many physical qubits",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "weak"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6554,
                      "faiss_score": 0.9731938242912292,
                      "faiss_rank": 1,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 9.974997520446777,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0009988424135372043,
                        "neutral": 0.005816575605422258,
                        "support": 0.9931846261024475
                      },
                      "stance_score": 0.9921857836889103,
                      "evidence_contribution": 9.897050732119423,
                      "combined_rank_score": 10.948191344738007
                    },
                    {
                      "id": 4792,
                      "faiss_score": 0.9309069514274597,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 39,
                      "sentence": "According to the quantum Hamming bound, encoding a single logical qubit with the ability to correct any single-qubit error requires at least five physical qubits.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 8.700674057006836,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.003476664423942566,
                        "neutral": 0.07948478311300278,
                        "support": 0.9170385003089905
                      },
                      "stance_score": 0.9135618358850479,
                      "evidence_contribution": 7.948603764956573,
                      "combined_rank_score": 9.631581008434296
                    },
                    {
                      "id": 765,
                      "faiss_score": 0.9033190608024597,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 169,
                      "sentence": "Careful estimates show that at least 3 million physical qubits would factor 2,048-bit integer in 5 months on a fully error-corrected trapped-ion quantum computer.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 6.840010643005371,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0012221212964504957,
                        "neutral": 0.220154270529747,
                        "support": 0.7786235809326172
                      },
                      "stance_score": 0.7774014596361667,
                      "evidence_contribution": 5.317434257799291,
                      "combined_rank_score": 7.743329703807831
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 4795,
                      "faiss_score": 0.8958277702331543,
                      "faiss_rank": 19,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 3.683861255645752,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.2773669958114624,
                        "neutral": 0.6024173498153687,
                        "support": 0.12021563202142715
                      },
                      "stance_score": -0.15715136379003525,
                      "evidence_contribution": -0.5789238203380016,
                      "combined_rank_score": 4.579689025878906
                    },
                    {
                      "id": 4774,
                      "faiss_score": 0.8971900939941406,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "Consequently, errors on an n-qubit system can be described by a binary string of length 2n, allowing classical error-correction techniques to be applied under suitable constraints.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": -0.24668924510478973,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.44234442710876465,
                        "neutral": 0.5252451300621033,
                        "support": 0.032410457730293274
                      },
                      "stance_score": -0.4099339693784714,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.6505008488893509
                    }
                  ],
                  "neutral": [
                    {
                      "id": 760,
                      "faiss_score": 0.9304232597351074,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 164,
                      "sentence": "However, the use of error correction brings with it the cost of a greatly increased number of required qubits.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 1.8865216970443726,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.0037514108698815107,
                        "neutral": 0.9413119554519653,
                        "support": 0.05493664741516113
                      },
                      "stance_score": 0.05118523654527962,
                      "evidence_contribution": 0.09656205931101855,
                      "combined_rank_score": 2.81694495677948
                    },
                    {
                      "id": 4719,
                      "faiss_score": 0.9088607430458069,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": -0.015107106417417526,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.003241618163883686,
                        "neutral": 0.990568220615387,
                        "support": 0.006190212909132242
                      },
                      "stance_score": 0.002948594745248556,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.8937536366283894
                    }
                  ]
                }
              },
              {
                "subclaim": "Quantum error correction introduces large overhead",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 764,
                      "faiss_score": 0.9220078587532043,
                      "faiss_rank": 6,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 168,
                      "sentence": "However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 5.192421913146973,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0018393112113699317,
                        "neutral": 0.101406030356884,
                        "support": 0.8967546820640564
                      },
                      "stance_score": 0.8949153708526865,
                      "evidence_contribution": 4.646778182027539,
                      "combined_rank_score": 6.114429771900177
                    },
                    {
                      "id": 793,
                      "faiss_score": 0.9589597582817078,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.363793849945068,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0015836000675335526,
                        "neutral": 0.13629405200481415,
                        "support": 0.8621222972869873
                      },
                      "stance_score": 0.8605386972194538,
                      "evidence_contribution": 3.7552134745659935,
                      "combined_rank_score": 5.322753608226776
                    },
                    {
                      "id": 6553,
                      "faiss_score": 0.9579821825027466,
                      "faiss_rank": 3,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": -1.1027113199234009,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.0016034289728850126,
                        "neutral": 0.17472197115421295,
                        "support": 0.8236745595932007
                      },
                      "stance_score": 0.8220711306203157,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.1447291374206543
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 4795,
                      "faiss_score": 0.8939375281333923,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": -0.6144595146179199,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.1697005182504654,
                        "neutral": 0.8215643763542175,
                        "support": 0.008735112845897675
                      },
                      "stance_score": -0.16096540540456772,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.2794780135154724
                    },
                    {
                      "id": 4824,
                      "faiss_score": 0.8887735605239868,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 71,
                      "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": -1.4877570867538452,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.2573750913143158,
                        "neutral": 0.7392532229423523,
                        "support": 0.00337165012024343
                      },
                      "stance_score": -0.25400344119407237,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.5989835262298584
                    }
                  ],
                  "neutral": [
                    {
                      "id": 4862,
                      "faiss_score": 0.891497790813446,
                      "faiss_rank": 15,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 109,
                      "sentence": "Quantum error correction can be applied to quantum metrology.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": -0.8552758693695068,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.008183952420949936,
                        "neutral": 0.9910325407981873,
                        "support": 0.0007834827993065119
                      },
                      "stance_score": -0.007400469621643424,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.03622192144393921
                    },
                    {
                      "id": 6554,
                      "faiss_score": 0.9196069836616516,
                      "faiss_rank": 8,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": -1.5475598573684692,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.002062471816316247,
                        "neutral": 0.9819877743721008,
                        "support": 0.015949703752994537
                      },
                      "stance_score": 0.01388723193667829,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.6279528737068176
                    },
                    {
                      "id": 6555,
                      "faiss_score": 0.9095394611358643,
                      "faiss_rank": 9,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 15,
                      "sentence": "This overhead means that a useful, fault-tolerant quantum computer would need orders of magnitude more qubits than are currently available.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": -1.880692720413208,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.005107593722641468,
                        "neutral": 0.9913631677627563,
                        "support": 0.003529252950102091
                      },
                      "stance_score": -0.0015783407725393772,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.9711532592773438
                    }
                  ]
                }
              },
              {
                "subclaim": "Quantum error correction limits near-term feasibility",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "weak"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6562,
                      "faiss_score": 0.9053820371627808,
                      "faiss_rank": 5,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 22,
                      "sentence": "This constraint has motivated interest in near-term quantum devices that operate without full error correction, often referred to as noisy intermediate-scale quantum systems.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 3.418632984161377,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.03480833023786545,
                        "neutral": 0.36403393745422363,
                        "support": 0.6011576652526855
                      },
                      "stance_score": 0.5663493350148201,
                      "evidence_contribution": 1.9361405172395258,
                      "combined_rank_score": 4.324015021324158
                    },
                    {
                      "id": 793,
                      "faiss_score": 0.9205553531646729,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": -2.2823166847229004,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.006018023006618023,
                        "neutral": 0.7036983370780945,
                        "support": 0.29028359055519104
                      },
                      "stance_score": 0.284265567548573,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.3617613315582275
                    },
                    {
                      "id": 6553,
                      "faiss_score": 0.9212385416030884,
                      "faiss_rank": 1,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": -2.804901123046875,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.06996207684278488,
                        "neutral": 0.5819587111473083,
                        "support": 0.34807923436164856
                      },
                      "stance_score": 0.2781171575188637,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.8836625814437866
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 4795,
                      "faiss_score": 0.8939117193222046,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 1.228224277496338,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.3718124032020569,
                        "neutral": 0.5707080364227295,
                        "support": 0.057479534298181534
                      },
                      "stance_score": -0.31433286890387535,
                      "evidence_contribution": -0.3860712608028134,
                      "combined_rank_score": 2.1221359968185425
                    },
                    {
                      "id": 4824,
                      "faiss_score": 0.8983690738677979,
                      "faiss_rank": 8,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 71,
                      "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": -0.5382038950920105,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.1801389753818512,
                        "neutral": 0.813462495803833,
                        "support": 0.006398575846105814
                      },
                      "stance_score": -0.17374039953574538,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.36016517877578735
                    }
                  ],
                  "neutral": [
                    {
                      "id": 756,
                      "faiss_score": 0.8903536200523376,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 160,
                      "sentence": "As described by the threshold theorem, if the error rate is small enough, it is thought to be possible to use quantum error correction to suppress errors and decoherence.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": -1.1330981254577637,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.040424078702926636,
                        "neutral": 0.9570391774177551,
                        "support": 0.0025368176866322756
                      },
                      "stance_score": -0.03788726101629436,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.24274450540542603
                    },
                    {
                      "id": 4862,
                      "faiss_score": 0.8929986357688904,
                      "faiss_rank": 11,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 109,
                      "sentence": "Quantum error correction can be applied to quantum metrology.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": -1.851318120956421,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.0033798974473029375,
                        "neutral": 0.9954323768615723,
                        "support": 0.0011877332581207156
                      },
                      "stance_score": -0.002192164189182222,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.9583194851875305
                    },
                    {
                      "id": 6554,
                      "faiss_score": 0.8998539447784424,
                      "faiss_rank": 7,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": -2.0523762702941895,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.0015734165208414197,
                        "neutral": 0.9968135952949524,
                        "support": 0.0016129479045048356
                      },
                      "stance_score": 3.953138366341591e-05,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.152522325515747
                    }
                  ]
                }
              },
              {
                "subclaim": "Quantum error correction increases system complexity",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 764,
                      "faiss_score": 0.9119178056716919,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 168,
                      "sentence": "However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.0019683837890625,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.007853573188185692,
                        "neutral": 0.3639184236526489,
                        "support": 0.6282280087471008
                      },
                      "stance_score": 0.6203744355589151,
                      "evidence_contribution": 2.4827188772177635,
                      "combined_rank_score": 4.913886189460754
                    },
                    {
                      "id": 793,
                      "faiss_score": 0.9260589480400085,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 2.7478976249694824,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.025753580033779144,
                        "neutral": 0.47671034932136536,
                        "support": 0.4975360333919525
                      },
                      "stance_score": 0.47178245335817337,
                      "evidence_contribution": 1.2964098830852002,
                      "combined_rank_score": 3.673956573009491
                    },
                    {
                      "id": 760,
                      "faiss_score": 0.9287818670272827,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 164,
                      "sentence": "However, the use of error correction brings with it the cost of a greatly increased number of required qubits.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": -1.370161771774292,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.008020526729524136,
                        "neutral": 0.6548252701759338,
                        "support": 0.33715423941612244
                      },
                      "stance_score": 0.3291337126865983,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.4413799047470093
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 4862,
                      "faiss_score": 0.8882696628570557,
                      "faiss_rank": 18,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 109,
                      "sentence": "Quantum error correction can be applied to quantum metrology.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 0.5913575291633606,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.12550652027130127,
                        "neutral": 0.870574414730072,
                        "support": 0.00391906825825572
                      },
                      "stance_score": -0.12158745201304555,
                      "evidence_contribution": -0.07190165519970329,
                      "combined_rank_score": 1.4796271920204163
                    },
                    {
                      "id": 4786,
                      "faiss_score": 0.8890231847763062,
                      "faiss_rank": 15,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 33,
                      "sentence": "It states that errors can be corrected by recursively concatenating quantum codes\u2014such as CSS codes\u2014across logarithmically many levels, provided the error rate of individual quantum gates remains below a certain threshold.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": -0.16000661253929138,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.11752313375473022,
                        "neutral": 0.8781783580780029,
                        "support": 0.00429850909858942
                      },
                      "stance_score": -0.1132246246561408,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": 0.7290165722370148
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6553,
                      "faiss_score": 0.9282435178756714,
                      "faiss_rank": 3,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 13,
                      "sentence": "Error correction is essential for scaling quantum computers, but it comes at a substantial cost.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 2.169891357421875,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.12731100618839264,
                        "neutral": 0.6538717150688171,
                        "support": 0.21881726384162903
                      },
                      "stance_score": 0.09150625765323639,
                      "evidence_contribution": 0.19855863763177695,
                      "combined_rank_score": 3.0981348752975464
                    },
                    {
                      "id": 6519,
                      "faiss_score": 0.9061214923858643,
                      "faiss_rank": 9,
                      "doc_id": "local_physics_computation.txt",
                      "file_type": ".txt",
                      "position": 48,
                      "sentence": "However, practical quantum systems face substantial obstacles, including noise, error correction, and scalability.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                      "primary_category": null,
                      "rerank_score": 1.6414793729782104,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.004262776114046574,
                        "neutral": 0.9861917495727539,
                        "support": 0.009545507840812206
                      },
                      "stance_score": 0.005282731726765633,
                      "evidence_contribution": 0.00867149516246335,
                      "combined_rank_score": 2.5476008653640747
                    },
                    {
                      "id": 6554,
                      "faiss_score": 0.9244345426559448,
                      "faiss_rank": 6,
                      "doc_id": "local_physics_quantum_overview.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "Quantum error correction schemes require many physical qubits to represent a single logical qubit.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_quantum_overview.txt",
                      "primary_category": null,
                      "rerank_score": 1.0123276710510254,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.007600029930472374,
                        "neutral": 0.9813321828842163,
                        "support": 0.0110678281635046
                      },
                      "stance_score": 0.0034677982330322266,
                      "evidence_contribution": 0.003510548108920375,
                      "combined_rank_score": 1.9367622137069702
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Quantum error correction enables reliable quantum computation",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 4795,
                      "faiss_score": 0.8979429006576538,
                      "faiss_rank": 15,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 42,
                      "sentence": "Surface codes are pivotal for scalable quantum error correction in 2025, enabling below-threshold logical qubits with improved fidelity in superconducting systems.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 5.168917655944824,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.018803942948579788,
                        "neutral": 0.6484461426734924,
                        "support": 0.3327498435974121
                      },
                      "stance_score": 0.3139459006488323,
                      "evidence_contribution": 1.622760508875249,
                      "combined_rank_score": 6.066860556602478
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 4719,
                      "faiss_score": 0.9038070440292358,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 197,
                      "sentence": "If quantum error correction is used to scale quantum computers to practical applications, its overhead may undermine the speedup offered by many quantum algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.851476669311523,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.7934035658836365,
                        "neutral": 0.1942514181137085,
                        "support": 0.012345007620751858
                      },
                      "stance_score": -0.7810585582628846,
                      "evidence_contribution": -3.78928737277848,
                      "combined_rank_score": 5.755283713340759
                    }
                  ],
                  "neutral": [
                    {
                      "id": 4862,
                      "faiss_score": 0.920432448387146,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 109,
                      "sentence": "Quantum error correction can be applied to quantum metrology.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 4.4757890701293945,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.007816636003553867,
                        "neutral": 0.962035059928894,
                        "support": 0.03014826402068138
                      },
                      "stance_score": 0.022331628017127514,
                      "evidence_contribution": 0.09995165659725469,
                      "combined_rank_score": 5.3962215185165405
                    },
                    {
                      "id": 756,
                      "faiss_score": 0.8932042121887207,
                      "faiss_rank": 19,
                      "doc_id": "wiki_Quantum_computing",
                      "file_type": ".txt",
                      "position": 160,
                      "sentence": "As described by the threshold theorem, if the error rate is small enough, it is thought to be possible to use quantum error correction to suppress errors and decoherence.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": 4.274754047393799,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0026605098973959684,
                        "neutral": 0.9881643056869507,
                        "support": 0.009175206534564495
                      },
                      "stance_score": 0.006514696637168527,
                      "evidence_contribution": 0.02784872581727893,
                      "combined_rank_score": 5.1679582595825195
                    },
                    {
                      "id": 4824,
                      "faiss_score": 0.8977358341217041,
                      "faiss_rank": 16,
                      "doc_id": "wiki_Quantum_error_correction",
                      "file_type": ".txt",
                      "position": 71,
                      "sentence": "In April 2024, researchers at Microsoft claimed to have successfully tested a quantum error correction code that allowed them to achieve an error rate with logical qubits that is 800 times better than the underlying physical error rate.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Quantum_error_correction",
                      "primary_category": "articles with short description",
                      "rerank_score": 4.153146743774414,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.0023480842355638742,
                        "neutral": 0.971402108669281,
                        "support": 0.026249852031469345
                      },
                      "stance_score": 0.02390176779590547,
                      "evidence_contribution": 0.09926754909201696,
                      "combined_rank_score": 5.050882577896118
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Large language models generate fluent text, perform many tasks, and generalize across domains, but hallucinate facts, encode societal biases, lack grounded reasoning, and require massive datasets.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "CONTRADICT",
      "subclaims": [
        {
          "subclaim": "Large language models generate fluent text",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 1.06089048643004,
            "contradict": 4.2170162808528975,
            "total": 5.277906767282937
          },
          "evidence": {
            "supporting": [
              {
                "id": 6042,
                "faiss_score": 0.8820071816444397,
                "faiss_rank": 8,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 2,
                "sentence": "This simple training signal, when combined with large datasets and high model capacity, produces systems that can generate coherent text, answer questions, summarize documents, and perform a wide variety of language-related tasks without explicit task-specific programming.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 3.112881660461426,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.008569732308387756,
                  "neutral": 0.6420539617538452,
                  "support": 0.34937629103660583
                },
                "stance_score": 0.3408065587282181,
                "evidence_contribution": 1.06089048643004,
                "combined_rank_score": 3.9948888421058655
              }
            ],
            "contradicting": [
              {
                "id": 6072,
                "faiss_score": 0.8710505366325378,
                "faiss_rank": 17,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 32,
                "sentence": "Another limitation of large language models is their lack of persistent memory beyond the context window.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.723887324333191,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.8763479590415955,
                  "neutral": 0.11980066448450089,
                  "support": 0.003851353656500578
                },
                "stance_score": -0.8724966053850949,
                "evidence_contribution": -1.5040858385471032,
                "combined_rank_score": 2.5949378609657288
              },
              {
                "id": 6121,
                "faiss_score": 0.9008355140686035,
                "faiss_rank": 2,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 81,
                "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 4.061986923217773,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.3488886058330536,
                  "neutral": 0.6439270973205566,
                  "support": 0.00718427961692214
                },
                "stance_score": -0.34170432621613145,
                "evidence_contribution": -1.3879985046968661,
                "combined_rank_score": 4.962822437286377
              },
              {
                "id": 2020,
                "faiss_score": 0.872475266456604,
                "faiss_rank": 15,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 288,
                "sentence": "Despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 1.3532600402832031,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.6882504224777222,
                  "neutral": 0.30820220708847046,
                  "support": 0.0035474055912345648
                },
                "stance_score": -0.6847030168864876,
                "evidence_contribution": -0.9265812322138389,
                "combined_rank_score": 2.225735306739807
              },
              {
                "id": 6047,
                "faiss_score": 0.8832986354827881,
                "faiss_rank": 7,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Despite these capabilities, the behavior of large language models remains fundamentally probabilistic.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 2.1860270500183105,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.18742139637470245,
                  "neutral": 0.8073830604553223,
                  "support": 0.005195515230298042
                },
                "stance_score": -0.1822258811444044,
                "evidence_contribution": -0.39835070539508965,
                "combined_rank_score": 3.0693256855010986
              }
            ],
            "neutral": [
              {
                "id": 6040,
                "faiss_score": 0.9094253778457642,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 0,
                "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 6.935098171234131,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.01122680027037859,
                  "neutral": 0.9780644178390503,
                  "support": 0.010708799585700035
                },
                "stance_score": -0.0005180006846785545,
                "evidence_contribution": -0.0035923856010122712,
                "combined_rank_score": 7.844523549079895
              },
              {
                "id": 6043,
                "faiss_score": 0.8940900564193726,
                "faiss_rank": 3,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 3,
                "sentence": "The defining feature of large language models is scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 2.7434394359588623,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.06802935898303986,
                  "neutral": 0.9228030443191528,
                  "support": 0.009167566895484924
                },
                "stance_score": -0.05886179208755493,
                "evidence_contribution": -0.16148376168420953,
                "combined_rank_score": 3.637529492378235
              },
              {
                "id": 6079,
                "faiss_score": 0.8712449669837952,
                "faiss_rank": 16,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "Large language models are often described as general-purpose systems because they can be adapted to many tasks with minimal fine-tuning.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 2.302955389022827,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0054175821132957935,
                  "neutral": 0.9915776252746582,
                  "support": 0.003004750469699502
                },
                "stance_score": -0.0024128316435962915,
                "evidence_contribution": -0.005556643636424885,
                "combined_rank_score": 3.1742003560066223
              }
            ]
          }
        },
        {
          "subclaim": "perform many tasks",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [
              {
                "id": 5691,
                "faiss_score": 0.8415981531143188,
                "faiss_rank": 8,
                "doc_id": "local_math_computation_limits.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Many important computational tasks fall into classes that are believed to be hard.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_computation_limits.txt",
                "primary_category": null,
                "rerank_score": -0.4781818985939026,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.006888913922011852,
                  "neutral": 0.5298597812652588,
                  "support": 0.4632512927055359
                },
                "stance_score": 0.45636237878352404,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.36341625452041626
              },
              {
                "id": 3881,
                "faiss_score": 0.8591969013214111,
                "faiss_rank": 4,
                "doc_id": "wiki_Latency_(engineering)",
                "file_type": ".txt",
                "position": 50,
                "sentence": "When all of the tasks are done at the same time, however, it is possible to reduce the latency to the length of the longest task.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Latency_(engineering)",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": -3.914613723754883,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.005307700484991074,
                  "neutral": 0.5157715082168579,
                  "support": 0.47892075777053833
                },
                "stance_score": 0.47361305728554726,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.0554168224334717
              },
              {
                "id": 1871,
                "faiss_score": 0.8370813727378845,
                "faiss_rank": 17,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 139,
                "sentence": "Alternatively, it can propose increasingly difficult tasks for curriculum learning.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -5.142264366149902,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.010847136378288269,
                  "neutral": 0.6954389810562134,
                  "support": 0.29371392726898193
                },
                "stance_score": 0.28286679089069366,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.305182993412018
              },
              {
                "id": 790,
                "faiss_score": 0.8379898071289062,
                "faiss_rank": 15,
                "doc_id": "wiki_Quantum_computing",
                "file_type": ".txt",
                "position": 194,
                "sentence": "Some promising tasks and applications require resources far beyond those available today.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Quantum_computing",
                "primary_category": "all wikipedia articles written in american english",
                "rerank_score": -5.572847843170166,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.02995305135846138,
                  "neutral": 0.8351297974586487,
                  "support": 0.13491712510585785
                },
                "stance_score": 0.10496407374739647,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.73485803604126
              },
              {
                "id": 6707,
                "faiss_score": 0.8720545768737793,
                "faiss_rank": 1,
                "doc_id": "local_systems_latency_throughput_tradeoffs.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "Some workloads consist of many small, independent requests, while others involve long-running operations.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_latency_throughput_tradeoffs.txt",
                "primary_category": null,
                "rerank_score": -6.178860664367676,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0020800072234123945,
                  "neutral": 0.29610347747802734,
                  "support": 0.7018164396286011
                },
                "stance_score": 0.6997364324051887,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.3068060874938965
              },
              {
                "id": 6434,
                "faiss_score": 0.8501757383346558,
                "faiss_rank": 5,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 173,
                "sentence": "Ensuring consistent behavior across tasks remains challenging.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -6.617842674255371,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.0040571861900389194,
                  "neutral": 0.6257938146591187,
                  "support": 0.37014898657798767
                },
                "stance_score": 0.36609180038794875,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.767666935920715
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 2193,
                "faiss_score": 0.8400170803070068,
                "faiss_rank": 12,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 124,
                "sentence": "All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": -3.0649709701538086,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.001011320622637868,
                  "neutral": 0.9790430068969727,
                  "support": 0.01994563639163971
                },
                "stance_score": 0.01893431576900184,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.2249538898468018
              },
              {
                "id": 6110,
                "faiss_score": 0.8411685824394226,
                "faiss_rank": 9,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 70,
                "sentence": "Tasks that once required specialized pipelines can now be addressed using general-purpose models.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -5.834390640258789,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.007752911187708378,
                  "neutral": 0.9306866526603699,
                  "support": 0.06156041473150253
                },
                "stance_score": 0.053807503543794155,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.9932220578193665
              },
              {
                "id": 5687,
                "faiss_score": 0.8360838890075684,
                "faiss_rank": 19,
                "doc_id": "local_math_computation_limits.txt",
                "file_type": ".txt",
                "position": 10,
                "sentence": "These categories provide a framework for understanding why certain tasks remain difficult despite advances in hardware.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_computation_limits.txt",
                "primary_category": null,
                "rerank_score": -7.54439640045166,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.07008431851863861,
                  "neutral": 0.8799291253089905,
                  "support": 0.049986544996500015
                },
                "stance_score": -0.020097773522138596,
                "evidence_contribution": -0.0,
                "combined_rank_score": -6.708312511444092
              }
            ]
          }
        },
        {
          "subclaim": "generalize across domains",
          "verdict": "MIXED",
          "controversial": true,
          "strengths": {
            "support": 0.3182367224006386,
            "contradict": 0.3649601796199988,
            "total": 0.6831969020206374
          },
          "evidence": {
            "supporting": [
              {
                "id": 6405,
                "faiss_score": 0.8751733899116516,
                "faiss_rank": 6,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 144,
                "sentence": "These representations encode statistical regularities of language and sequence structure, allowing models to generalize across contexts.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 0.37290430068969727,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.008740616962313652,
                  "neutral": 0.12911827862262726,
                  "support": 0.8621411323547363
                },
                "stance_score": 0.8534005153924227,
                "evidence_contribution": 0.3182367224006386,
                "combined_rank_score": 1.2480776906013489
              },
              {
                "id": 6128,
                "faiss_score": 0.875458836555481,
                "faiss_rank": 4,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 4,
                "sentence": "These trends have been observed across different domains and architectures, suggesting that scaling captures general properties of learning systems rather than task-specific quirks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -0.6710819602012634,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.006305638700723648,
                  "neutral": 0.11608841270208359,
                  "support": 0.8776058554649353
                },
                "stance_score": 0.8713002167642117,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.20437687635421753
              },
              {
                "id": 4323,
                "faiss_score": 0.8630200624465942,
                "faiss_rank": 16,
                "doc_id": "wiki_No_free_lunch_theorem",
                "file_type": ".txt",
                "position": 34,
                "sentence": "Moreover, the Kolmogorov complexity of machine learning models can be upper bounded through compressions of their data labeling, and it is possible to produce non-vacuous cross-domain generalization bounds via Kolmogorov complexity.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/No_free_lunch_theorem",
                "primary_category": "all articles needing additional references",
                "rerank_score": -2.4023613929748535,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.011967157945036888,
                  "neutral": 0.14573383331298828,
                  "support": 0.8422990441322327
                },
                "stance_score": 0.8303318861871958,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.5393413305282593
              }
            ],
            "contradicting": [
              {
                "id": 5562,
                "faiss_score": 0.8713772296905518,
                "faiss_rank": 9,
                "doc_id": "local_bio_gene_editing.txt",
                "file_type": ".txt",
                "position": 35,
                "sentence": "These factors complicate efforts to generalize results across systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_gene_editing.txt",
                "primary_category": null,
                "rerank_score": 1.713374137878418,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.4898774325847626,
                  "neutral": 0.23325194418430328,
                  "support": 0.2768707275390625
                },
                "stance_score": -0.21300670504570007,
                "evidence_contribution": -0.3649601796199988,
                "combined_rank_score": 2.5847513675689697
              }
            ],
            "neutral": [
              {
                "id": 6004,
                "faiss_score": 0.876814603805542,
                "faiss_rank": 3,
                "doc_id": "local_ml_evaluation_and_generalization.txt",
                "file_type": ".txt",
                "position": 30,
                "sentence": "Generalization is also influenced by data quality.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                "primary_category": null,
                "rerank_score": -0.932371973991394,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.021363889798521996,
                  "neutral": 0.923850417137146,
                  "support": 0.054785650223493576
                },
                "stance_score": 0.03342176042497158,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.05555737018585205
              },
              {
                "id": 6017,
                "faiss_score": 0.8753159642219543,
                "faiss_rank": 5,
                "doc_id": "local_ml_evaluation_and_generalization.txt",
                "file_type": ".txt",
                "position": 43,
                "sentence": "Generalization is not solely a property of models; it emerges from interactions between models, data, and deployment context.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                "primary_category": null,
                "rerank_score": -1.114978313446045,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.012674612924456596,
                  "neutral": 0.8975697755813599,
                  "support": 0.08975567668676376
                },
                "stance_score": 0.07708106376230717,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.23966234922409058
              },
              {
                "id": 5770,
                "faiss_score": 0.8777358531951904,
                "faiss_rank": 2,
                "doc_id": "local_math_information_theory_and_learning.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "Generalization requires extracting information that is predictive of unseen data while discarding irrelevant details.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_information_theory_and_learning.txt",
                "primary_category": null,
                "rerank_score": -1.220008134841919,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.004218193702399731,
                  "neutral": 0.9678018093109131,
                  "support": 0.027980053797364235
                },
                "stance_score": 0.023761860094964504,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.3422722816467285
              }
            ]
          }
        },
        {
          "subclaim": "hallucinate facts",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.2338354128140443,
            "contradict": 3.042030399540667,
            "total": 3.275865812354711
          },
          "evidence": {
            "supporting": [
              {
                "id": 1958,
                "faiss_score": 0.8864883184432983,
                "faiss_rank": 2,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 226,
                "sentence": "Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \"hallucination\".",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 0.47651806473731995,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.1281960904598236,
                  "neutral": 0.25289106369018555,
                  "support": 0.6189128756523132
                },
                "stance_score": 0.4907167851924896,
                "evidence_contribution": 0.2338354128140443,
                "combined_rank_score": 1.3630063831806183
              },
              {
                "id": 6062,
                "faiss_score": 0.8709315061569214,
                "faiss_rank": 6,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 22,
                "sentence": "Hallucinations can occur even when the model has seen relevant information during training, as generation depends on local likelihood rather than global verification.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -0.13184425234794617,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.16398467123508453,
                  "neutral": 0.570544958114624,
                  "support": 0.265470415353775
                },
                "stance_score": 0.10148574411869049,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.7390872538089752
              }
            ],
            "contradicting": [
              {
                "id": 2021,
                "faiss_score": 0.8814721703529358,
                "faiss_rank": 3,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 289,
                "sentence": "Hallucinations represent a fundamental challenge, wherein models generate syntactically fluent text that appears factually sound, but is internally inconsistent with training data or factually incorrect.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 2.190915584564209,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.6222884654998779,
                  "neutral": 0.27321556210517883,
                  "support": 0.10449595004320145
                },
                "stance_score": -0.5177925154566765,
                "evidence_contribution": -1.1344396916847366,
                "combined_rank_score": 3.0723877549171448
              },
              {
                "id": 1959,
                "faiss_score": 0.8755703568458557,
                "faiss_rank": 5,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 227,
                "sentence": "Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 1.0273222923278809,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.9376419186592102,
                  "neutral": 0.04097185656428337,
                  "support": 0.021386215463280678
                },
                "stance_score": -0.9162557031959295,
                "evidence_contribution": -0.9412899093657368,
                "combined_rank_score": 1.9028926491737366
              },
              {
                "id": 2022,
                "faiss_score": 0.8594096899032593,
                "faiss_rank": 7,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 290,
                "sentence": "These hallucinations arise partly through memorization of training data combined with extrapolation beyond factual boundaries, with evaluations demonstrating that models can output verbatim passages from training data, when subjected to specific prompting sequences.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 0.8990537524223328,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.8523752093315125,
                  "neutral": 0.1056460589170456,
                  "support": 0.04197878763079643
                },
                "stance_score": -0.810396421700716,
                "evidence_contribution": -0.7285899438796599,
                "combined_rank_score": 1.758463442325592
              },
              {
                "id": 6063,
                "faiss_score": 0.8869999647140503,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 23,
                "sentence": "The tendency to hallucinate is influenced by prompting, context length, and decoding strategies.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.881400227546692,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.28592926263809204,
                  "neutral": 0.5544893145561218,
                  "support": 0.15958142280578613
                },
                "stance_score": -0.1263478398323059,
                "evidence_contribution": -0.23771085461053332,
                "combined_rank_score": 2.768400192260742
              },
              {
                "id": 1961,
                "faiss_score": 0.8519169688224792,
                "faiss_rank": 8,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 229,
                "sentence": "Efforts to reduce or compensate for hallucinations have employed automated reasoning, retrieval-augmented generation (RAG), fine-tuning, and other methods.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -5.1598896980285645,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.7880223393440247,
                  "neutral": 0.18474780023097992,
                  "support": 0.02722986228764057
                },
                "stance_score": -0.7607924770563841,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.307972729206085
              },
              {
                "id": 6065,
                "faiss_score": 0.838513195514679,
                "faiss_rank": 9,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 25,
                "sentence": "Conversely, conservative decoding may reduce hallucination but lead to repetitive or overly cautious outputs.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -5.377496719360352,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.495147705078125,
                  "neutral": 0.4244162440299988,
                  "support": 0.08043601363897324
                },
                "stance_score": -0.41471169143915176,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.538983523845673
              },
              {
                "id": 6048,
                "faiss_score": 0.8023523092269897,
                "faiss_rank": 15,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 8,
                "sentence": "They do not store explicit facts or rules in a symbolic form.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -10.567695617675781,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.5973492860794067,
                  "neutral": 0.3601573705673218,
                  "support": 0.04249336197972298
                },
                "stance_score": -0.5548559240996838,
                "evidence_contribution": -0.0,
                "combined_rank_score": -9.765343308448792
              }
            ],
            "neutral": [
              {
                "id": 6060,
                "faiss_score": 0.8803726434707642,
                "faiss_rank": 4,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 20,
                "sentence": "Hallucination is one of the most widely discussed failure modes of large language models.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": 1.5415492057800293,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.2402685284614563,
                  "neutral": 0.5552210211753845,
                  "support": 0.20451043546199799
                },
                "stance_score": -0.03575809299945831,
                "evidence_contribution": -0.05512285986352339,
                "combined_rank_score": 2.4219218492507935
              }
            ]
          }
        },
        {
          "subclaim": "encode societal biases",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [
              {
                "id": 411,
                "faiss_score": 0.9120092988014221,
                "faiss_rank": 1,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 302,
                "sentence": "When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -3.2401061058044434,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.04815658554434776,
                  "neutral": 0.2623845934867859,
                  "support": 0.6894589066505432
                },
                "stance_score": 0.6413023211061954,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.3280968070030212
              },
              {
                "id": 412,
                "faiss_score": 0.911769449710846,
                "faiss_rank": 3,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 303,
                "sentence": "Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -3.7880208492279053,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0075705829076468945,
                  "neutral": 0.11338216811418533,
                  "support": 0.8790472149848938
                },
                "stance_score": 0.8714766320772469,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.8762513995170593
              },
              {
                "id": 419,
                "faiss_score": 0.8740242719650269,
                "faiss_rank": 8,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 310,
                "sentence": "Language models learned from data have been shown to contain human-like biases.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -5.875236988067627,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.046064868569374084,
                  "neutral": 0.7679744362831116,
                  "support": 0.18596072494983673
                },
                "stance_score": 0.13989585638046265,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.0012127161026
              }
            ],
            "contradicting": [
              {
                "id": 2038,
                "faiss_score": 0.8740885257720947,
                "faiss_rank": 7,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 306,
                "sentence": "Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -4.511942386627197,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.9876894950866699,
                  "neutral": 0.010320872068405151,
                  "support": 0.001989632612094283
                },
                "stance_score": -0.9856998624745756,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.6378538608551025
              },
              {
                "id": 2039,
                "faiss_score": 0.8821737170219421,
                "faiss_rank": 5,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 307,
                "sentence": "Language models may also exhibit political biases.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -6.484963417053223,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.5698038339614868,
                  "neutral": 0.17982693016529083,
                  "support": 0.25036919116973877
                },
                "stance_score": -0.31943464279174805,
                "evidence_contribution": -0.0,
                "combined_rank_score": -5.6027897000312805
              }
            ],
            "neutral": [
              {
                "id": 354,
                "faiss_score": 0.8632741570472717,
                "faiss_rank": 16,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 245,
                "sentence": "Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -4.579390525817871,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.23116815090179443,
                  "neutral": 0.47713980078697205,
                  "support": 0.2916921079158783
                },
                "stance_score": 0.06052395701408386,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.7161163687705994
              }
            ]
          }
        },
        {
          "subclaim": "lack grounded reasoning",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [
              {
                "id": 6111,
                "faiss_score": 0.8929279446601868,
                "faiss_rank": 1,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 71,
                "sentence": "At the same time, this progress has highlighted fundamental limitations related to grounding, reasoning, and reliability.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -4.8359198570251465,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.09977399557828903,
                  "neutral": 0.6214102506637573,
                  "support": 0.27881574630737305
                },
                "stance_score": 0.17904175072908401,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.9429919123649597
              },
              {
                "id": 2754,
                "faiss_score": 0.8560482859611511,
                "faiss_rank": 5,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 349,
                "sentence": "A main criticism concerns the lack of theory surrounding some methods.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": -7.450339317321777,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0030245366506278515,
                  "neutral": 0.24703992903232574,
                  "support": 0.7499355673789978
                },
                "stance_score": 0.74691103072837,
                "evidence_contribution": 0.0,
                "combined_rank_score": -6.594291031360626
              },
              {
                "id": 6187,
                "faiss_score": 0.8757100701332092,
                "faiss_rank": 3,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 63,
                "sentence": "It can unlock capabilities that are difficult to achieve otherwise, but it does not solve fundamental problems related to understanding, grounding, or reasoning.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -7.920139312744141,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.09269913285970688,
                  "neutral": 0.4912792444229126,
                  "support": 0.4160216450691223
                },
                "stance_score": 0.32332251220941544,
                "evidence_contribution": 0.0,
                "combined_rank_score": -7.044429242610931
              },
              {
                "id": 6376,
                "faiss_score": 0.848895788192749,
                "faiss_rank": 9,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 115,
                "sentence": "This vulnerability arises in part from their reliance on statistical patterns rather than explicit reasoning mechanisms.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -8.360023498535156,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0039749909192323685,
                  "neutral": 0.31459662318229675,
                  "support": 0.6814284920692444
                },
                "stance_score": 0.677453501150012,
                "evidence_contribution": 0.0,
                "combined_rank_score": -7.511127710342407
              },
              {
                "id": 1956,
                "faiss_score": 0.8405802249908447,
                "faiss_rank": 17,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 224,
                "sentence": "In contrast, some skeptics of LLM understanding believe that existing LLMs are \"simply remixing and recombining existing writing\", a phenomenon known as stochastic parrot, or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -9.912699699401855,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.012527014128863811,
                  "neutral": 0.624691367149353,
                  "support": 0.3627816140651703
                },
                "stance_score": 0.3502545999363065,
                "evidence_contribution": 0.0,
                "combined_rank_score": -9.07211947441101
              },
              {
                "id": 6054,
                "faiss_score": 0.8733642101287842,
                "faiss_rank": 4,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "Others counter that any appearance of understanding is an artifact of training on massive datasets and that the models lack grounding in real-world experience.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -10.256382942199707,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.011887310072779655,
                  "neutral": 0.4753391742706299,
                  "support": 0.5127735137939453
                },
                "stance_score": 0.5008862037211657,
                "evidence_contribution": 0.0,
                "combined_rank_score": -9.383018732070923
              },
              {
                "id": 5265,
                "faiss_score": 0.846418023109436,
                "faiss_rank": 12,
                "doc_id": "wiki_P_versus_NP_problem",
                "file_type": ".txt",
                "position": 90,
                "sentence": "This is, in my opinion, a very weak argument.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/P_versus_NP_problem",
                "primary_category": "1956 in computing",
                "rerank_score": -10.36082649230957,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0017298628808930516,
                  "neutral": 0.4352908432483673,
                  "support": 0.5629792213439941
                },
                "stance_score": 0.5612493584631011,
                "evidence_contribution": 0.0,
                "combined_rank_score": -9.514408469200134
              },
              {
                "id": 6122,
                "faiss_score": 0.8488896489143372,
                "faiss_rank": 10,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 82,
                "sentence": "Their strengths lie in flexibility, fluency, and scalability, while their weaknesses center on grounding, reliability, and interpretability.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -10.750747680664062,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.00916263833642006,
                  "neutral": 0.5942029356956482,
                  "support": 0.39663445949554443
                },
                "stance_score": 0.3874718211591244,
                "evidence_contribution": 0.0,
                "combined_rank_score": -9.901858031749725
              }
            ],
            "contradicting": [
              {
                "id": 5745,
                "faiss_score": 0.8376482725143433,
                "faiss_rank": 20,
                "doc_id": "local_math_computation_limits.txt",
                "file_type": ".txt",
                "position": 68,
                "sentence": "While theory does not predict all future discoveries, it provides a framework for skepticism grounded in evidence.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_computation_limits.txt",
                "primary_category": null,
                "rerank_score": -8.084772109985352,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.9659310579299927,
                  "neutral": 0.031816720962524414,
                  "support": 0.0022521638311445713
                },
                "stance_score": -0.9636788940988481,
                "evidence_contribution": -0.0,
                "combined_rank_score": -7.247123837471008
              },
              {
                "id": 6409,
                "faiss_score": 0.8798010349273682,
                "faiss_rank": 2,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 148,
                "sentence": "Tasks that require multi-step inference, logical consistency, or grounding in external reality often expose limitations.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -10.726405143737793,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.4127979576587677,
                  "neutral": 0.5286512970924377,
                  "support": 0.05855078622698784
                },
                "stance_score": -0.35424717143177986,
                "evidence_contribution": -0.0,
                "combined_rank_score": -9.846604108810425
              }
            ],
            "neutral": []
          }
        },
        {
          "subclaim": "require massive datasets",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 1.1530581552348924,
            "contradict": 0.0,
            "total": 1.1530581552348924
          },
          "evidence": {
            "supporting": [
              {
                "id": 6309,
                "faiss_score": 0.8829025030136108,
                "faiss_rank": 2,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 48,
                "sentence": "High-capacity models are prone to overfitting when data is scarce, and strong generalization typically requires pretraining on massive corpora.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": 1.0930875539779663,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.002582910005003214,
                  "neutral": 0.23085328936576843,
                  "support": 0.7665637731552124
                },
                "stance_score": 0.7639808631502092,
                "evidence_contribution": 0.8350979729868375,
                "combined_rank_score": 1.9759900569915771
              },
              {
                "id": 3968,
                "faiss_score": 0.8802938461303711,
                "faiss_rank": 4,
                "doc_id": "wiki_Throughput",
                "file_type": ".txt",
                "position": 63,
                "sentence": "Large data loads that require processing impose data processing requirements on hardware.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Network_throughput",
                "primary_category": "all articles needing additional references",
                "rerank_score": 0.3394312262535095,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.007633762899786234,
                  "neutral": 0.2982155978679657,
                  "support": 0.6941506266593933
                },
                "stance_score": 0.6865168637596071,
                "evidence_contribution": 0.23302526090963696,
                "combined_rank_score": 1.2197250723838806
              },
              {
                "id": 1394,
                "faiss_score": 0.8828780651092529,
                "faiss_rank": 3,
                "doc_id": "wiki_Overfitting",
                "file_type": ".txt",
                "position": 44,
                "sentence": "The optimal function usually needs verification on bigger or completely new datasets.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                "primary_category": "machine learning",
                "rerank_score": 0.20189504325389862,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.005039381794631481,
                  "neutral": 0.569232702255249,
                  "support": 0.42572787404060364
                },
                "stance_score": 0.42068849224597216,
                "evidence_contribution": 0.08493492133841794,
                "combined_rank_score": 1.0847731083631516
              },
              {
                "id": 6308,
                "faiss_score": 0.8858730792999268,
                "faiss_rank": 1,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 47,
                "sentence": "Another challenge associated with transformers is their reliance on large datasets for effective training.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -0.6717810034751892,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0018392824567854404,
                  "neutral": 0.18031391501426697,
                  "support": 0.8178468942642212
                },
                "stance_score": 0.8160076118074358,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.21409207582473755
              },
              {
                "id": 3002,
                "faiss_score": 0.8612390756607056,
                "faiss_rank": 20,
                "doc_id": "wiki_Transformer_(machine_learning_model)",
                "file_type": ".txt",
                "position": 57,
                "sentence": "The pretrain dataset is typically an unlabeled large corpus, such as The Pile.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
                "primary_category": "artificial intelligence",
                "rerank_score": -1.3166024684906006,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.011000572703778744,
                  "neutral": 0.7051967978477478,
                  "support": 0.28380265831947327
                },
                "stance_score": 0.2728020856156945,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.455363392829895
              },
              {
                "id": 6138,
                "faiss_score": 0.8628324270248413,
                "faiss_rank": 18,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 14,
                "sentence": "This observation has motivated large-scale data collection and curation efforts, as well as synthetic data generation in some settings.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -6.367038726806641,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.0027826400473713875,
                  "neutral": 0.6612303853034973,
                  "support": 0.3359869420528412
                },
                "stance_score": 0.3332043020054698,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.504206299781799
              },
              {
                "id": 1789,
                "faiss_score": 0.8626959919929504,
                "faiss_rank": 19,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 57,
                "sentence": "Training of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -6.865152359008789,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.005022028926759958,
                  "neutral": 0.6281405091285706,
                  "support": 0.3668374717235565
                },
                "stance_score": 0.36181544279679656,
                "evidence_contribution": 0.0,
                "combined_rank_score": -6.002456367015839
              },
              {
                "id": 350,
                "faiss_score": 0.8709700107574463,
                "faiss_rank": 10,
                "doc_id": "wiki_Machine_learning",
                "file_type": ".txt",
                "position": 241,
                "sentence": "When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                "primary_category": "machine learning",
                "rerank_score": -7.28029727935791,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.001810239045880735,
                  "neutral": 0.35577476024627686,
                  "support": 0.6424149870872498
                },
                "stance_score": 0.640604748041369,
                "evidence_contribution": 0.0,
                "combined_rank_score": -6.409327268600464
              }
            ],
            "contradicting": [],
            "neutral": [
              {
                "id": 5929,
                "faiss_score": 0.8740013241767883,
                "faiss_rank": 5,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 24,
                "sentence": "Large models require many iterations to converge, consuming significant compute.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -6.440184116363525,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.005193229299038649,
                  "neutral": 0.9560596346855164,
                  "support": 0.03874707594513893
                },
                "stance_score": 0.03355384664610028,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.566182792186737
              },
              {
                "id": 2393,
                "faiss_score": 0.8667515516281128,
                "faiss_rank": 16,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 324,
                "sentence": "Large and effective neural networks require considerable computing resources.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": -6.914121150970459,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.006497465074062347,
                  "neutral": 0.8906980156898499,
                  "support": 0.10280458629131317
                },
                "stance_score": 0.09630712121725082,
                "evidence_contribution": 0.0,
                "combined_rank_score": -6.047369599342346
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is contradicted by strong evidence.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "require massive datasets",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6309,
                      "faiss_score": 0.8829025030136108,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 48,
                      "sentence": "High-capacity models are prone to overfitting when data is scarce, and strong generalization typically requires pretraining on massive corpora.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 1.0930875539779663,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.002582910005003214,
                        "neutral": 0.23085328936576843,
                        "support": 0.7665637731552124
                      },
                      "stance_score": 0.7639808631502092,
                      "evidence_contribution": 0.8350979729868375,
                      "combined_rank_score": 1.9759900569915771
                    },
                    {
                      "id": 3968,
                      "faiss_score": 0.8802938461303711,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Throughput",
                      "file_type": ".txt",
                      "position": 63,
                      "sentence": "Large data loads that require processing impose data processing requirements on hardware.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Network_throughput",
                      "primary_category": "all articles needing additional references",
                      "rerank_score": 0.3394312262535095,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.007633762899786234,
                        "neutral": 0.2982155978679657,
                        "support": 0.6941506266593933
                      },
                      "stance_score": 0.6865168637596071,
                      "evidence_contribution": 0.23302526090963696,
                      "combined_rank_score": 1.2197250723838806
                    },
                    {
                      "id": 1394,
                      "faiss_score": 0.8828780651092529,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Overfitting",
                      "file_type": ".txt",
                      "position": 44,
                      "sentence": "The optimal function usually needs verification on bigger or completely new datasets.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Overfitting",
                      "primary_category": "machine learning",
                      "rerank_score": 0.20189504325389862,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.005039381794631481,
                        "neutral": 0.569232702255249,
                        "support": 0.42572787404060364
                      },
                      "stance_score": 0.42068849224597216,
                      "evidence_contribution": 0.08493492133841794,
                      "combined_rank_score": 1.0847731083631516
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 5929,
                      "faiss_score": 0.8740013241767883,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 24,
                      "sentence": "Large models require many iterations to converge, consuming significant compute.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -6.440184116363525,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.005193229299038649,
                        "neutral": 0.9560596346855164,
                        "support": 0.03874707594513893
                      },
                      "stance_score": 0.03355384664610028,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -5.566182792186737
                    },
                    {
                      "id": 2393,
                      "faiss_score": 0.8667515516281128,
                      "faiss_rank": 16,
                      "doc_id": "wiki_Artificial_neural_network",
                      "file_type": ".txt",
                      "position": 324,
                      "sentence": "Large and effective neural networks require considerable computing resources.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                      "primary_category": "neural networks",
                      "rerank_score": -6.914121150970459,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.006497465074062347,
                        "neutral": 0.8906980156898499,
                        "support": 0.10280458629131317
                      },
                      "stance_score": 0.09630712121725082,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -6.047369599342346
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Large language models generate fluent text",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6042,
                      "faiss_score": 0.8820071816444397,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "This simple training signal, when combined with large datasets and high model capacity, produces systems that can generate coherent text, answer questions, summarize documents, and perform a wide variety of language-related tasks without explicit task-specific programming.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 3.112881660461426,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.008569732308387756,
                        "neutral": 0.6420539617538452,
                        "support": 0.34937629103660583
                      },
                      "stance_score": 0.3408065587282181,
                      "evidence_contribution": 1.06089048643004,
                      "combined_rank_score": 3.9948888421058655
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6121,
                      "faiss_score": 0.9008355140686035,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 81,
                      "sentence": "Ultimately, large language models represent a powerful but imperfect approach to language processing.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 4.061986923217773,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.3488886058330536,
                        "neutral": 0.6439270973205566,
                        "support": 0.00718427961692214
                      },
                      "stance_score": -0.34170432621613145,
                      "evidence_contribution": -1.3879985046968661,
                      "combined_rank_score": 4.962822437286377
                    },
                    {
                      "id": 6047,
                      "faiss_score": 0.8832986354827881,
                      "faiss_rank": 7,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "Despite these capabilities, the behavior of large language models remains fundamentally probabilistic.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 2.1860270500183105,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.18742139637470245,
                        "neutral": 0.8073830604553223,
                        "support": 0.005195515230298042
                      },
                      "stance_score": -0.1822258811444044,
                      "evidence_contribution": -0.39835070539508965,
                      "combined_rank_score": 3.0693256855010986
                    },
                    {
                      "id": 6072,
                      "faiss_score": 0.8710505366325378,
                      "faiss_rank": 17,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 32,
                      "sentence": "Another limitation of large language models is their lack of persistent memory beyond the context window.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.723887324333191,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.8763479590415955,
                        "neutral": 0.11980066448450089,
                        "support": 0.003851353656500578
                      },
                      "stance_score": -0.8724966053850949,
                      "evidence_contribution": -1.5040858385471032,
                      "combined_rank_score": 2.5949378609657288
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6040,
                      "faiss_score": 0.9094253778457642,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 0,
                      "sentence": "Large language models are neural systems designed to process and generate human language by learning statistical patterns from large text corpora.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 6.935098171234131,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.01122680027037859,
                        "neutral": 0.9780644178390503,
                        "support": 0.010708799585700035
                      },
                      "stance_score": -0.0005180006846785545,
                      "evidence_contribution": -0.0035923856010122712,
                      "combined_rank_score": 7.844523549079895
                    },
                    {
                      "id": 6043,
                      "faiss_score": 0.8940900564193726,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 3,
                      "sentence": "The defining feature of large language models is scale.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 2.7434394359588623,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.06802935898303986,
                        "neutral": 0.9228030443191528,
                        "support": 0.009167566895484924
                      },
                      "stance_score": -0.05886179208755493,
                      "evidence_contribution": -0.16148376168420953,
                      "combined_rank_score": 3.637529492378235
                    },
                    {
                      "id": 6079,
                      "faiss_score": 0.8712449669837952,
                      "faiss_rank": 16,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 39,
                      "sentence": "Large language models are often described as general-purpose systems because they can be adapted to many tasks with minimal fine-tuning.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 2.302955389022827,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0054175821132957935,
                        "neutral": 0.9915776252746582,
                        "support": 0.003004750469699502
                      },
                      "stance_score": -0.0024128316435962915,
                      "evidence_contribution": -0.005556643636424885,
                      "combined_rank_score": 3.1742003560066223
                    }
                  ]
                }
              },
              {
                "subclaim": "hallucinate facts",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "strong"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 1958,
                      "faiss_score": 0.8864883184432983,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 226,
                      "sentence": "Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \"hallucination\".",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 0.47651806473731995,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.1281960904598236,
                        "neutral": 0.25289106369018555,
                        "support": 0.6189128756523132
                      },
                      "stance_score": 0.4907167851924896,
                      "evidence_contribution": 0.2338354128140443,
                      "combined_rank_score": 1.3630063831806183
                    },
                    {
                      "id": 6062,
                      "faiss_score": 0.8709315061569214,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 22,
                      "sentence": "Hallucinations can occur even when the model has seen relevant information during training, as generation depends on local likelihood rather than global verification.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -0.13184425234794617,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.16398467123508453,
                        "neutral": 0.570544958114624,
                        "support": 0.265470415353775
                      },
                      "stance_score": 0.10148574411869049,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.7390872538089752
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 2021,
                      "faiss_score": 0.8814721703529358,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 289,
                      "sentence": "Hallucinations represent a fundamental challenge, wherein models generate syntactically fluent text that appears factually sound, but is internally inconsistent with training data or factually incorrect.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 2.190915584564209,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.6222884654998779,
                        "neutral": 0.27321556210517883,
                        "support": 0.10449595004320145
                      },
                      "stance_score": -0.5177925154566765,
                      "evidence_contribution": -1.1344396916847366,
                      "combined_rank_score": 3.0723877549171448
                    },
                    {
                      "id": 6063,
                      "faiss_score": 0.8869999647140503,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 23,
                      "sentence": "The tendency to hallucinate is influenced by prompting, context length, and decoding strategies.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.881400227546692,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.28592926263809204,
                        "neutral": 0.5544893145561218,
                        "support": 0.15958142280578613
                      },
                      "stance_score": -0.1263478398323059,
                      "evidence_contribution": -0.23771085461053332,
                      "combined_rank_score": 2.768400192260742
                    },
                    {
                      "id": 1959,
                      "faiss_score": 0.8755703568458557,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 227,
                      "sentence": "Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 1.0273222923278809,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.9376419186592102,
                        "neutral": 0.04097185656428337,
                        "support": 0.021386215463280678
                      },
                      "stance_score": -0.9162557031959295,
                      "evidence_contribution": -0.9412899093657368,
                      "combined_rank_score": 1.9028926491737366
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6060,
                      "faiss_score": 0.8803726434707642,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 20,
                      "sentence": "Hallucination is one of the most widely discussed failure modes of large language models.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": 1.5415492057800293,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.2402685284614563,
                        "neutral": 0.5552210211753845,
                        "support": 0.20451043546199799
                      },
                      "stance_score": -0.03575809299945831,
                      "evidence_contribution": -0.05512285986352339,
                      "combined_rank_score": 2.4219218492507935
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "CONTROVERSIAL_ASPECTS",
            "items": [
              {
                "subclaim": "generalize across domains",
                "verdict": "MIXED",
                "strength_summary": {
                  "support": "weak",
                  "contradict": "weak"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6405,
                      "faiss_score": 0.8751733899116516,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 144,
                      "sentence": "These representations encode statistical regularities of language and sequence structure, allowing models to generalize across contexts.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": 0.37290430068969727,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.008740616962313652,
                        "neutral": 0.12911827862262726,
                        "support": 0.8621411323547363
                      },
                      "stance_score": 0.8534005153924227,
                      "evidence_contribution": 0.3182367224006386,
                      "combined_rank_score": 1.2480776906013489
                    },
                    {
                      "id": 6128,
                      "faiss_score": 0.875458836555481,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 4,
                      "sentence": "These trends have been observed across different domains and architectures, suggesting that scaling captures general properties of learning systems rather than task-specific quirks.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -0.6710819602012634,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.006305638700723648,
                        "neutral": 0.11608841270208359,
                        "support": 0.8776058554649353
                      },
                      "stance_score": 0.8713002167642117,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.20437687635421753
                    },
                    {
                      "id": 4323,
                      "faiss_score": 0.8630200624465942,
                      "faiss_rank": 16,
                      "doc_id": "wiki_No_free_lunch_theorem",
                      "file_type": ".txt",
                      "position": 34,
                      "sentence": "Moreover, the Kolmogorov complexity of machine learning models can be upper bounded through compressions of their data labeling, and it is possible to produce non-vacuous cross-domain generalization bounds via Kolmogorov complexity.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/No_free_lunch_theorem",
                      "primary_category": "all articles needing additional references",
                      "rerank_score": -2.4023613929748535,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.011967157945036888,
                        "neutral": 0.14573383331298828,
                        "support": 0.8422990441322327
                      },
                      "stance_score": 0.8303318861871958,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.5393413305282593
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 5562,
                      "faiss_score": 0.8713772296905518,
                      "faiss_rank": 9,
                      "doc_id": "local_bio_gene_editing.txt",
                      "file_type": ".txt",
                      "position": 35,
                      "sentence": "These factors complicate efforts to generalize results across systems.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\bio_gene_editing.txt",
                      "primary_category": null,
                      "rerank_score": 1.713374137878418,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.4898774325847626,
                        "neutral": 0.23325194418430328,
                        "support": 0.2768707275390625
                      },
                      "stance_score": -0.21300670504570007,
                      "evidence_contribution": -0.3649601796199988,
                      "combined_rank_score": 2.5847513675689697
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6004,
                      "faiss_score": 0.876814603805542,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_evaluation_and_generalization.txt",
                      "file_type": ".txt",
                      "position": 30,
                      "sentence": "Generalization is also influenced by data quality.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                      "primary_category": null,
                      "rerank_score": -0.932371973991394,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.021363889798521996,
                        "neutral": 0.923850417137146,
                        "support": 0.054785650223493576
                      },
                      "stance_score": 0.03342176042497158,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.05555737018585205
                    },
                    {
                      "id": 6017,
                      "faiss_score": 0.8753159642219543,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_evaluation_and_generalization.txt",
                      "file_type": ".txt",
                      "position": 43,
                      "sentence": "Generalization is not solely a property of models; it emerges from interactions between models, data, and deployment context.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                      "primary_category": null,
                      "rerank_score": -1.114978313446045,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.012674612924456596,
                        "neutral": 0.8975697755813599,
                        "support": 0.08975567668676376
                      },
                      "stance_score": 0.07708106376230717,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.23966234922409058
                    },
                    {
                      "id": 5770,
                      "faiss_score": 0.8777358531951904,
                      "faiss_rank": 2,
                      "doc_id": "local_math_information_theory_and_learning.txt",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "Generalization requires extracting information that is predictive of unseen data while discarding irrelevant details.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_information_theory_and_learning.txt",
                      "primary_category": null,
                      "rerank_score": -1.220008134841919,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.004218193702399731,
                        "neutral": 0.9678018093109131,
                        "support": 0.027980053797364235
                      },
                      "stance_score": 0.023761860094964504,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.3422722816467285
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "perform many tasks",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 5691,
                      "faiss_score": 0.8415981531143188,
                      "faiss_rank": 8,
                      "doc_id": "local_math_computation_limits.txt",
                      "file_type": ".txt",
                      "position": 14,
                      "sentence": "Many important computational tasks fall into classes that are believed to be hard.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_computation_limits.txt",
                      "primary_category": null,
                      "rerank_score": -0.4781818985939026,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.006888913922011852,
                        "neutral": 0.5298597812652588,
                        "support": 0.4632512927055359
                      },
                      "stance_score": 0.45636237878352404,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.36341625452041626
                    },
                    {
                      "id": 3881,
                      "faiss_score": 0.8591969013214111,
                      "faiss_rank": 4,
                      "doc_id": "wiki_Latency_(engineering)",
                      "file_type": ".txt",
                      "position": 50,
                      "sentence": "When all of the tasks are done at the same time, however, it is possible to reduce the latency to the length of the longest task.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Latency_(engineering)",
                      "primary_category": "all wikipedia articles written in american english",
                      "rerank_score": -3.914613723754883,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.005307700484991074,
                        "neutral": 0.5157715082168579,
                        "support": 0.47892075777053833
                      },
                      "stance_score": 0.47361305728554726,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -3.0554168224334717
                    },
                    {
                      "id": 1871,
                      "faiss_score": 0.8370813727378845,
                      "faiss_rank": 17,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 139,
                      "sentence": "Alternatively, it can propose increasingly difficult tasks for curriculum learning.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -5.142264366149902,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.010847136378288269,
                        "neutral": 0.6954389810562134,
                        "support": 0.29371392726898193
                      },
                      "stance_score": 0.28286679089069366,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -4.305182993412018
                    }
                  ],
                  "contradicting": [],
                  "neutral": [
                    {
                      "id": 2193,
                      "faiss_score": 0.8400170803070068,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Artificial_neural_network",
                      "file_type": ".txt",
                      "position": 124,
                      "sentence": "All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                      "primary_category": "neural networks",
                      "rerank_score": -3.0649709701538086,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.001011320622637868,
                        "neutral": 0.9790430068969727,
                        "support": 0.01994563639163971
                      },
                      "stance_score": 0.01893431576900184,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.2249538898468018
                    },
                    {
                      "id": 6110,
                      "faiss_score": 0.8411685824394226,
                      "faiss_rank": 9,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 70,
                      "sentence": "Tasks that once required specialized pipelines can now be addressed using general-purpose models.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -5.834390640258789,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.007752911187708378,
                        "neutral": 0.9306866526603699,
                        "support": 0.06156041473150253
                      },
                      "stance_score": 0.053807503543794155,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -4.9932220578193665
                    },
                    {
                      "id": 5687,
                      "faiss_score": 0.8360838890075684,
                      "faiss_rank": 19,
                      "doc_id": "local_math_computation_limits.txt",
                      "file_type": ".txt",
                      "position": 10,
                      "sentence": "These categories provide a framework for understanding why certain tasks remain difficult despite advances in hardware.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_computation_limits.txt",
                      "primary_category": null,
                      "rerank_score": -7.54439640045166,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.07008431851863861,
                        "neutral": 0.8799291253089905,
                        "support": 0.049986544996500015
                      },
                      "stance_score": -0.020097773522138596,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -6.708312511444092
                    }
                  ]
                }
              },
              {
                "subclaim": "encode societal biases",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 411,
                      "faiss_score": 0.9120092988014221,
                      "faiss_rank": 1,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 302,
                      "sentence": "When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -3.2401061058044434,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.04815658554434776,
                        "neutral": 0.2623845934867859,
                        "support": 0.6894589066505432
                      },
                      "stance_score": 0.6413023211061954,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.3280968070030212
                    },
                    {
                      "id": 412,
                      "faiss_score": 0.911769449710846,
                      "faiss_rank": 3,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 303,
                      "sentence": "Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -3.7880208492279053,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0075705829076468945,
                        "neutral": 0.11338216811418533,
                        "support": 0.8790472149848938
                      },
                      "stance_score": 0.8714766320772469,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.8762513995170593
                    },
                    {
                      "id": 419,
                      "faiss_score": 0.8740242719650269,
                      "faiss_rank": 8,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 310,
                      "sentence": "Language models learned from data have been shown to contain human-like biases.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -5.875236988067627,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.046064868569374084,
                        "neutral": 0.7679744362831116,
                        "support": 0.18596072494983673
                      },
                      "stance_score": 0.13989585638046265,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -5.0012127161026
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 2038,
                      "faiss_score": 0.8740885257720947,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 306,
                      "sentence": "Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -4.511942386627197,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.9876894950866699,
                        "neutral": 0.010320872068405151,
                        "support": 0.001989632612094283
                      },
                      "stance_score": -0.9856998624745756,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -3.6378538608551025
                    },
                    {
                      "id": 2039,
                      "faiss_score": 0.8821737170219421,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 307,
                      "sentence": "Language models may also exhibit political biases.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -6.484963417053223,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.5698038339614868,
                        "neutral": 0.17982693016529083,
                        "support": 0.25036919116973877
                      },
                      "stance_score": -0.31943464279174805,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -5.6027897000312805
                    }
                  ],
                  "neutral": [
                    {
                      "id": 354,
                      "faiss_score": 0.8632741570472717,
                      "faiss_rank": 16,
                      "doc_id": "wiki_Machine_learning",
                      "file_type": ".txt",
                      "position": 245,
                      "sentence": "Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Machine_learning",
                      "primary_category": "machine learning",
                      "rerank_score": -4.579390525817871,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.23116815090179443,
                        "neutral": 0.47713980078697205,
                        "support": 0.2916921079158783
                      },
                      "stance_score": 0.06052395701408386,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -3.7161163687705994
                    }
                  ]
                }
              },
              {
                "subclaim": "lack grounded reasoning",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6111,
                      "faiss_score": 0.8929279446601868,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 71,
                      "sentence": "At the same time, this progress has highlighted fundamental limitations related to grounding, reasoning, and reliability.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -4.8359198570251465,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.09977399557828903,
                        "neutral": 0.6214102506637573,
                        "support": 0.27881574630737305
                      },
                      "stance_score": 0.17904175072908401,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -3.9429919123649597
                    },
                    {
                      "id": 2754,
                      "faiss_score": 0.8560482859611511,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 349,
                      "sentence": "A main criticism concerns the lack of theory surrounding some methods.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": -7.450339317321777,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0030245366506278515,
                        "neutral": 0.24703992903232574,
                        "support": 0.7499355673789978
                      },
                      "stance_score": 0.74691103072837,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -6.594291031360626
                    },
                    {
                      "id": 6187,
                      "faiss_score": 0.8757100701332092,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 63,
                      "sentence": "It can unlock capabilities that are difficult to achieve otherwise, but it does not solve fundamental problems related to understanding, grounding, or reasoning.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -7.920139312744141,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.09269913285970688,
                        "neutral": 0.4912792444229126,
                        "support": 0.4160216450691223
                      },
                      "stance_score": 0.32332251220941544,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -7.044429242610931
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 5745,
                      "faiss_score": 0.8376482725143433,
                      "faiss_rank": 20,
                      "doc_id": "local_math_computation_limits.txt",
                      "file_type": ".txt",
                      "position": 68,
                      "sentence": "While theory does not predict all future discoveries, it provides a framework for skepticism grounded in evidence.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_computation_limits.txt",
                      "primary_category": null,
                      "rerank_score": -8.084772109985352,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.9659310579299927,
                        "neutral": 0.031816720962524414,
                        "support": 0.0022521638311445713
                      },
                      "stance_score": -0.9636788940988481,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -7.247123837471008
                    },
                    {
                      "id": 6409,
                      "faiss_score": 0.8798010349273682,
                      "faiss_rank": 2,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 148,
                      "sentence": "Tasks that require multi-step inference, logical consistency, or grounding in external reality often expose limitations.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -10.726405143737793,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.4127979576587677,
                        "neutral": 0.5286512970924377,
                        "support": 0.05855078622698784
                      },
                      "stance_score": -0.35424717143177986,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -9.846604108810425
                    }
                  ],
                  "neutral": []
                }
              }
            ]
          }
        ]
      }
    },
    {
      "claim": "Increasing computational resources improves deep learning training speed, enables larger models, and stabilizes optimization, but increases cost, energy usage, hardware dependence, and environmental impact.",
      "expected_verdict": "MIXED",
      "predicted_verdict": "SUPPORT",
      "subclaims": [
        {
          "subclaim": "Increasing computational resources improves deep learning training speed",
          "verdict": "MIXED",
          "controversial": true,
          "strengths": {
            "support": 0.37336022577575034,
            "contradict": 0.31031676614679327,
            "total": 0.6836769919225436
          },
          "evidence": {
            "supporting": [
              {
                "id": 2164,
                "faiss_score": 0.8744220733642578,
                "faiss_rank": 13,
                "doc_id": "wiki_Artificial_neural_network",
                "file_type": ".txt",
                "position": 95,
                "sentence": "Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                "primary_category": "neural networks",
                "rerank_score": 3.0568156242370605,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0037023804616183043,
                  "neutral": 0.8704550266265869,
                  "support": 0.1258426308631897
                },
                "stance_score": 0.12214025040157139,
                "evidence_contribution": 0.37336022577575034,
                "combined_rank_score": 3.9312376976013184
              },
              {
                "id": 2625,
                "faiss_score": 0.87431401014328,
                "faiss_rank": 14,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 220,
                "sentence": "Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": -0.11400533467531204,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.003962540999054909,
                  "neutral": 0.5770430564880371,
                  "support": 0.41899439692497253
                },
                "stance_score": 0.4150318559259176,
                "evidence_contribution": 0.0,
                "combined_rank_score": 0.760308675467968
              },
              {
                "id": 1901,
                "faiss_score": 0.8789441585540771,
                "faiss_rank": 9,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 169,
                "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -1.2347545623779297,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.005857494659721851,
                  "neutral": 0.885772168636322,
                  "support": 0.10837028920650482
                },
                "stance_score": 0.10251279454678297,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.35581040382385254
              }
            ],
            "contradicting": [
              {
                "id": 5906,
                "faiss_score": 0.877042293548584,
                "faiss_rank": 10,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 1.090013861656189,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.29159143567085266,
                  "neutral": 0.7015077471733093,
                  "support": 0.006900775246322155
                },
                "stance_score": -0.2846906604245305,
                "evidence_contribution": -0.31031676614679327,
                "combined_rank_score": 1.967056155204773
              },
              {
                "id": 5907,
                "faiss_score": 0.8837117552757263,
                "faiss_rank": 6,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 2,
                "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -1.9216840267181396,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.45006662607192993,
                  "neutral": 0.5438736081123352,
                  "support": 0.006059776060283184
                },
                "stance_score": -0.44400685001164675,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.0379722714424133
              }
            ],
            "neutral": [
              {
                "id": 2624,
                "faiss_score": 0.8756795525550842,
                "faiss_rank": 12,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 219,
                "sentence": "OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 3.08569598197937,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.05276564508676529,
                  "neutral": 0.9433174729347229,
                  "support": 0.003916928544640541
                },
                "stance_score": -0.04884871654212475,
                "evidence_contribution": -0.15073228835888353,
                "combined_rank_score": 3.9613755345344543
              },
              {
                "id": 2622,
                "faiss_score": 0.892419695854187,
                "faiss_rank": 2,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 217,
                "sentence": "Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": 1.4879298210144043,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0014016155619174242,
                  "neutral": 0.9192524552345276,
                  "support": 0.0793459415435791
                },
                "stance_score": 0.07794432598166168,
                "evidence_contribution": 0.11597568700698224,
                "combined_rank_score": 2.3803495168685913
              },
              {
                "id": 6044,
                "faiss_score": 0.890583872795105,
                "faiss_rank": 3,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 4,
                "sentence": "Increasing the number of parameters, the amount of training data, and the total compute used during training has been shown to produce consistent improvements in performance across many benchmarks.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -0.8982527256011963,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.0011553606018424034,
                  "neutral": 0.9447371959686279,
                  "support": 0.0541074313223362
                },
                "stance_score": 0.052952070720493793,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.007668852806091309
              }
            ]
          }
        },
        {
          "subclaim": "Increasing computational resources enables larger models",
          "verdict": "CONTRADICT",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.5486682067239927,
            "total": 0.5486682067239927
          },
          "evidence": {
            "supporting": [
              {
                "id": 6131,
                "faiss_score": 0.8856824636459351,
                "faiss_rank": 20,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 7,
                "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": -2.550759792327881,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0036448510363698006,
                  "neutral": 0.844368040561676,
                  "support": 0.15198716521263123
                },
                "stance_score": 0.14834231417626143,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.6650773286819458
              }
            ],
            "contradicting": [
              {
                "id": 6147,
                "faiss_score": 0.88716059923172,
                "faiss_rank": 19,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 23,
                "sentence": "For example, increasing model size without increasing data may yield limited benefits, while increasing data without sufficient model capacity may fail to exploit the additional information.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 1.6675896644592285,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.3358306884765625,
                  "neutral": 0.6573573350906372,
                  "support": 0.006811974570155144
                },
                "stance_score": -0.32901871390640736,
                "evidence_contribution": -0.5486682067239927,
                "combined_rank_score": 2.5547502636909485
              }
            ],
            "neutral": [
              {
                "id": 5907,
                "faiss_score": 0.8947705626487732,
                "faiss_rank": 10,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 2,
                "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": 3.712005615234375,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.23810796439647675,
                  "neutral": 0.5151563882827759,
                  "support": 0.24673567712306976
                },
                "stance_score": 0.008627712726593018,
                "evidence_contribution": 0.03202611808774236,
                "combined_rank_score": 4.606776177883148
              },
              {
                "id": 1901,
                "faiss_score": 0.8953041434288025,
                "faiss_rank": 9,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 169,
                "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": 1.582042932510376,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0021848832257092,
                  "neutral": 0.9945988655090332,
                  "support": 0.003216272220015526
                },
                "stance_score": 0.001031388994306326,
                "evidence_contribution": 0.0016317016691113073,
                "combined_rank_score": 2.4773470759391785
              },
              {
                "id": 6132,
                "faiss_score": 0.9344620704650879,
                "faiss_rank": 1,
                "doc_id": "local_ml_scaling_and_efficiency.txt",
                "file_type": ".txt",
                "position": 8,
                "sentence": "Larger models have greater representational capacity, allowing them to fit more complex functions.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                "primary_category": null,
                "rerank_score": 0.2170807421207428,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0012542104814201593,
                  "neutral": 0.9608927369117737,
                  "support": 0.03785300627350807
                },
                "stance_score": 0.03659879579208791,
                "evidence_contribution": 0.007944893751271963,
                "combined_rank_score": 1.1515428125858307
              }
            ]
          }
        },
        {
          "subclaim": "Increasing computational resources stabilizes optimization",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 5890,
                "faiss_score": 0.8737372756004333,
                "faiss_rank": 15,
                "doc_id": "local_math_optimization_and_convergence.txt",
                "file_type": ".txt",
                "position": 59,
                "sentence": "Asynchronous optimization can improve throughput but may slow convergence or introduce bias.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                "primary_category": null,
                "rerank_score": -4.720156669616699,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.6580743789672852,
                  "neutral": 0.33626577258110046,
                  "support": 0.005659839604049921
                },
                "stance_score": -0.6524145393632352,
                "evidence_contribution": -0.0,
                "combined_rank_score": -3.846419394016266
              },
              {
                "id": 79,
                "faiss_score": 0.8764380216598511,
                "faiss_rank": 10,
                "doc_id": "openreview_ztgT8Iok130",
                "file_type": ".txt",
                "position": 1,
                "sentence": "Computational methods have achieved initial success but still struggle with simultaneously optimizing multiple competing properties in a sample-efficient manner.",
                "source_type": "openreview",
                "credibility": 0.7,
                "source_url": "https://openreview.net/forum?id=ztgT8Iok130",
                "primary_category": null,
                "rerank_score": -7.7861409187316895,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.3723789155483246,
                  "neutral": 0.6259058713912964,
                  "support": 0.001715235412120819
                },
                "stance_score": -0.37066368013620377,
                "evidence_contribution": -0.0,
                "combined_rank_score": -6.909702897071838
              }
            ],
            "neutral": [
              {
                "id": 5868,
                "faiss_score": 0.8724570274353027,
                "faiss_rank": 19,
                "doc_id": "local_math_optimization_and_convergence.txt",
                "file_type": ".txt",
                "position": 37,
                "sentence": "Regularization can improve generalization and stabilize optimization by smoothing the objective landscape.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                "primary_category": null,
                "rerank_score": -1.1302709579467773,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.028316836804151535,
                  "neutral": 0.9502172470092773,
                  "support": 0.021465880796313286
                },
                "stance_score": -0.006850956007838249,
                "evidence_contribution": -0.0,
                "combined_rank_score": -0.2578139305114746
              },
              {
                "id": 1901,
                "faiss_score": 0.8825475573539734,
                "faiss_rank": 5,
                "doc_id": "wiki_Large_language_model",
                "file_type": ".txt",
                "position": 169,
                "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                "primary_category": "natural language processing",
                "rerank_score": -1.6149537563323975,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.02632685750722885,
                  "neutral": 0.9125757217407227,
                  "support": 0.06109738349914551
                },
                "stance_score": 0.034770525991916656,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.7324061989784241
              },
              {
                "id": 5857,
                "faiss_score": 0.8886821269989014,
                "faiss_rank": 2,
                "doc_id": "local_math_optimization_and_convergence.txt",
                "file_type": ".txt",
                "position": 26,
                "sentence": "Balancing exploration and stability is a recurring theme in optimization theory and practice.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                "primary_category": null,
                "rerank_score": -3.3342981338500977,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.004754449240863323,
                  "neutral": 0.9945986270904541,
                  "support": 0.0006469713989645243
                },
                "stance_score": -0.004107477841898799,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.4456160068511963
              }
            ]
          }
        },
        {
          "subclaim": "Increasing computational resources increases cost",
          "verdict": "SUPPORT",
          "controversial": false,
          "strengths": {
            "support": 2.475734798548877,
            "contradict": 0.0,
            "total": 2.475734798548877
          },
          "evidence": {
            "supporting": [
              {
                "id": 5995,
                "faiss_score": 0.8827309012413025,
                "faiss_rank": 5,
                "doc_id": "local_ml_evaluation_and_generalization.txt",
                "file_type": ".txt",
                "position": 21,
                "sentence": "This technique reduces variance in evaluation estimates but increases computational cost.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                "primary_category": null,
                "rerank_score": 4.090311050415039,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.001421174150891602,
                  "neutral": 0.3918896019458771,
                  "support": 0.6066892743110657
                },
                "stance_score": 0.6052681001601741,
                "evidence_contribution": 2.475734798548877,
                "combined_rank_score": 4.9730419516563416
              },
              {
                "id": 6651,
                "faiss_score": 0.8786903619766235,
                "faiss_rank": 13,
                "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                "file_type": ".txt",
                "position": 54,
                "sentence": "Adding redundancy increases cost and complexity.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                "primary_category": null,
                "rerank_score": -1.3506048917770386,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0019581627566367388,
                  "neutral": 0.4421144723892212,
                  "support": 0.5559273958206177
                },
                "stance_score": 0.5539692330639809,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.47191452980041504
              },
              {
                "id": 2503,
                "faiss_score": 0.881470799446106,
                "faiss_rank": 7,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 98,
                "sentence": "However, those were more computationally expensive compared to backpropagation.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": -6.590202331542969,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.0016182546969503164,
                  "neutral": 0.8172633647918701,
                  "support": 0.18111830949783325
                },
                "stance_score": 0.17950005480088294,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.708731532096863
              }
            ],
            "contradicting": [
              {
                "id": 5923,
                "faiss_score": 0.8798081874847412,
                "faiss_rank": 8,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 18,
                "sentence": "This technique shifts computational cost from deployment to training.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -2.631453037261963,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.14017978310585022,
                  "neutral": 0.8297988176345825,
                  "support": 0.030021382495760918
                },
                "stance_score": -0.1101584006100893,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.7516448497772217
              }
            ],
            "neutral": [
              {
                "id": 6092,
                "faiss_score": 0.8832039833068848,
                "faiss_rank": 4,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 52,
                "sentence": "The computational cost of training large language models is substantial.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -3.503066062927246,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.0005319765186868608,
                  "neutral": 0.9969078898429871,
                  "support": 0.002560136839747429
                },
                "stance_score": 0.002028160321060568,
                "evidence_contribution": 0.0,
                "combined_rank_score": -2.6198620796203613
              },
              {
                "id": 2615,
                "faiss_score": 0.8780773878097534,
                "faiss_rank": 15,
                "doc_id": "wiki_Deep_learning",
                "file_type": ".txt",
                "position": 210,
                "sentence": "Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                "primary_category": "deep learning",
                "rerank_score": -4.063608646392822,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.0008609522483311594,
                  "neutral": 0.9922782182693481,
                  "support": 0.006860824301838875
                },
                "stance_score": 0.0059998720535077155,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.185531258583069
              },
              {
                "id": 6527,
                "faiss_score": 0.8844648003578186,
                "faiss_rank": 3,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 56,
                "sentence": "Software plays a critical role in determining how effectively computational resources are used.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": -5.428127765655518,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.00802410114556551,
                  "neutral": 0.9895147085189819,
                  "support": 0.002461161930114031
                },
                "stance_score": -0.005562939215451479,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.543662965297699
              }
            ]
          }
        },
        {
          "subclaim": "Increasing computational resources increases energy usage",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [
              {
                "id": 5907,
                "faiss_score": 0.8672863245010376,
                "faiss_rank": 16,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 2,
                "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -1.2218883037567139,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.011085989885032177,
                  "neutral": 0.8547480702400208,
                  "support": 0.1341659426689148
                },
                "stance_score": 0.12307995278388262,
                "evidence_contribution": 0.0,
                "combined_rank_score": -0.35460197925567627
              },
              {
                "id": 5432,
                "faiss_score": 0.9075989723205566,
                "faiss_rank": 2,
                "doc_id": "wiki_Landauer's_principle",
                "file_type": ".txt",
                "position": 2,
                "sentence": "As of 2012, modern computers use about a billion times as much energy per operation.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Landauer%27s_principle",
                "primary_category": "all articles containing potentially dated statements",
                "rerank_score": -2.027003526687622,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0011741387424990535,
                  "neutral": 0.6837896704673767,
                  "support": 0.31503620743751526
                },
                "stance_score": 0.3138620686950162,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.1194045543670654
              },
              {
                "id": 5381,
                "faiss_score": 0.8766747713088989,
                "faiss_rank": 11,
                "doc_id": "wiki_Reversible_computing",
                "file_type": ".txt",
                "position": 6,
                "sentence": "Reversible computing proponents argue that a significant portion of this energy consumption is due to architectural overheads.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Reversible_computing",
                "primary_category": "articles with short description",
                "rerank_score": -2.1637558937072754,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.03369560465216637,
                  "neutral": 0.725196897983551,
                  "support": 0.24110743403434753
                },
                "stance_score": 0.20741182938218117,
                "evidence_contribution": 0.0,
                "combined_rank_score": -1.2870811223983765
              },
              {
                "id": 5952,
                "faiss_score": 0.909400463104248,
                "faiss_rank": 1,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 47,
                "sentence": "Energy efficiency has become increasingly important as machine learning workloads scale.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -4.106639862060547,
                "rerank_rank": 5,
                "probs": {
                  "contradict": 0.09253448247909546,
                  "neutral": 0.5258767008781433,
                  "support": 0.38158881664276123
                },
                "stance_score": 0.28905433416366577,
                "evidence_contribution": 0.0,
                "combined_rank_score": -3.197239398956299
              },
              {
                "id": 6484,
                "faiss_score": 0.8717974424362183,
                "faiss_rank": 13,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 13,
                "sentence": "Increasing clock speeds leads to higher energy dissipation, which must be managed to prevent overheating.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": -5.127678871154785,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.001851145876571536,
                  "neutral": 0.1359056979417801,
                  "support": 0.8622431755065918
                },
                "stance_score": 0.8603920296300203,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.255881428718567
              },
              {
                "id": 6483,
                "faiss_score": 0.8976882696151733,
                "faiss_rank": 3,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 12,
                "sentence": "Power consumption has emerged as a dominant constraint in modern computing systems.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": -6.0382490158081055,
                "rerank_rank": 9,
                "probs": {
                  "contradict": 0.001076680375263095,
                  "neutral": 0.8365802764892578,
                  "support": 0.16234302520751953
                },
                "stance_score": 0.16126634483225644,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.140560746192932
              }
            ],
            "contradicting": [
              {
                "id": 5954,
                "faiss_score": 0.8787448406219482,
                "faiss_rank": 8,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 49,
                "sentence": "Techniques that reduce computation or enable reuse of pretrained components can lower energy consumption.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -2.529705047607422,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.9419150948524475,
                  "neutral": 0.05364186316728592,
                  "support": 0.004443134646862745
                },
                "stance_score": -0.9374719602055848,
                "evidence_contribution": -0.0,
                "combined_rank_score": -1.6509602069854736
              },
              {
                "id": 5380,
                "faiss_score": 0.8628027439117432,
                "faiss_rank": 20,
                "doc_id": "wiki_Reversible_computing",
                "file_type": ".txt",
                "position": 5,
                "sentence": "The Landauer limit was millions of times below the energy consumption of computers in the 2000s and thousands of times less in the 2010s.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Reversible_computing",
                "primary_category": "articles with short description",
                "rerank_score": -5.459789276123047,
                "rerank_rank": 7,
                "probs": {
                  "contradict": 0.1545737087726593,
                  "neutral": 0.8282281756401062,
                  "support": 0.01719808205962181
                },
                "stance_score": -0.1373756267130375,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.596986532211304
              }
            ],
            "neutral": [
              {
                "id": 5382,
                "faiss_score": 0.8740999698638916,
                "faiss_rank": 12,
                "doc_id": "wiki_Reversible_computing",
                "file_type": ".txt",
                "position": 7,
                "sentence": "These overheads are the energy costs associated with non-computational parts of the system, such as wires, transistors, and memory, that are required to make a computer work.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Reversible_computing",
                "primary_category": "articles with short description",
                "rerank_score": -5.730232238769531,
                "rerank_rank": 8,
                "probs": {
                  "contradict": 0.0461512990295887,
                  "neutral": 0.9230378866195679,
                  "support": 0.03081084042787552
                },
                "stance_score": -0.01534045860171318,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.85613226890564
              },
              {
                "id": 6527,
                "faiss_score": 0.8845221996307373,
                "faiss_rank": 6,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 56,
                "sentence": "Software plays a critical role in determining how effectively computational resources are used.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": -6.074432849884033,
                "rerank_rank": 10,
                "probs": {
                  "contradict": 0.0268668495118618,
                  "neutral": 0.9703036546707153,
                  "support": 0.002829460659995675
                },
                "stance_score": -0.024037388851866126,
                "evidence_contribution": -0.0,
                "combined_rank_score": -5.189910650253296
              }
            ]
          }
        },
        {
          "subclaim": "Increasing computational resources increases hardware dependence",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 6495,
                "faiss_score": 0.872918426990509,
                "faiss_rank": 20,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 24,
                "sentence": "The design of computing systems increasingly involves co-optimization of hardware and software.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": -7.28829288482666,
                "rerank_rank": 6,
                "probs": {
                  "contradict": 0.36655545234680176,
                  "neutral": 0.6029763221740723,
                  "support": 0.030468249693512917
                },
                "stance_score": -0.33608720265328884,
                "evidence_contribution": -0.0,
                "combined_rank_score": -6.415374457836151
              }
            ],
            "neutral": [
              {
                "id": 6530,
                "faiss_score": 0.8772687911987305,
                "faiss_rank": 11,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 59,
                "sentence": "This dependence underscores the importance of considering computation as a layered system.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": -3.2304980754852295,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.0025236783549189568,
                  "neutral": 0.9954648613929749,
                  "support": 0.002011485630646348
                },
                "stance_score": -0.0005121927242726088,
                "evidence_contribution": -0.0,
                "combined_rank_score": -2.353229284286499
              },
              {
                "id": 5944,
                "faiss_score": 0.8837687969207764,
                "faiss_rank": 5,
                "doc_id": "local_ml_efficiency_and_compression.txt",
                "file_type": ".txt",
                "position": 39,
                "sentence": "Hardware plays a significant role in shaping efficiency strategies.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                "primary_category": null,
                "rerank_score": -5.977171897888184,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.0010084060486406088,
                  "neutral": 0.9978429079055786,
                  "support": 0.0011486936127766967
                },
                "stance_score": 0.0001402875641360879,
                "evidence_contribution": 0.0,
                "combined_rank_score": -5.093403100967407
              },
              {
                "id": 5900,
                "faiss_score": 0.8796184062957764,
                "faiss_rank": 9,
                "doc_id": "local_math_optimization_and_convergence.txt",
                "file_type": ".txt",
                "position": 69,
                "sentence": "Communication, memory access, and hardware utilization influence practical convergence speed.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                "primary_category": null,
                "rerank_score": -6.411072731018066,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.0015513167018070817,
                  "neutral": 0.997253954410553,
                  "support": 0.0011947569437325
                },
                "stance_score": -0.0003565597580745816,
                "evidence_contribution": -0.0,
                "combined_rank_score": -5.53145432472229
              }
            ]
          }
        },
        {
          "subclaim": "Increasing computational resources increases environmental impact",
          "verdict": "INCONCLUSIVE",
          "controversial": false,
          "strengths": {
            "support": 0.0,
            "contradict": 0.0,
            "total": 0.0
          },
          "evidence": {
            "supporting": [],
            "contradicting": [
              {
                "id": 6534,
                "faiss_score": 0.8964979648590088,
                "faiss_rank": 1,
                "doc_id": "local_physics_computation.txt",
                "file_type": ".txt",
                "position": 63,
                "sentence": "Research in computation increasingly emphasizes efficiency and sustainability.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                "primary_category": null,
                "rerank_score": -5.121312141418457,
                "rerank_rank": 2,
                "probs": {
                  "contradict": 0.28368353843688965,
                  "neutral": 0.7062990665435791,
                  "support": 0.010017365217208862
                },
                "stance_score": -0.2736661732196808,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.224814176559448
              }
            ],
            "neutral": [
              {
                "id": 6094,
                "faiss_score": 0.8690935373306274,
                "faiss_rank": 9,
                "doc_id": "local_ml_llms.txt",
                "file_type": ".txt",
                "position": 54,
                "sentence": "These costs limit participation to well-resourced organizations and raise concerns about environmental impact.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                "primary_category": null,
                "rerank_score": -4.8720855712890625,
                "rerank_rank": 1,
                "probs": {
                  "contradict": 0.009297624230384827,
                  "neutral": 0.9804723858833313,
                  "support": 0.010230030864477158
                },
                "stance_score": 0.0009324066340923309,
                "evidence_contribution": 0.0,
                "combined_rank_score": -4.002992033958435
              },
              {
                "id": 6310,
                "faiss_score": 0.8743864297866821,
                "faiss_rank": 7,
                "doc_id": "local_ml_transformers.txt",
                "file_type": ".txt",
                "position": 49,
                "sentence": "This dependence raises concerns about accessibility, reproducibility, and environmental impact.",
                "source_type": "local",
                "credibility": 0.9,
                "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                "primary_category": null,
                "rerank_score": -5.240941524505615,
                "rerank_rank": 3,
                "probs": {
                  "contradict": 0.005509320180863142,
                  "neutral": 0.9902440905570984,
                  "support": 0.004246656317263842
                },
                "stance_score": -0.0012626638635993004,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.366555094718933
              },
              {
                "id": 4381,
                "faiss_score": 0.8680335283279419,
                "faiss_rank": 10,
                "doc_id": "wiki_Computational_complexity_theory",
                "file_type": ".txt",
                "position": 57,
                "sentence": "However, some computational problems are easier to analyze in terms of more unusual resources.",
                "source_type": "wikipedia",
                "credibility": 0.6,
                "source_url": "https://en.wikipedia.org/wiki/Computational_complexity_theory",
                "primary_category": "articles with short description",
                "rerank_score": -5.572509765625,
                "rerank_rank": 4,
                "probs": {
                  "contradict": 0.054365962743759155,
                  "neutral": 0.941412627696991,
                  "support": 0.004221378359943628
                },
                "stance_score": -0.05014458438381553,
                "evidence_contribution": -0.0,
                "combined_rank_score": -4.704476237297058
              }
            ]
          }
        }
      ],
      "explanation": {
        "summary": "The claim is generally supported by the available evidence, with some limitations.",
        "sections": [
          {
            "type": "SUPPORTED_ASPECTS",
            "items": [
              {
                "subclaim": "Increasing computational resources increases cost",
                "verdict": "SUPPORT",
                "strength_summary": {
                  "support": "strong",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 5995,
                      "faiss_score": 0.8827309012413025,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_evaluation_and_generalization.txt",
                      "file_type": ".txt",
                      "position": 21,
                      "sentence": "This technique reduces variance in evaluation estimates but increases computational cost.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_evaluation_and_generalization.txt",
                      "primary_category": null,
                      "rerank_score": 4.090311050415039,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.001421174150891602,
                        "neutral": 0.3918896019458771,
                        "support": 0.6066892743110657
                      },
                      "stance_score": 0.6052681001601741,
                      "evidence_contribution": 2.475734798548877,
                      "combined_rank_score": 4.9730419516563416
                    },
                    {
                      "id": 6651,
                      "faiss_score": 0.8786903619766235,
                      "faiss_rank": 13,
                      "doc_id": "local_systems_fault_tolerance_and_reliability.txt",
                      "file_type": ".txt",
                      "position": 54,
                      "sentence": "Adding redundancy increases cost and complexity.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\systems_fault_tolerance_and_reliability.txt",
                      "primary_category": null,
                      "rerank_score": -1.3506048917770386,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0019581627566367388,
                        "neutral": 0.4421144723892212,
                        "support": 0.5559273958206177
                      },
                      "stance_score": 0.5539692330639809,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.47191452980041504
                    },
                    {
                      "id": 2503,
                      "faiss_score": 0.881470799446106,
                      "faiss_rank": 7,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 98,
                      "sentence": "However, those were more computationally expensive compared to backpropagation.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": -6.590202331542969,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.0016182546969503164,
                        "neutral": 0.8172633647918701,
                        "support": 0.18111830949783325
                      },
                      "stance_score": 0.17950005480088294,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -5.708731532096863
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 5923,
                      "faiss_score": 0.8798081874847412,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 18,
                      "sentence": "This technique shifts computational cost from deployment to training.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -2.631453037261963,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.14017978310585022,
                        "neutral": 0.8297988176345825,
                        "support": 0.030021382495760918
                      },
                      "stance_score": -0.1101584006100893,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.7516448497772217
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6092,
                      "faiss_score": 0.8832039833068848,
                      "faiss_rank": 4,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 52,
                      "sentence": "The computational cost of training large language models is substantial.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -3.503066062927246,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.0005319765186868608,
                        "neutral": 0.9969078898429871,
                        "support": 0.002560136839747429
                      },
                      "stance_score": 0.002028160321060568,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -2.6198620796203613
                    },
                    {
                      "id": 2615,
                      "faiss_score": 0.8780773878097534,
                      "faiss_rank": 15,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 210,
                      "sentence": "Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": -4.063608646392822,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.0008609522483311594,
                        "neutral": 0.9922782182693481,
                        "support": 0.006860824301838875
                      },
                      "stance_score": 0.0059998720535077155,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -3.185531258583069
                    },
                    {
                      "id": 6527,
                      "faiss_score": 0.8844648003578186,
                      "faiss_rank": 3,
                      "doc_id": "local_physics_computation.txt",
                      "file_type": ".txt",
                      "position": 56,
                      "sentence": "Software plays a critical role in determining how effectively computational resources are used.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                      "primary_category": null,
                      "rerank_score": -5.428127765655518,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.00802410114556551,
                        "neutral": 0.9895147085189819,
                        "support": 0.002461161930114031
                      },
                      "stance_score": -0.005562939215451479,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -4.543662965297699
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "CONTRADICTED_ASPECTS",
            "items": [
              {
                "subclaim": "Increasing computational resources enables larger models",
                "verdict": "CONTRADICT",
                "strength_summary": {
                  "support": "none",
                  "contradict": "weak"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 6131,
                      "faiss_score": 0.8856824636459351,
                      "faiss_rank": 20,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "Model scaling involves increasing the number of parameters in a neural network.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": -2.550759792327881,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.0036448510363698006,
                        "neutral": 0.844368040561676,
                        "support": 0.15198716521263123
                      },
                      "stance_score": 0.14834231417626143,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.6650773286819458
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 6147,
                      "faiss_score": 0.88716059923172,
                      "faiss_rank": 19,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 23,
                      "sentence": "For example, increasing model size without increasing data may yield limited benefits, while increasing data without sufficient model capacity may fail to exploit the additional information.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 1.6675896644592285,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.3358306884765625,
                        "neutral": 0.6573573350906372,
                        "support": 0.006811974570155144
                      },
                      "stance_score": -0.32901871390640736,
                      "evidence_contribution": -0.5486682067239927,
                      "combined_rank_score": 2.5547502636909485
                    }
                  ],
                  "neutral": [
                    {
                      "id": 5907,
                      "faiss_score": 0.8947705626487732,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 3.712005615234375,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.23810796439647675,
                        "neutral": 0.5151563882827759,
                        "support": 0.24673567712306976
                      },
                      "stance_score": 0.008627712726593018,
                      "evidence_contribution": 0.03202611808774236,
                      "combined_rank_score": 4.606776177883148
                    },
                    {
                      "id": 1901,
                      "faiss_score": 0.8953041434288025,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 169,
                      "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": 1.582042932510376,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0021848832257092,
                        "neutral": 0.9945988655090332,
                        "support": 0.003216272220015526
                      },
                      "stance_score": 0.001031388994306326,
                      "evidence_contribution": 0.0016317016691113073,
                      "combined_rank_score": 2.4773470759391785
                    },
                    {
                      "id": 6132,
                      "faiss_score": 0.9344620704650879,
                      "faiss_rank": 1,
                      "doc_id": "local_ml_scaling_and_efficiency.txt",
                      "file_type": ".txt",
                      "position": 8,
                      "sentence": "Larger models have greater representational capacity, allowing them to fit more complex functions.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_scaling_and_efficiency.txt",
                      "primary_category": null,
                      "rerank_score": 0.2170807421207428,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.0012542104814201593,
                        "neutral": 0.9608927369117737,
                        "support": 0.03785300627350807
                      },
                      "stance_score": 0.03659879579208791,
                      "evidence_contribution": 0.007944893751271963,
                      "combined_rank_score": 1.1515428125858307
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "CONTROVERSIAL_ASPECTS",
            "items": [
              {
                "subclaim": "Increasing computational resources improves deep learning training speed",
                "verdict": "MIXED",
                "strength_summary": {
                  "support": "weak",
                  "contradict": "weak"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 2164,
                      "faiss_score": 0.8744220733642578,
                      "faiss_rank": 13,
                      "doc_id": "wiki_Artificial_neural_network",
                      "file_type": ".txt",
                      "position": 95,
                      "sentence": "Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
                      "primary_category": "neural networks",
                      "rerank_score": 3.0568156242370605,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0037023804616183043,
                        "neutral": 0.8704550266265869,
                        "support": 0.1258426308631897
                      },
                      "stance_score": 0.12214025040157139,
                      "evidence_contribution": 0.37336022577575034,
                      "combined_rank_score": 3.9312376976013184
                    },
                    {
                      "id": 2625,
                      "faiss_score": 0.87431401014328,
                      "faiss_rank": 14,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 220,
                      "sentence": "Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": -0.11400533467531204,
                      "rerank_rank": 5,
                      "probs": {
                        "contradict": 0.003962540999054909,
                        "neutral": 0.5770430564880371,
                        "support": 0.41899439692497253
                      },
                      "stance_score": 0.4150318559259176,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": 0.760308675467968
                    },
                    {
                      "id": 1901,
                      "faiss_score": 0.8789441585540771,
                      "faiss_rank": 9,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 169,
                      "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -1.2347545623779297,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.005857494659721851,
                        "neutral": 0.885772168636322,
                        "support": 0.10837028920650482
                      },
                      "stance_score": 0.10251279454678297,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.35581040382385254
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 5906,
                      "faiss_score": 0.877042293548584,
                      "faiss_rank": 10,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 1,
                      "sentence": "Early successes in deep learning were often achieved by increasing model size and training data, but this approach quickly encounters practical limits.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": 1.090013861656189,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.29159143567085266,
                        "neutral": 0.7015077471733093,
                        "support": 0.006900775246322155
                      },
                      "stance_score": -0.2846906604245305,
                      "evidence_contribution": -0.31031676614679327,
                      "combined_rank_score": 1.967056155204773
                    },
                    {
                      "id": 5907,
                      "faiss_score": 0.8837117552757263,
                      "faiss_rank": 6,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -1.9216840267181396,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.45006662607192993,
                        "neutral": 0.5438736081123352,
                        "support": 0.006059776060283184
                      },
                      "stance_score": -0.44400685001164675,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.0379722714424133
                    }
                  ],
                  "neutral": [
                    {
                      "id": 2624,
                      "faiss_score": 0.8756795525550842,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 219,
                      "sentence": "OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": 3.08569598197937,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.05276564508676529,
                        "neutral": 0.9433174729347229,
                        "support": 0.003916928544640541
                      },
                      "stance_score": -0.04884871654212475,
                      "evidence_contribution": -0.15073228835888353,
                      "combined_rank_score": 3.9613755345344543
                    },
                    {
                      "id": 2622,
                      "faiss_score": 0.892419695854187,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Deep_learning",
                      "file_type": ".txt",
                      "position": 217,
                      "sentence": "Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Deep_learning",
                      "primary_category": "deep learning",
                      "rerank_score": 1.4879298210144043,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0014016155619174242,
                        "neutral": 0.9192524552345276,
                        "support": 0.0793459415435791
                      },
                      "stance_score": 0.07794432598166168,
                      "evidence_contribution": 0.11597568700698224,
                      "combined_rank_score": 2.3803495168685913
                    },
                    {
                      "id": 6044,
                      "faiss_score": 0.890583872795105,
                      "faiss_rank": 3,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 4,
                      "sentence": "Increasing the number of parameters, the amount of training data, and the total compute used during training has been shown to produce consistent improvements in performance across many benchmarks.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -0.8982527256011963,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.0011553606018424034,
                        "neutral": 0.9447371959686279,
                        "support": 0.0541074313223362
                      },
                      "stance_score": 0.052952070720493793,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.007668852806091309
                    }
                  ]
                }
              }
            ]
          },
          {
            "type": "EVIDENCE_LIMITATIONS",
            "items": [
              {
                "subclaim": "Increasing computational resources stabilizes optimization",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 5890,
                      "faiss_score": 0.8737372756004333,
                      "faiss_rank": 15,
                      "doc_id": "local_math_optimization_and_convergence.txt",
                      "file_type": ".txt",
                      "position": 59,
                      "sentence": "Asynchronous optimization can improve throughput but may slow convergence or introduce bias.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                      "primary_category": null,
                      "rerank_score": -4.720156669616699,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.6580743789672852,
                        "neutral": 0.33626577258110046,
                        "support": 0.005659839604049921
                      },
                      "stance_score": -0.6524145393632352,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -3.846419394016266
                    },
                    {
                      "id": 79,
                      "faiss_score": 0.8764380216598511,
                      "faiss_rank": 10,
                      "doc_id": "openreview_ztgT8Iok130",
                      "file_type": ".txt",
                      "position": 1,
                      "sentence": "Computational methods have achieved initial success but still struggle with simultaneously optimizing multiple competing properties in a sample-efficient manner.",
                      "source_type": "openreview",
                      "credibility": 0.7,
                      "source_url": "https://openreview.net/forum?id=ztgT8Iok130",
                      "primary_category": null,
                      "rerank_score": -7.7861409187316895,
                      "rerank_rank": 9,
                      "probs": {
                        "contradict": 0.3723789155483246,
                        "neutral": 0.6259058713912964,
                        "support": 0.001715235412120819
                      },
                      "stance_score": -0.37066368013620377,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -6.909702897071838
                    }
                  ],
                  "neutral": [
                    {
                      "id": 5868,
                      "faiss_score": 0.8724570274353027,
                      "faiss_rank": 19,
                      "doc_id": "local_math_optimization_and_convergence.txt",
                      "file_type": ".txt",
                      "position": 37,
                      "sentence": "Regularization can improve generalization and stabilize optimization by smoothing the objective landscape.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                      "primary_category": null,
                      "rerank_score": -1.1302709579467773,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.028316836804151535,
                        "neutral": 0.9502172470092773,
                        "support": 0.021465880796313286
                      },
                      "stance_score": -0.006850956007838249,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -0.2578139305114746
                    },
                    {
                      "id": 1901,
                      "faiss_score": 0.8825475573539734,
                      "faiss_rank": 5,
                      "doc_id": "wiki_Large_language_model",
                      "file_type": ".txt",
                      "position": 169,
                      "sentence": "OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Large_language_model",
                      "primary_category": "natural language processing",
                      "rerank_score": -1.6149537563323975,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.02632685750722885,
                        "neutral": 0.9125757217407227,
                        "support": 0.06109738349914551
                      },
                      "stance_score": 0.034770525991916656,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.7324061989784241
                    },
                    {
                      "id": 5857,
                      "faiss_score": 0.8886821269989014,
                      "faiss_rank": 2,
                      "doc_id": "local_math_optimization_and_convergence.txt",
                      "file_type": ".txt",
                      "position": 26,
                      "sentence": "Balancing exploration and stability is a recurring theme in optimization theory and practice.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                      "primary_category": null,
                      "rerank_score": -3.3342981338500977,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.004754449240863323,
                        "neutral": 0.9945986270904541,
                        "support": 0.0006469713989645243
                      },
                      "stance_score": -0.004107477841898799,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -2.4456160068511963
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing computational resources increases energy usage",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [
                    {
                      "id": 5907,
                      "faiss_score": 0.8672863245010376,
                      "faiss_rank": 16,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "Training large models requires substantial computational resources, and deploying them in real-world systems introduces constraints related to latency, memory, and energy consumption.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -1.2218883037567139,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.011085989885032177,
                        "neutral": 0.8547480702400208,
                        "support": 0.1341659426689148
                      },
                      "stance_score": 0.12307995278388262,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -0.35460197925567627
                    },
                    {
                      "id": 5432,
                      "faiss_score": 0.9075989723205566,
                      "faiss_rank": 2,
                      "doc_id": "wiki_Landauer's_principle",
                      "file_type": ".txt",
                      "position": 2,
                      "sentence": "As of 2012, modern computers use about a billion times as much energy per operation.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Landauer%27s_principle",
                      "primary_category": "all articles containing potentially dated statements",
                      "rerank_score": -2.027003526687622,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0011741387424990535,
                        "neutral": 0.6837896704673767,
                        "support": 0.31503620743751526
                      },
                      "stance_score": 0.3138620686950162,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.1194045543670654
                    },
                    {
                      "id": 5381,
                      "faiss_score": 0.8766747713088989,
                      "faiss_rank": 11,
                      "doc_id": "wiki_Reversible_computing",
                      "file_type": ".txt",
                      "position": 6,
                      "sentence": "Reversible computing proponents argue that a significant portion of this energy consumption is due to architectural overheads.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Reversible_computing",
                      "primary_category": "articles with short description",
                      "rerank_score": -2.1637558937072754,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.03369560465216637,
                        "neutral": 0.725196897983551,
                        "support": 0.24110743403434753
                      },
                      "stance_score": 0.20741182938218117,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -1.2870811223983765
                    }
                  ],
                  "contradicting": [
                    {
                      "id": 5954,
                      "faiss_score": 0.8787448406219482,
                      "faiss_rank": 8,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 49,
                      "sentence": "Techniques that reduce computation or enable reuse of pretrained components can lower energy consumption.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -2.529705047607422,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.9419150948524475,
                        "neutral": 0.05364186316728592,
                        "support": 0.004443134646862745
                      },
                      "stance_score": -0.9374719602055848,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -1.6509602069854736
                    },
                    {
                      "id": 5380,
                      "faiss_score": 0.8628027439117432,
                      "faiss_rank": 20,
                      "doc_id": "wiki_Reversible_computing",
                      "file_type": ".txt",
                      "position": 5,
                      "sentence": "The Landauer limit was millions of times below the energy consumption of computers in the 2000s and thousands of times less in the 2010s.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Reversible_computing",
                      "primary_category": "articles with short description",
                      "rerank_score": -5.459789276123047,
                      "rerank_rank": 7,
                      "probs": {
                        "contradict": 0.1545737087726593,
                        "neutral": 0.8282281756401062,
                        "support": 0.01719808205962181
                      },
                      "stance_score": -0.1373756267130375,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -4.596986532211304
                    }
                  ],
                  "neutral": [
                    {
                      "id": 5382,
                      "faiss_score": 0.8740999698638916,
                      "faiss_rank": 12,
                      "doc_id": "wiki_Reversible_computing",
                      "file_type": ".txt",
                      "position": 7,
                      "sentence": "These overheads are the energy costs associated with non-computational parts of the system, such as wires, transistors, and memory, that are required to make a computer work.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Reversible_computing",
                      "primary_category": "articles with short description",
                      "rerank_score": -5.730232238769531,
                      "rerank_rank": 8,
                      "probs": {
                        "contradict": 0.0461512990295887,
                        "neutral": 0.9230378866195679,
                        "support": 0.03081084042787552
                      },
                      "stance_score": -0.01534045860171318,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -4.85613226890564
                    },
                    {
                      "id": 6527,
                      "faiss_score": 0.8845221996307373,
                      "faiss_rank": 6,
                      "doc_id": "local_physics_computation.txt",
                      "file_type": ".txt",
                      "position": 56,
                      "sentence": "Software plays a critical role in determining how effectively computational resources are used.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                      "primary_category": null,
                      "rerank_score": -6.074432849884033,
                      "rerank_rank": 10,
                      "probs": {
                        "contradict": 0.0268668495118618,
                        "neutral": 0.9703036546707153,
                        "support": 0.002829460659995675
                      },
                      "stance_score": -0.024037388851866126,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -5.189910650253296
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing computational resources increases hardware dependence",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 6495,
                      "faiss_score": 0.872918426990509,
                      "faiss_rank": 20,
                      "doc_id": "local_physics_computation.txt",
                      "file_type": ".txt",
                      "position": 24,
                      "sentence": "The design of computing systems increasingly involves co-optimization of hardware and software.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                      "primary_category": null,
                      "rerank_score": -7.28829288482666,
                      "rerank_rank": 6,
                      "probs": {
                        "contradict": 0.36655545234680176,
                        "neutral": 0.6029763221740723,
                        "support": 0.030468249693512917
                      },
                      "stance_score": -0.33608720265328884,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -6.415374457836151
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6530,
                      "faiss_score": 0.8772687911987305,
                      "faiss_rank": 11,
                      "doc_id": "local_physics_computation.txt",
                      "file_type": ".txt",
                      "position": 59,
                      "sentence": "This dependence underscores the importance of considering computation as a layered system.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                      "primary_category": null,
                      "rerank_score": -3.2304980754852295,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.0025236783549189568,
                        "neutral": 0.9954648613929749,
                        "support": 0.002011485630646348
                      },
                      "stance_score": -0.0005121927242726088,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -2.353229284286499
                    },
                    {
                      "id": 5944,
                      "faiss_score": 0.8837687969207764,
                      "faiss_rank": 5,
                      "doc_id": "local_ml_efficiency_and_compression.txt",
                      "file_type": ".txt",
                      "position": 39,
                      "sentence": "Hardware plays a significant role in shaping efficiency strategies.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_efficiency_and_compression.txt",
                      "primary_category": null,
                      "rerank_score": -5.977171897888184,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.0010084060486406088,
                        "neutral": 0.9978429079055786,
                        "support": 0.0011486936127766967
                      },
                      "stance_score": 0.0001402875641360879,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -5.093403100967407
                    },
                    {
                      "id": 5900,
                      "faiss_score": 0.8796184062957764,
                      "faiss_rank": 9,
                      "doc_id": "local_math_optimization_and_convergence.txt",
                      "file_type": ".txt",
                      "position": 69,
                      "sentence": "Communication, memory access, and hardware utilization influence practical convergence speed.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\math_optimization_and_convergence.txt",
                      "primary_category": null,
                      "rerank_score": -6.411072731018066,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.0015513167018070817,
                        "neutral": 0.997253954410553,
                        "support": 0.0011947569437325
                      },
                      "stance_score": -0.0003565597580745816,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -5.53145432472229
                    }
                  ]
                }
              },
              {
                "subclaim": "Increasing computational resources increases environmental impact",
                "verdict": "INCONCLUSIVE",
                "strength_summary": {
                  "support": "none",
                  "contradict": "none"
                },
                "evidence": {
                  "supporting": [],
                  "contradicting": [
                    {
                      "id": 6534,
                      "faiss_score": 0.8964979648590088,
                      "faiss_rank": 1,
                      "doc_id": "local_physics_computation.txt",
                      "file_type": ".txt",
                      "position": 63,
                      "sentence": "Research in computation increasingly emphasizes efficiency and sustainability.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\physics_computation.txt",
                      "primary_category": null,
                      "rerank_score": -5.121312141418457,
                      "rerank_rank": 2,
                      "probs": {
                        "contradict": 0.28368353843688965,
                        "neutral": 0.7062990665435791,
                        "support": 0.010017365217208862
                      },
                      "stance_score": -0.2736661732196808,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -4.224814176559448
                    }
                  ],
                  "neutral": [
                    {
                      "id": 6094,
                      "faiss_score": 0.8690935373306274,
                      "faiss_rank": 9,
                      "doc_id": "local_ml_llms.txt",
                      "file_type": ".txt",
                      "position": 54,
                      "sentence": "These costs limit participation to well-resourced organizations and raise concerns about environmental impact.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_llms.txt",
                      "primary_category": null,
                      "rerank_score": -4.8720855712890625,
                      "rerank_rank": 1,
                      "probs": {
                        "contradict": 0.009297624230384827,
                        "neutral": 0.9804723858833313,
                        "support": 0.010230030864477158
                      },
                      "stance_score": 0.0009324066340923309,
                      "evidence_contribution": 0.0,
                      "combined_rank_score": -4.002992033958435
                    },
                    {
                      "id": 6310,
                      "faiss_score": 0.8743864297866821,
                      "faiss_rank": 7,
                      "doc_id": "local_ml_transformers.txt",
                      "file_type": ".txt",
                      "position": 49,
                      "sentence": "This dependence raises concerns about accessibility, reproducibility, and environmental impact.",
                      "source_type": "local",
                      "credibility": 0.9,
                      "source_url": "C:\\Users\\Harshal Raj\\Fact-Anchor\\ml-service\\data\\local_curated\\ml_transformers.txt",
                      "primary_category": null,
                      "rerank_score": -5.240941524505615,
                      "rerank_rank": 3,
                      "probs": {
                        "contradict": 0.005509320180863142,
                        "neutral": 0.9902440905570984,
                        "support": 0.004246656317263842
                      },
                      "stance_score": -0.0012626638635993004,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -4.366555094718933
                    },
                    {
                      "id": 4381,
                      "faiss_score": 0.8680335283279419,
                      "faiss_rank": 10,
                      "doc_id": "wiki_Computational_complexity_theory",
                      "file_type": ".txt",
                      "position": 57,
                      "sentence": "However, some computational problems are easier to analyze in terms of more unusual resources.",
                      "source_type": "wikipedia",
                      "credibility": 0.6,
                      "source_url": "https://en.wikipedia.org/wiki/Computational_complexity_theory",
                      "primary_category": "articles with short description",
                      "rerank_score": -5.572509765625,
                      "rerank_rank": 4,
                      "probs": {
                        "contradict": 0.054365962743759155,
                        "neutral": 0.941412627696991,
                        "support": 0.004221378359943628
                      },
                      "stance_score": -0.05014458438381553,
                      "evidence_contribution": -0.0,
                      "combined_rank_score": -4.704476237297058
                    }
                  ]
                }
              }
            ]
          }
        ]
      }
    }
  ],
  "metrics": {
    "final_accuracy": 0.5925925925925926,
    "confusion_matrix": {
      "SUPPORT": {
        "SUPPORT": 5,
        "CONTRADICT": 1,
        "MIXED": 0,
        "INCONCLUSIVE": 0
      },
      "CONTRADICT": {
        "SUPPORT": 1,
        "CONTRADICT": 5,
        "MIXED": 0,
        "INCONCLUSIVE": 0
      },
      "MIXED": {
        "SUPPORT": 1,
        "CONTRADICT": 6,
        "MIXED": 4,
        "INCONCLUSIVE": 0
      },
      "INCONCLUSIVE": {
        "SUPPORT": 0,
        "CONTRADICT": 2,
        "MIXED": 0,
        "INCONCLUSIVE": 2
      }
    },
    "precision_recall_f1": {
      "per_class": {
        "SUPPORT": {
          "precision": 0.7142857142857143,
          "recall": 0.8333333333333334,
          "f1": 0.7692307692307692
        },
        "CONTRADICT": {
          "precision": 0.35714285714285715,
          "recall": 0.8333333333333334,
          "f1": 0.5
        },
        "MIXED": {
          "precision": 1.0,
          "recall": 0.36363636363636365,
          "f1": 0.5333333333333333
        },
        "INCONCLUSIVE": {
          "precision": 1.0,
          "recall": 0.5,
          "f1": 0.6666666666666666
        }
      },
      "macro_f1": 0.6173076923076922
    },
    "subclaim_accuracy": null,
    "decomposition_stats": {
      "avg_subclaims_per_claim": 2.8518518518518516,
      "min_subclaims": 1,
      "max_subclaims": 7
    }
  }
}